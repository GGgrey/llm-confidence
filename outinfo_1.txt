==================================================
Configurations:
Model name: /data/sunqiao/projects/models/Llama-3.1-8B-Instruct
Lingua model name: microsoft/llmlingua-2-xlm-roberta-large-meetingbank
Aggregate: True
k: 16
Number of samples: 500
Seed: 11
Data directory: data
Batch size: 1
Dataset files: {'gsm8k': 'openai_gsm8k_test_processed.parquet'}
Multihop: False
==================================================

`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.34s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.35s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
`torch_dtype` is deprecated! Use `dtype` instead!
Dataset name: gsm8k

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 0/500 [00:00<?, ?it/s]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 8.159370404397533
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 8.159370404397533
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 15.994536578655243
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 1.0
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 13.51953125
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 2.1743199825286865
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 1.1707829385995865
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 1.0843654125928879
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: To find the total ounces of meat consumed by the team, we need to calculate the ...
    Score: 9.925715446472168
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 0/500 [04:47<?, ?it/s, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 1/500 [04:47<39:50:11, 287.40s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 8.712936681051389
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 8.712936681051389
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 15.999293088912964
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 1.0
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 10.97265625
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 1.361386425793171
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 1.0208358615636826
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 1.0179579481482506
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the cost of each mixture and then fi...
    Score: 5.476904183626175
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 1/500 [10:34<39:50:11, 287.40s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 2/500 [10:34<44:35:12, 322.31s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 8.269755207121307
    Answer: 480
    Ground truth:  480
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 8.269755207121307
    Answer: 480
    Ground truth:  480
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 15.993333160877228
    Answer: 480
    Ground truth:  480
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 1.0
    Answer: 480
    Ground truth:  480
Method 5: p_true
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 14.7109375
    Answer: 480
    Ground truth:  480
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 1.308920405805111
    Answer: 480
    Ground truth:  480
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 1.1442984640598297
    Answer: 480
    Ground truth:  480
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 1.1444352865219116
    Answer: 480
    Ground truth:  480
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Mark makes in a week, we need to calculate his total hours ...
    Score: 2.4648377597332
    Answer: 480
    Ground truth:  480
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 2/500 [14:33<44:35:12, 322.31s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 3/500 [14:33<39:17:23, 284.59s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 8.046574460823525
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 8.046574460823525
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 15.999075591564178
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 1.0
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 15.19140625
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 2.85208398103714
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 2.039443761110306
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 2.0298091769218445
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. 40 pupils in the class like blue: 40 * 1/2...
    Score: 4.946235001087189
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 3/500 [18:21<39:17:23, 284.59s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 4/500 [18:21<36:07:58, 262.26s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 3.977529116406001
    Answer: 45
    Ground truth:  45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 3.977529116406001
    Answer: 45
    Ground truth:  45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 7.998237371444702
    Answer: 45
    Ground truth:  45
Method 4: self_consistency
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 0.5
    Answer: 45
    Ground truth:  45
Method 5: p_true
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 7.71484375
    Answer: 45
    Ground truth:  45
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 5.954769313335419
    Answer: 45
    Ground truth:  45
Method 7: normilized_entropy
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 5.223605215549469
    Answer: 45
    Ground truth:  45
Method 8: topk_entropy
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 4.640587627887726
    Answer: 45
    Ground truth:  45
Method 9: window_entropy
  Batch 1:
    Text: To find the time Emily will need to peel and cook 90 shrimp, let's break it down...
    Score: 9.751915752887726
    Answer: 45
    Ground truth:  45
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 4/500 [24:21<36:07:58, 262.26s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 5/500 [24:21<40:52:31, 297.28s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 5.599327448487497
    Answer: 14
    Ground truth:  14
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 5.599327448487497
    Answer: 14
    Ground truth:  14
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 10.998716473579407
    Answer: 14
    Ground truth:  14
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 0.6875
    Answer: 14
    Ground truth:  14
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 9.796875
    Answer: 14
    Ground truth:  14
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 3.441023424267769
    Answer: 14
    Ground truth:  14
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 3.833861142396927
    Answer: 14
    Ground truth:  14
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 3.50647309422493
    Answer: 14
    Ground truth:  14
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Martha invited 2 families with 6 ...
    Score: 11.939687073230743
    Answer: 14
    Ground truth:  14
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 5/500 [29:54<40:52:31, 297.28s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 6/500 [29:54<42:29:03, 309.60s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 3.40609704773776
    Answer: 4400
    Ground truth:  4400
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 3.40609704773776
    Answer: 4400
    Ground truth:  4400
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 2.7889781498474946
    Answer: 4400
    Ground truth:  4400
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 0.4375
    Answer: 4400
    Ground truth:  4400
Method 5: p_true
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 6.390625
    Answer: 4400
    Ground truth:  4400
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 9.991100758314133
    Answer: 4400
    Ground truth:  4400
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 9.366298720240593
    Answer: 4400
    Ground truth:  4400
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 7.908121556043625
    Answer: 4400
    Ground truth:  4400
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of replacing Mike's movie collection, we need to break it...
    Score: 10.355420887470245
    Answer: 4400
    Ground truth:  4400
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 6/500 [39:30<42:29:03, 309.60s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|▏         | 7/500 [39:30<54:19:47, 396.73s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 8.21966770272746
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 8.21966770272746
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 15.99415498971939
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 1.0
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 14.05859375
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 1.7938932329416275
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 1.4468287974596024
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 1.443903610110283
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To determine how much Greg will spend on Jello, we need to find out how many box...
    Score: 3.7392412424087524
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|▏         | 7/500 [43:52<54:19:47, 396.73s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 8/500 [43:52<48:19:57, 353.65s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 7.005558592846719
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 7.005558592846719
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 14.976898789405823
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 0.9375
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 8.630859375
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 14.27470862865448
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 12.222518861293793
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 10.536309659481049
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how many kilograms James will have left, we need to calculate the to...
    Score: 16.431851506233215
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 8/500 [48:17<48:19:57, 353.65s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 9/500 [48:17<44:28:30, 326.09s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 4.014781818147141
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 4.014781818147141
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 6.819515898823738
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 0.5
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 7.6015625
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 6.029776394367218
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 5.443153470754623
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 4.875059098005295
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

On Monday and Tuesday, Frankie watch...
    Score: 10.800480306148529
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 9/500 [56:15<44:28:30, 326.09s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 10/500 [56:15<50:46:17, 373.02s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 4.479870839697239
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 4.479870839697239
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 8.928739368915558
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 0.5625
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 8.15234375
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 8.535151079297066
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 7.9901246428489685
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 6.893561244010925
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. Let the amount of wat...
    Score: 11.713084697723389
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 10/500 [1:05:45<50:46:17, 373.02s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 11/500 [1:05:45<58:50:54, 433.24s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 1.7298202868133172
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 1.7298202868133172
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 3.999651849269867
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 0.25
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 1.9853515625
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 1.4703731536865234
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 1.2160428017377853
    Answer: 13
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 1.0979142040014267
    Answer: 13
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find the number of days the teacher has before he has to recycle the chalk, w...
    Score: 1.9903514385223389
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 91.66666666666666
Method name: topk_entropy, running accuracy: 91.66666666666666
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 11/500 [1:16:14<58:50:54, 433.24s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=91.67%, topk_entropy_acc=91.67%, window_entropy_acc=100.00%]  
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 12/500 [1:16:14<66:46:28, 492.60s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=91.67%, topk_entropy_acc=91.67%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 6.676853340220338
    Answer: 48
    Ground truth:  48
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 6.676853340220338
    Answer: 48
    Ground truth:  48
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 12.758276104927063
    Answer: 48
    Ground truth:  48
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 0.8125
    Answer: 48
    Ground truth:  48
Method 5: p_true
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 10.15234375
    Answer: 48
    Ground truth:  48
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 1.6285970956087112
    Answer: 48
    Ground truth:  48
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 1.253128170967102
    Answer: 48
    Ground truth:  48
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 1.234370544552803
    Answer: 48
    Ground truth:  48
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Calculate the total number of brownies Greta had at the beginning of the...
    Score: 4.476697862148285
    Answer: 48
    Ground truth:  48
Method name: attention_weighted_confidence, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 92.3076923076923
Method name: topk_entropy, running accuracy: 92.3076923076923
Method name: window_entropy, running accuracy: 100.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 12/500 [1:22:24<66:46:28, 492.60s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=92.31%, topk_entropy_acc=92.31%, window_entropy_acc=100.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 13/500 [1:22:24<61:38:24, 455.66s/it, attention_weighted_confidence_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=92.31%, topk_entropy_acc=92.31%, window_entropy_acc=100.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 2.5037047927423703
    Answer: 9
    Ground truth:  22
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 2.5037047927423703
    Answer: 9
    Ground truth:  22
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 4.99101597070694
    Answer: 9
    Ground truth:  22
Method 4: self_consistency
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 0.3125
    Answer: 9
    Ground truth:  22
Method 5: p_true
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 4.60546875
    Answer: 9
    Ground truth:  22
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 5.219508111476898
    Answer: 9
    Ground truth:  22
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 5.022908538579941
    Answer: 9
    Ground truth:  22
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 4.1820268034935
    Answer: 9
    Ground truth:  22
Method 9: window_entropy
  Batch 1:
    Text: To find out how many days it will take for Lauren to complete the project, we ne...
    Score: 6.558212459087372
    Answer: 9
    Ground truth:  22
Method name: attention_weighted_confidence, running accuracy: 92.85714285714286
Method name: cer_entropy_weighted_mean_all, running accuracy: 92.85714285714286
Method name: cer_prob_product_log_last, running accuracy: 92.85714285714286
Method name: self_consistency, running accuracy: 92.85714285714286
Method name: p_true, running accuracy: 92.85714285714286
Method name: normilized_likelihood, running accuracy: 92.85714285714286
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 92.85714285714286

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 13/500 [1:32:20<61:38:24, 455.66s/it, attention_weighted_confidence_acc=92.86%, cer_entropy_weighted_mean_all_acc=92.86%, cer_prob_product_log_last_acc=92.86%, self_consistency_acc=92.86%, p_true_acc=92.86%, normilized_likelihood_acc=92.86%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=92.86%]       
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 14/500 [1:32:20<67:13:27, 497.96s/it, attention_weighted_confidence_acc=92.86%, cer_entropy_weighted_mean_all_acc=92.86%, cer_prob_product_log_last_acc=92.86%, self_consistency_acc=92.86%, p_true_acc=92.86%, normilized_likelihood_acc=92.86%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=92.86%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 8.105835595444793
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 8.105835595444793
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 15.756308495998383
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 12.46875
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 16.815351635217667
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 17.009164288640022
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 14.795413836836815
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how much free time Harold has left in his day, we need to calculate ...
    Score: 28.589023530483246
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 93.33333333333333
Method name: cer_entropy_weighted_mean_all, running accuracy: 93.33333333333333
Method name: cer_prob_product_log_last, running accuracy: 93.33333333333333
Method name: self_consistency, running accuracy: 93.33333333333333
Method name: p_true, running accuracy: 93.33333333333333
Method name: normilized_likelihood, running accuracy: 93.33333333333333
Method name: normilized_entropy, running accuracy: 86.66666666666667
Method name: topk_entropy, running accuracy: 86.66666666666667
Method name: window_entropy, running accuracy: 93.33333333333333

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 14/500 [1:37:17<67:13:27, 497.96s/it, attention_weighted_confidence_acc=93.33%, cer_entropy_weighted_mean_all_acc=93.33%, cer_prob_product_log_last_acc=93.33%, self_consistency_acc=93.33%, p_true_acc=93.33%, normilized_likelihood_acc=93.33%, normilized_entropy_acc=86.67%, topk_entropy_acc=86.67%, window_entropy_acc=93.33%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 15/500 [1:37:17<58:56:58, 437.56s/it, attention_weighted_confidence_acc=93.33%, cer_entropy_weighted_mean_all_acc=93.33%, cer_prob_product_log_last_acc=93.33%, self_consistency_acc=93.33%, p_true_acc=93.33%, normilized_likelihood_acc=93.33%, normilized_entropy_acc=86.67%, topk_entropy_acc=86.67%, window_entropy_acc=93.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 7.7182062728528225
    Answer: 26
    Ground truth:  26
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 7.7182062728528225
    Answer: 26
    Ground truth:  26
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 15.999794125556946
    Answer: 26
    Ground truth:  26
Method 4: self_consistency
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 1.0
    Answer: 26
    Ground truth:  26
Method 5: p_true
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 11.45703125
    Answer: 26
    Ground truth:  26
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 1.8820349276065826
    Answer: 26
    Ground truth:  26
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 0.8398538380861282
    Answer: 26
    Ground truth:  26
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 0.8381894826889038
    Answer: 26
    Ground truth:  26
Method 9: window_entropy
  Batch 1:
    Text: To find out how much more Keegan and Tashay need to earn, we need to find the to...
    Score: 2.2611187994480133
    Answer: 26
    Ground truth:  26
Method name: attention_weighted_confidence, running accuracy: 93.75
Method name: cer_entropy_weighted_mean_all, running accuracy: 93.75
Method name: cer_prob_product_log_last, running accuracy: 93.75
Method name: self_consistency, running accuracy: 93.75
Method name: p_true, running accuracy: 93.75
Method name: normilized_likelihood, running accuracy: 93.75
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 93.75

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 15/500 [1:40:24<58:56:58, 437.56s/it, attention_weighted_confidence_acc=93.75%, cer_entropy_weighted_mean_all_acc=93.75%, cer_prob_product_log_last_acc=93.75%, self_consistency_acc=93.75%, p_true_acc=93.75%, normilized_likelihood_acc=93.75%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=93.75%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 16/500 [1:40:24<48:39:43, 361.95s/it, attention_weighted_confidence_acc=93.75%, cer_entropy_weighted_mean_all_acc=93.75%, cer_prob_product_log_last_acc=93.75%, self_consistency_acc=93.75%, p_true_acc=93.75%, normilized_likelihood_acc=93.75%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=93.75%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 1.4133982999044488
    Answer: 452
    Ground truth:  452
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 1.4133982999044488
    Answer: 452
    Ground truth:  452
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 2.9960785508155823
    Answer: 452
    Ground truth:  452
Method 4: self_consistency
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 0.1875
    Answer: 452
    Ground truth:  452
Method 5: p_true
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 2.78515625
    Answer: 452
    Ground truth:  452
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 1.9962850213050842
    Answer: 452
    Ground truth:  452
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 1.9732251465320587
    Answer: 452
    Ground truth:  452
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 1.7081941962242126
    Answer: 452
    Ground truth:  452
Method 9: window_entropy
  Batch 1:
    Text: To find out how many minutes Kim slept on her bed that day, we need to calculate...
    Score: 3.480002760887146
    Answer: 452
    Ground truth:  452
Method name: attention_weighted_confidence, running accuracy: 94.11764705882352
Method name: cer_entropy_weighted_mean_all, running accuracy: 94.11764705882352
Method name: cer_prob_product_log_last, running accuracy: 94.11764705882352
Method name: self_consistency, running accuracy: 94.11764705882352
Method name: p_true, running accuracy: 94.11764705882352
Method name: normilized_likelihood, running accuracy: 94.11764705882352
Method name: normilized_entropy, running accuracy: 88.23529411764706
Method name: topk_entropy, running accuracy: 88.23529411764706
Method name: window_entropy, running accuracy: 94.11764705882352

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 16/500 [1:49:26<48:39:43, 361.95s/it, attention_weighted_confidence_acc=94.12%, cer_entropy_weighted_mean_all_acc=94.12%, cer_prob_product_log_last_acc=94.12%, self_consistency_acc=94.12%, p_true_acc=94.12%, normilized_likelihood_acc=94.12%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=94.12%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 17/500 [1:49:26<55:49:03, 416.03s/it, attention_weighted_confidence_acc=94.12%, cer_entropy_weighted_mean_all_acc=94.12%, cer_prob_product_log_last_acc=94.12%, self_consistency_acc=94.12%, p_true_acc=94.12%, normilized_likelihood_acc=94.12%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=94.12%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 7.9786088683593555
    Answer: 45
    Ground truth:  45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 7.9786088683593555
    Answer: 45
    Ground truth:  45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 15.999245941638947
    Answer: 45
    Ground truth:  45
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 1.0
    Answer: 45
    Ground truth:  45
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 13.87890625
    Answer: 45
    Ground truth:  45
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 2.4589940011501312
    Answer: 45
    Ground truth:  45
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 1.1701139509677887
    Answer: 45
    Ground truth:  45
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 1.1546486169099808
    Answer: 45
    Ground truth:  45
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Initially, Katelyn s...
    Score: 3.213224470615387
    Answer: 45
    Ground truth:  45
Method name: attention_weighted_confidence, running accuracy: 94.44444444444444
Method name: cer_entropy_weighted_mean_all, running accuracy: 94.44444444444444
Method name: cer_prob_product_log_last, running accuracy: 94.44444444444444
Method name: self_consistency, running accuracy: 94.44444444444444
Method name: p_true, running accuracy: 94.44444444444444
Method name: normilized_likelihood, running accuracy: 94.44444444444444
Method name: normilized_entropy, running accuracy: 88.88888888888889
Method name: topk_entropy, running accuracy: 88.88888888888889
Method name: window_entropy, running accuracy: 94.44444444444444

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 17/500 [1:53:02<55:49:03, 416.03s/it, attention_weighted_confidence_acc=94.44%, cer_entropy_weighted_mean_all_acc=94.44%, cer_prob_product_log_last_acc=94.44%, self_consistency_acc=94.44%, p_true_acc=94.44%, normilized_likelihood_acc=94.44%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=94.44%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▎         | 18/500 [1:53:02<47:40:25, 356.07s/it, attention_weighted_confidence_acc=94.44%, cer_entropy_weighted_mean_all_acc=94.44%, cer_prob_product_log_last_acc=94.44%, self_consistency_acc=94.44%, p_true_acc=94.44%, normilized_likelihood_acc=94.44%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=94.44%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 7.764899052297361
    Answer: 44
    Ground truth:  44
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 7.764899052297361
    Answer: 44
    Ground truth:  44
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 15.93201857805252
    Answer: 44
    Ground truth:  44
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 1.0
    Answer: 44
    Ground truth:  44
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 12.765625
    Answer: 44
    Ground truth:  44
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 7.696811333298683
    Answer: 44
    Ground truth:  44
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 7.6452188193798065
    Answer: 44
    Ground truth:  44
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 6.3808958530426025
    Answer: 44
    Ground truth:  44
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down into steps.

1. First, let's fin...
    Score: 22.25587546825409
    Answer: 44
    Ground truth:  44
Method name: attention_weighted_confidence, running accuracy: 94.73684210526315
Method name: cer_entropy_weighted_mean_all, running accuracy: 94.73684210526315
Method name: cer_prob_product_log_last, running accuracy: 94.73684210526315
Method name: self_consistency, running accuracy: 94.73684210526315
Method name: p_true, running accuracy: 94.73684210526315
Method name: normilized_likelihood, running accuracy: 94.73684210526315
Method name: normilized_entropy, running accuracy: 89.47368421052632
Method name: topk_entropy, running accuracy: 89.47368421052632
Method name: window_entropy, running accuracy: 94.73684210526315

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▎         | 18/500 [1:59:09<47:40:25, 356.07s/it, attention_weighted_confidence_acc=94.74%, cer_entropy_weighted_mean_all_acc=94.74%, cer_prob_product_log_last_acc=94.74%, self_consistency_acc=94.74%, p_true_acc=94.74%, normilized_likelihood_acc=94.74%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=94.74%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 19/500 [1:59:09<48:01:27, 359.43s/it, attention_weighted_confidence_acc=94.74%, cer_entropy_weighted_mean_all_acc=94.74%, cer_prob_product_log_last_acc=94.74%, self_consistency_acc=94.74%, p_true_acc=94.74%, normilized_likelihood_acc=94.74%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=94.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 5.2131463818319155
    Answer: 8
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 5.2131463818319155
    Answer: 8
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 9.999667048454285
    Answer: 8
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 0.625
    Answer: 8
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 9.34375
    Answer: 8
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 2.418582022190094
    Answer: 8
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 2.533031687140465
    Answer: 8
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 2.4368479549884796
    Answer: 8
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To find out how much John has left after he spends on gas and buys pencils, we f...
    Score: 8.969651639461517
    Answer: 8
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 90.0
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.0
Method name: normilized_likelihood, running accuracy: 90.0
Method name: normilized_entropy, running accuracy: 85.0
Method name: topk_entropy, running accuracy: 85.0
Method name: window_entropy, running accuracy: 90.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 19/500 [2:04:55<48:01:27, 359.43s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.00%, p_true_acc=90.00%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=90.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 20/500 [2:04:55<47:21:26, 355.18s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.00%, p_true_acc=90.00%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=90.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 8.376129712529409
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 8.376129712529409
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 15.999933004379272
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 1.0
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 15.0234375
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 0.4261627122759819
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 0.368177130818367
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 0.37303023040294647
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Howard originally had $100.

2. On Monda...
    Score: 1.8455719947814941
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 90.47619047619048
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.47619047619048
Method name: cer_prob_product_log_last, running accuracy: 90.47619047619048
Method name: self_consistency, running accuracy: 90.47619047619048
Method name: p_true, running accuracy: 90.47619047619048
Method name: normilized_likelihood, running accuracy: 90.47619047619048
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 90.47619047619048

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 20/500 [2:08:31<47:21:26, 355.18s/it, attention_weighted_confidence_acc=90.48%, cer_entropy_weighted_mean_all_acc=90.48%, cer_prob_product_log_last_acc=90.48%, self_consistency_acc=90.48%, p_true_acc=90.48%, normilized_likelihood_acc=90.48%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=90.48%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 21/500 [2:08:31<41:43:23, 313.58s/it, attention_weighted_confidence_acc=90.48%, cer_entropy_weighted_mean_all_acc=90.48%, cer_prob_product_log_last_acc=90.48%, self_consistency_acc=90.48%, p_true_acc=90.48%, normilized_likelihood_acc=90.48%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=90.48%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 6.016018867090724
    Answer: 7
    Ground truth:  7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 6.016018867090724
    Answer: 7
    Ground truth:  7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 9.318732678890228
    Answer: 7
    Ground truth:  7
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 0.75
    Answer: 7
    Ground truth:  7
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 11.71875
    Answer: 7
    Ground truth:  7
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 11.13431091606617
    Answer: 7
    Ground truth:  7
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 10.526106238365173
    Answer: 7
    Ground truth:  7
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 8.885403335094452
    Answer: 7
    Ground truth:  7
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's denote the number of fish in each aquarium as 'x'. ...
    Score: 19.57728922367096
    Answer: 7
    Ground truth:  7
Method name: attention_weighted_confidence, running accuracy: 90.9090909090909
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.9090909090909
Method name: cer_prob_product_log_last, running accuracy: 90.9090909090909
Method name: self_consistency, running accuracy: 90.9090909090909
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 90.9090909090909
Method name: normilized_entropy, running accuracy: 86.36363636363636
Method name: topk_entropy, running accuracy: 86.36363636363636
Method name: window_entropy, running accuracy: 90.9090909090909

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 21/500 [2:12:55<41:43:23, 313.58s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.91%, self_consistency_acc=90.91%, p_true_acc=90.91%, normilized_likelihood_acc=90.91%, normilized_entropy_acc=86.36%, topk_entropy_acc=86.36%, window_entropy_acc=90.91%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 22/500 [2:12:55<39:39:45, 298.71s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.91%, self_consistency_acc=90.91%, p_true_acc=90.91%, normilized_likelihood_acc=90.91%, normilized_entropy_acc=86.36%, topk_entropy_acc=86.36%, window_entropy_acc=90.91%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 1.53694109770886
    Answer: 10
    Ground truth:  11
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 1.53694109770886
    Answer: 10
    Ground truth:  11
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 2.9912612438201904
    Answer: 10
    Ground truth:  11
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 0.1875
    Answer: 10
    Ground truth:  11
Method 5: p_true
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 2.3828125
    Answer: 10
    Ground truth:  11
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 2.8012397289276123
    Answer: 10
    Ground truth:  11
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 2.7103274762630463
    Answer: 10
    Ground truth:  11
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 2.335360825061798
    Answer: 10
    Ground truth:  11
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Milly will lose, we need to calculate the cost of shi...
    Score: 4.477891623973846
    Answer: 10
    Ground truth:  11
Method name: attention_weighted_confidence, running accuracy: 86.95652173913044
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.95652173913044
Method name: cer_prob_product_log_last, running accuracy: 86.95652173913044
Method name: self_consistency, running accuracy: 86.95652173913044
Method name: p_true, running accuracy: 86.95652173913044
Method name: normilized_likelihood, running accuracy: 86.95652173913044
Method name: normilized_entropy, running accuracy: 82.6086956521739
Method name: topk_entropy, running accuracy: 82.6086956521739
Method name: window_entropy, running accuracy: 86.95652173913044

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 22/500 [2:20:52<39:39:45, 298.71s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=86.96%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=82.61%, topk_entropy_acc=82.61%, window_entropy_acc=86.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▍         | 23/500 [2:20:52<46:40:30, 352.27s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=86.96%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=82.61%, topk_entropy_acc=82.61%, window_entropy_acc=86.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 7.171630241068133
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 7.171630241068133
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 13.999829053878784
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 0.875
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 12.640625
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 1.6469318270683289
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 1.108773946762085
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 1.0975136160850525
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Andy's car is consuming, we need to calculate the fol...
    Score: 4.474804043769836
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 87.5
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.5
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 87.5
Method name: p_true, running accuracy: 87.5
Method name: normilized_likelihood, running accuracy: 87.5
Method name: normilized_entropy, running accuracy: 83.33333333333334
Method name: topk_entropy, running accuracy: 83.33333333333334
Method name: window_entropy, running accuracy: 87.5

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▍         | 23/500 [2:27:05<46:40:30, 352.27s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=83.33%, topk_entropy_acc=83.33%, window_entropy_acc=87.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▍         | 24/500 [2:27:05<47:23:36, 358.44s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=83.33%, topk_entropy_acc=83.33%, window_entropy_acc=87.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 4.338138097353473
    Answer: 48
    Ground truth:  48
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 4.338138097353473
    Answer: 48
    Ground truth:  48
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 8.381509989500046
    Answer: 48
    Ground truth:  48
Method 4: self_consistency
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 0.5625
    Answer: 48
    Ground truth:  48
Method 5: p_true
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 8.125
    Answer: 48
    Ground truth:  48
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 4.979955166578293
    Answer: 48
    Ground truth:  48
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 4.674344301223755
    Answer: 48
    Ground truth:  48
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 4.175999075174332
    Answer: 48
    Ground truth:  48
Method 9: window_entropy
  Batch 1:
    Text: To find out how many grams of chips you can eat, we first need to determine how ...
    Score: 8.079525232315063
    Answer: 48
    Ground truth:  48
Method name: attention_weighted_confidence, running accuracy: 88.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.0
Method name: cer_prob_product_log_last, running accuracy: 88.0
Method name: self_consistency, running accuracy: 88.0
Method name: p_true, running accuracy: 88.0
Method name: normilized_likelihood, running accuracy: 88.0
Method name: normilized_entropy, running accuracy: 84.0
Method name: topk_entropy, running accuracy: 84.0
Method name: window_entropy, running accuracy: 88.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▍         | 24/500 [2:33:35<47:23:36, 358.44s/it, attention_weighted_confidence_acc=88.00%, cer_entropy_weighted_mean_all_acc=88.00%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=88.00%, p_true_acc=88.00%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=84.00%, topk_entropy_acc=84.00%, window_entropy_acc=88.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 25/500 [2:33:35<48:31:30, 367.77s/it, attention_weighted_confidence_acc=88.00%, cer_entropy_weighted_mean_all_acc=88.00%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=88.00%, p_true_acc=88.00%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=84.00%, topk_entropy_acc=84.00%, window_entropy_acc=88.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 3.9037136804775985
    Answer: 192
    Ground truth:  192
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 3.9037136804775985
    Answer: 192
    Ground truth:  192
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 7.985443353652954
    Answer: 192
    Ground truth:  192
Method 4: self_consistency
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 0.5
    Answer: 192
    Ground truth:  192
Method 5: p_true
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 6.5390625
    Answer: 192
    Ground truth:  192
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 6.62007611989975
    Answer: 192
    Ground truth:  192
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 6.125993460416794
    Answer: 192
    Ground truth:  192
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 5.439140290021896
    Answer: 192
    Ground truth:  192
Method 9: window_entropy
  Batch 1:
    Text: To find out how many cows are in 8 of the stalls, we need to follow these steps:...
    Score: 11.163560807704926
    Answer: 192
    Ground truth:  192
Method name: attention_weighted_confidence, running accuracy: 88.46153846153845
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.46153846153845
Method name: cer_prob_product_log_last, running accuracy: 88.46153846153845
Method name: self_consistency, running accuracy: 88.46153846153845
Method name: p_true, running accuracy: 88.46153846153845
Method name: normilized_likelihood, running accuracy: 88.46153846153845
Method name: normilized_entropy, running accuracy: 84.61538461538461
Method name: topk_entropy, running accuracy: 84.61538461538461
Method name: window_entropy, running accuracy: 88.46153846153845

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 25/500 [2:38:28<48:31:30, 367.77s/it, attention_weighted_confidence_acc=88.46%, cer_entropy_weighted_mean_all_acc=88.46%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=88.46%, p_true_acc=88.46%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=88.46%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 26/500 [2:38:28<45:28:31, 345.38s/it, attention_weighted_confidence_acc=88.46%, cer_entropy_weighted_mean_all_acc=88.46%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=88.46%, p_true_acc=88.46%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=88.46%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 7.950382061738999
    Answer: 80
    Ground truth:  80
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 7.950382061738999
    Answer: 80
    Ground truth:  80
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 15.999975562095642
    Answer: 80
    Ground truth:  80
Method 4: self_consistency
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 1.0
    Answer: 80
    Ground truth:  80
Method 5: p_true
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 12.552734375
    Answer: 80
    Ground truth:  80
Method 6: normilized_likelihood
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 1.3320824354887009
    Answer: 80
    Ground truth:  80
Method 7: normilized_entropy
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 0.6733937859535217
    Answer: 80
    Ground truth:  80
Method 8: topk_entropy
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 0.659483328461647
    Answer: 80
    Ground truth:  80
Method 9: window_entropy
  Batch 1:
    Text: To find Wilson's current math grade, we need to calculate the average of his fiv...
    Score: 5.5277857184410095
    Answer: 80
    Ground truth:  80
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 88.88888888888889
Method name: p_true, running accuracy: 88.88888888888889
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 85.18518518518519
Method name: topk_entropy, running accuracy: 85.18518518518519
Method name: window_entropy, running accuracy: 88.88888888888889

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 26/500 [2:41:30<45:28:31, 345.38s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=88.89%, p_true_acc=88.89%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=85.19%, topk_entropy_acc=85.19%, window_entropy_acc=88.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 27/500 [2:41:30<38:55:26, 296.25s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=88.89%, p_true_acc=88.89%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=85.19%, topk_entropy_acc=85.19%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 8.095131010207849
    Answer: 7
    Ground truth:  7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 8.095131010207849
    Answer: 7
    Ground truth:  7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 15.998423039913177
    Answer: 7
    Ground truth:  7
Method 4: self_consistency
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 1.0
    Answer: 7
    Ground truth:  7
Method 5: p_true
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 13.2021484375
    Answer: 7
    Ground truth:  7
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 2.4537852853536606
    Answer: 7
    Ground truth:  7
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 1.317328080534935
    Answer: 7
    Ground truth:  7
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 1.2970406860113144
    Answer: 7
    Ground truth:  7
Method 9: window_entropy
  Batch 1:
    Text: To find out how many DVDs Billy sold on Tuesday, we need to calculate the total ...
    Score: 5.466255068778992
    Answer: 7
    Ground truth:  7
Method name: attention_weighted_confidence, running accuracy: 89.28571428571429
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.28571428571429
Method name: cer_prob_product_log_last, running accuracy: 89.28571428571429
Method name: self_consistency, running accuracy: 89.28571428571429
Method name: p_true, running accuracy: 89.28571428571429
Method name: normilized_likelihood, running accuracy: 89.28571428571429
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 89.28571428571429

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 27/500 [2:45:23<38:55:26, 296.25s/it, attention_weighted_confidence_acc=89.29%, cer_entropy_weighted_mean_all_acc=89.29%, cer_prob_product_log_last_acc=89.29%, self_consistency_acc=89.29%, p_true_acc=89.29%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=89.29%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 28/500 [2:45:23<36:21:13, 277.28s/it, attention_weighted_confidence_acc=89.29%, cer_entropy_weighted_mean_all_acc=89.29%, cer_prob_product_log_last_acc=89.29%, self_consistency_acc=89.29%, p_true_acc=89.29%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=89.29%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 7.7491779429298155
    Answer: 50
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 7.7491779429298155
    Answer: 50
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 15.999890685081482
    Answer: 50
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 1.0
    Answer: 50
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 14.78125
    Answer: 50
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 2.9258849918842316
    Answer: 50
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 1.821452647447586
    Answer: 50
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 1.7353788614273071
    Answer: 50
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to det...
    Score: 6.6634361743927
    Answer: 50
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 89.65517241379311
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.65517241379311
Method name: cer_prob_product_log_last, running accuracy: 89.65517241379311
Method name: self_consistency, running accuracy: 89.65517241379311
Method name: p_true, running accuracy: 89.65517241379311
Method name: normilized_likelihood, running accuracy: 89.65517241379311
Method name: normilized_entropy, running accuracy: 86.20689655172413
Method name: topk_entropy, running accuracy: 86.20689655172413
Method name: window_entropy, running accuracy: 89.65517241379311

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 28/500 [2:49:21<36:21:13, 277.28s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=89.66%, self_consistency_acc=89.66%, p_true_acc=89.66%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=86.21%, topk_entropy_acc=86.21%, window_entropy_acc=89.66%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 29/500 [2:49:21<34:44:59, 265.60s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=89.66%, self_consistency_acc=89.66%, p_true_acc=89.66%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=86.21%, topk_entropy_acc=86.21%, window_entropy_acc=89.66%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 8.335821598333201
    Answer: 68
    Ground truth:  68
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 8.335821598333201
    Answer: 68
    Ground truth:  68
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 15.530177891254425
    Answer: 68
    Ground truth:  68
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 1.0
    Answer: 68
    Ground truth:  68
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 14.7578125
    Answer: 68
    Ground truth:  68
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 1.5517470240592957
    Answer: 68
    Ground truth:  68
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 0.8279857039451599
    Answer: 68
    Ground truth:  68
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 0.8069777190685272
    Answer: 68
    Ground truth:  68
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The comforter has 1...
    Score: 5.1712646484375
    Answer: 68
    Ground truth:  68
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 90.0
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.0
Method name: normilized_likelihood, running accuracy: 90.0
Method name: normilized_entropy, running accuracy: 86.66666666666667
Method name: topk_entropy, running accuracy: 86.66666666666667
Method name: window_entropy, running accuracy: 90.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 29/500 [2:54:39<34:44:59, 265.60s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.00%, p_true_acc=90.00%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=86.67%, topk_entropy_acc=86.67%, window_entropy_acc=90.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 30/500 [2:54:39<36:44:42, 281.45s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.00%, p_true_acc=90.00%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=86.67%, topk_entropy_acc=86.67%, window_entropy_acc=90.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 7.739902175991697
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 7.739902175991697
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 14.999293208122253
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 0.9375
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 11.8828125
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 2.0565803200006485
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 1.3860709518194199
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 1.3830821067094803
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. There are 4 roses in the vase.
2. There ar...
    Score: 5.879612356424332
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 90.32258064516128
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.32258064516128
Method name: cer_prob_product_log_last, running accuracy: 90.32258064516128
Method name: self_consistency, running accuracy: 90.32258064516128
Method name: p_true, running accuracy: 90.32258064516128
Method name: normilized_likelihood, running accuracy: 90.32258064516128
Method name: normilized_entropy, running accuracy: 87.09677419354838
Method name: topk_entropy, running accuracy: 87.09677419354838
Method name: window_entropy, running accuracy: 90.32258064516128

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 30/500 [2:58:00<36:44:42, 281.45s/it, attention_weighted_confidence_acc=90.32%, cer_entropy_weighted_mean_all_acc=90.32%, cer_prob_product_log_last_acc=90.32%, self_consistency_acc=90.32%, p_true_acc=90.32%, normilized_likelihood_acc=90.32%, normilized_entropy_acc=87.10%, topk_entropy_acc=87.10%, window_entropy_acc=90.32%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 31/500 [2:58:00<33:29:25, 257.07s/it, attention_weighted_confidence_acc=90.32%, cer_entropy_weighted_mean_all_acc=90.32%, cer_prob_product_log_last_acc=90.32%, self_consistency_acc=90.32%, p_true_acc=90.32%, normilized_likelihood_acc=90.32%, normilized_entropy_acc=87.10%, topk_entropy_acc=87.10%, window_entropy_acc=90.32%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 8.337863217284246
    Answer: 147
    Ground truth:  147
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 8.337863217284246
    Answer: 147
    Ground truth:  147
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 15.843622982501984
    Answer: 147
    Ground truth:  147
Method 4: self_consistency
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 1.0
    Answer: 147
    Ground truth:  147
Method 5: p_true
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 14.125
    Answer: 147
    Ground truth:  147
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 2.5094283372163773
    Answer: 147
    Ground truth:  147
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 0.8008087873458862
    Answer: 147
    Ground truth:  147
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 0.7986541986465454
    Answer: 147
    Ground truth:  147
Method 9: window_entropy
  Batch 1:
    Text: To find out how many kilograms of dog food they need in a week, we first need to...
    Score: 2.887737810611725
    Answer: 147
    Ground truth:  147
Method name: attention_weighted_confidence, running accuracy: 90.625
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.625
Method name: cer_prob_product_log_last, running accuracy: 90.625
Method name: self_consistency, running accuracy: 90.625
Method name: p_true, running accuracy: 90.625
Method name: normilized_likelihood, running accuracy: 90.625
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 90.625

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 31/500 [3:02:41<33:29:25, 257.07s/it, attention_weighted_confidence_acc=90.62%, cer_entropy_weighted_mean_all_acc=90.62%, cer_prob_product_log_last_acc=90.62%, self_consistency_acc=90.62%, p_true_acc=90.62%, normilized_likelihood_acc=90.62%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=90.62%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▋         | 32/500 [3:02:41<34:21:31, 264.30s/it, attention_weighted_confidence_acc=90.62%, cer_entropy_weighted_mean_all_acc=90.62%, cer_prob_product_log_last_acc=90.62%, self_consistency_acc=90.62%, p_true_acc=90.62%, normilized_likelihood_acc=90.62%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=90.62%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 4.9726364013086615
    Answer: 36
    Ground truth:  36
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 4.9726364013086615
    Answer: 36
    Ground truth:  36
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 9.999791145324707
    Answer: 36
    Ground truth:  36
Method 4: self_consistency
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 0.625
    Answer: 36
    Ground truth:  36
Method 5: p_true
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 8.953125
    Answer: 36
    Ground truth:  36
Method 6: normilized_likelihood
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 3.656905919313431
    Answer: 36
    Ground truth:  36
Method 7: normilized_entropy
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 3.525509923696518
    Answer: 36
    Ground truth:  36
Method 8: topk_entropy
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 3.1816777884960175
    Answer: 36
    Ground truth:  36
Method 9: window_entropy
  Batch 1:
    Text: To find Gerald's new time, let's break it down step by step:

1. First, we need ...
    Score: 14.718888700008392
    Answer: 36
    Ground truth:  36
Method name: attention_weighted_confidence, running accuracy: 90.9090909090909
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.9090909090909
Method name: cer_prob_product_log_last, running accuracy: 90.9090909090909
Method name: self_consistency, running accuracy: 90.9090909090909
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 90.9090909090909
Method name: normilized_entropy, running accuracy: 87.87878787878788
Method name: topk_entropy, running accuracy: 87.87878787878788
Method name: window_entropy, running accuracy: 90.9090909090909

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▋         | 32/500 [3:07:36<34:21:31, 264.30s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.91%, self_consistency_acc=90.91%, p_true_acc=90.91%, normilized_likelihood_acc=90.91%, normilized_entropy_acc=87.88%, topk_entropy_acc=87.88%, window_entropy_acc=90.91%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 33/500 [3:07:36<35:28:20, 273.45s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.91%, self_consistency_acc=90.91%, p_true_acc=90.91%, normilized_likelihood_acc=90.91%, normilized_entropy_acc=87.88%, topk_entropy_acc=87.88%, window_entropy_acc=90.91%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 3.8847369919950214
    Answer: 168000
    Ground truth:  168000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 3.8847369919950214
    Answer: 168000
    Ground truth:  168000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 1.064521465623868
    Answer: 168000
    Ground truth:  168000
Method 4: self_consistency
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 0.5
    Answer: 168000
    Ground truth:  168000
Method 5: p_true
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 6.79296875
    Answer: 168000
    Ground truth:  168000
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 5.008231207728386
    Answer: 168000
    Ground truth:  168000
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 4.8850632309913635
    Answer: 168000
    Ground truth:  168000
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 4.324895069003105
    Answer: 168000
    Ground truth:  168000
Method 9: window_entropy
  Batch 1:
    Text: To calculate the total amount of money the company paid to the employees in the ...
    Score: 9.225467681884766
    Answer: 168000
    Ground truth:  168000
Method name: attention_weighted_confidence, running accuracy: 91.17647058823529
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.17647058823529
Method name: cer_prob_product_log_last, running accuracy: 91.17647058823529
Method name: self_consistency, running accuracy: 91.17647058823529
Method name: p_true, running accuracy: 91.17647058823529
Method name: normilized_likelihood, running accuracy: 91.17647058823529
Method name: normilized_entropy, running accuracy: 88.23529411764706
Method name: topk_entropy, running accuracy: 88.23529411764706
Method name: window_entropy, running accuracy: 91.17647058823529

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 33/500 [3:16:11<35:28:20, 273.45s/it, attention_weighted_confidence_acc=91.18%, cer_entropy_weighted_mean_all_acc=91.18%, cer_prob_product_log_last_acc=91.18%, self_consistency_acc=91.18%, p_true_acc=91.18%, normilized_likelihood_acc=91.18%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=91.18%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 34/500 [3:16:11<44:47:10, 345.99s/it, attention_weighted_confidence_acc=91.18%, cer_entropy_weighted_mean_all_acc=91.18%, cer_prob_product_log_last_acc=91.18%, self_consistency_acc=91.18%, p_true_acc=91.18%, normilized_likelihood_acc=91.18%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=91.18%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 8.067806466864916
    Answer: 70
    Ground truth:  70
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 8.067806466864916
    Answer: 70
    Ground truth:  70
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 15.979905009269714
    Answer: 70
    Ground truth:  70
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 1.0
    Answer: 70
    Ground truth:  70
Method 5: p_true
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 14.2421875
    Answer: 70
    Ground truth:  70
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 3.75737664103508
    Answer: 70
    Ground truth:  70
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 2.6814188808202744
    Answer: 70
    Ground truth:  70
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 2.581770643591881
    Answer: 70
    Ground truth:  70
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of holes in all the buttons, we need to calculate the h...
    Score: 9.671867102384567
    Answer: 70
    Ground truth:  70
Method name: attention_weighted_confidence, running accuracy: 91.42857142857143
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.42857142857143
Method name: cer_prob_product_log_last, running accuracy: 91.42857142857143
Method name: self_consistency, running accuracy: 91.42857142857143
Method name: p_true, running accuracy: 91.42857142857143
Method name: normilized_likelihood, running accuracy: 91.42857142857143
Method name: normilized_entropy, running accuracy: 88.57142857142857
Method name: topk_entropy, running accuracy: 88.57142857142857
Method name: window_entropy, running accuracy: 91.42857142857143

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 34/500 [3:20:41<44:47:10, 345.99s/it, attention_weighted_confidence_acc=91.43%, cer_entropy_weighted_mean_all_acc=91.43%, cer_prob_product_log_last_acc=91.43%, self_consistency_acc=91.43%, p_true_acc=91.43%, normilized_likelihood_acc=91.43%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=91.43%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 35/500 [3:20:41<41:44:36, 323.18s/it, attention_weighted_confidence_acc=91.43%, cer_entropy_weighted_mean_all_acc=91.43%, cer_prob_product_log_last_acc=91.43%, self_consistency_acc=91.43%, p_true_acc=91.43%, normilized_likelihood_acc=91.43%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=91.43%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 8.877556339832744
    Answer: 33.0
    Ground truth:  33
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 8.877556339832744
    Answer: 33.0
    Ground truth:  33
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 0.0
    Answer: 33.0
    Ground truth:  33
Method 4: self_consistency
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 1.0
    Answer: 33.0
    Ground truth:  33
Method 5: p_true
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 10.77734375
    Answer: 33.0
    Ground truth:  33
Method 6: normilized_likelihood
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 1.064401537179947
    Answer: 33.0
    Ground truth:  33
Method 7: normilized_entropy
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 0.8884471580386162
    Answer: 33.0
    Ground truth:  33
Method 8: topk_entropy
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 0.8719971254467964
    Answer: 33.0
    Ground truth:  33
Method 9: window_entropy
  Batch 1:
    Text: First, let's break down the costs into categories.

1. The initial purchase of t...
    Score: 5.918686479330063
    Answer: 33.0
    Ground truth:  33
Method name: attention_weighted_confidence, running accuracy: 91.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 91.66666666666666
Method name: self_consistency, running accuracy: 91.66666666666666
Method name: p_true, running accuracy: 91.66666666666666
Method name: normilized_likelihood, running accuracy: 91.66666666666666
Method name: normilized_entropy, running accuracy: 88.88888888888889
Method name: topk_entropy, running accuracy: 88.88888888888889
Method name: window_entropy, running accuracy: 91.66666666666666

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 35/500 [3:26:16<41:44:36, 323.18s/it, attention_weighted_confidence_acc=91.67%, cer_entropy_weighted_mean_all_acc=91.67%, cer_prob_product_log_last_acc=91.67%, self_consistency_acc=91.67%, p_true_acc=91.67%, normilized_likelihood_acc=91.67%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=91.67%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 36/500 [3:26:16<42:06:30, 326.70s/it, attention_weighted_confidence_acc=91.67%, cer_entropy_weighted_mean_all_acc=91.67%, cer_prob_product_log_last_acc=91.67%, self_consistency_acc=91.67%, p_true_acc=91.67%, normilized_likelihood_acc=91.67%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=91.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 5.902854670173092
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 5.902854670173092
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 11.974827527999878
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 0.75
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 11.11328125
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 1.5442015379667282
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 1.0582583993673325
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 1.051878347992897
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Hallie had dance practice for 1 hour on Tu...
    Score: 4.867229163646698
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 91.8918918918919
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.8918918918919
Method name: cer_prob_product_log_last, running accuracy: 91.8918918918919
Method name: self_consistency, running accuracy: 91.8918918918919
Method name: p_true, running accuracy: 91.8918918918919
Method name: normilized_likelihood, running accuracy: 91.8918918918919
Method name: normilized_entropy, running accuracy: 89.1891891891892
Method name: topk_entropy, running accuracy: 89.1891891891892
Method name: window_entropy, running accuracy: 91.8918918918919

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 36/500 [3:29:37<42:06:30, 326.70s/it, attention_weighted_confidence_acc=91.89%, cer_entropy_weighted_mean_all_acc=91.89%, cer_prob_product_log_last_acc=91.89%, self_consistency_acc=91.89%, p_true_acc=91.89%, normilized_likelihood_acc=91.89%, normilized_entropy_acc=89.19%, topk_entropy_acc=89.19%, window_entropy_acc=91.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 37/500 [3:29:37<37:11:14, 289.14s/it, attention_weighted_confidence_acc=91.89%, cer_entropy_weighted_mean_all_acc=91.89%, cer_prob_product_log_last_acc=91.89%, self_consistency_acc=91.89%, p_true_acc=91.89%, normilized_likelihood_acc=91.89%, normilized_entropy_acc=89.19%, topk_entropy_acc=89.19%, window_entropy_acc=91.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 7.61297065673731
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 7.61297065673731
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 14.027481999248266
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 0.9375
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 13.2109375
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 7.1112358421087265
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 4.634442746639252
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 4.419409364461899
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: To find the number of girl Pomeranians Jana has, follow these steps:

1. First, ...
    Score: 12.443731963634491
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 92.10526315789474
Method name: cer_entropy_weighted_mean_all, running accuracy: 92.10526315789474
Method name: cer_prob_product_log_last, running accuracy: 92.10526315789474
Method name: self_consistency, running accuracy: 92.10526315789474
Method name: p_true, running accuracy: 92.10526315789474
Method name: normilized_likelihood, running accuracy: 92.10526315789474
Method name: normilized_entropy, running accuracy: 89.47368421052632
Method name: topk_entropy, running accuracy: 89.47368421052632
Method name: window_entropy, running accuracy: 92.10526315789474

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 37/500 [3:33:35<37:11:14, 289.14s/it, attention_weighted_confidence_acc=92.11%, cer_entropy_weighted_mean_all_acc=92.11%, cer_prob_product_log_last_acc=92.11%, self_consistency_acc=92.11%, p_true_acc=92.11%, normilized_likelihood_acc=92.11%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=92.11%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 38/500 [3:33:35<35:08:36, 273.85s/it, attention_weighted_confidence_acc=92.11%, cer_entropy_weighted_mean_all_acc=92.11%, cer_prob_product_log_last_acc=92.11%, self_consistency_acc=92.11%, p_true_acc=92.11%, normilized_likelihood_acc=92.11%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=92.11%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 7.888321647570637
    Answer: 240
    Ground truth:  150
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 7.888321647570637
    Answer: 240
    Ground truth:  150
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 14.836699962615967
    Answer: 240
    Ground truth:  150
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 1.0
    Answer: 240
    Ground truth:  150
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 12.60546875
    Answer: 240
    Ground truth:  150
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 2.0137085765600204
    Answer: 240
    Ground truth:  150
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 0.6931616961956024
    Answer: 240
    Ground truth:  150
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 0.689922884106636
    Answer: 240
    Ground truth:  150
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Marin and Nancy each...
    Score: 3.633166342973709
    Answer: 240
    Ground truth:  150
Method name: attention_weighted_confidence, running accuracy: 89.74358974358975
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.74358974358975
Method name: cer_prob_product_log_last, running accuracy: 89.74358974358975
Method name: self_consistency, running accuracy: 89.74358974358975
Method name: p_true, running accuracy: 89.74358974358975
Method name: normilized_likelihood, running accuracy: 89.74358974358975
Method name: normilized_entropy, running accuracy: 87.17948717948718
Method name: topk_entropy, running accuracy: 87.17948717948718
Method name: window_entropy, running accuracy: 89.74358974358975

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 38/500 [3:36:37<35:08:36, 273.85s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=89.74%, self_consistency_acc=89.74%, p_true_acc=89.74%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=87.18%, topk_entropy_acc=87.18%, window_entropy_acc=89.74%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 39/500 [3:36:37<31:32:14, 246.28s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=89.74%, self_consistency_acc=89.74%, p_true_acc=89.74%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=87.18%, topk_entropy_acc=87.18%, window_entropy_acc=89.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 7.1678682435770265
    Answer: 30
    Ground truth:  35
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 7.1678682435770265
    Answer: 30
    Ground truth:  35
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 14.999359726905823
    Answer: 30
    Ground truth:  35
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 0.9375
    Answer: 30
    Ground truth:  35
Method 5: p_true
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 11.451171875
    Answer: 30
    Ground truth:  35
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 3.4747759103775024
    Answer: 30
    Ground truth:  35
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 3.833482176065445
    Answer: 30
    Ground truth:  35
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 3.636874198913574
    Answer: 30
    Ground truth:  35
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, we need to follow these steps:

1. The total dinner bill i...
    Score: 9.43956470489502
    Answer: 30
    Ground truth:  35
Method name: attention_weighted_confidence, running accuracy: 87.5
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.5
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 87.5
Method name: p_true, running accuracy: 87.5
Method name: normilized_likelihood, running accuracy: 87.5
Method name: normilized_entropy, running accuracy: 85.0
Method name: topk_entropy, running accuracy: 85.0
Method name: window_entropy, running accuracy: 87.5

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 39/500 [3:40:12<31:32:14, 246.28s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=87.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 40/500 [3:40:12<30:15:04, 236.75s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=87.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 8.388756738372297
    Answer: 36
    Ground truth:  36
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 8.388756738372297
    Answer: 36
    Ground truth:  36
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 15.999987721443176
    Answer: 36
    Ground truth:  36
Method 4: self_consistency
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 1.0
    Answer: 36
    Ground truth:  36
Method 5: p_true
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 15.109375
    Answer: 36
    Ground truth:  36
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 1.4298048913478851
    Answer: 36
    Ground truth:  36
Method 7: normilized_entropy
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 1.3661188334226608
    Answer: 36
    Ground truth:  36
Method 8: topk_entropy
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 1.1778914779424667
    Answer: 36
    Ground truth:  36
Method 9: window_entropy
  Batch 1:
    Text: To find the price of the toy after it was discounted in January, we'll break it ...
    Score: 17.94930651783943
    Answer: 36
    Ground truth:  36
Method name: attention_weighted_confidence, running accuracy: 87.8048780487805
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.8048780487805
Method name: cer_prob_product_log_last, running accuracy: 87.8048780487805
Method name: self_consistency, running accuracy: 87.8048780487805
Method name: p_true, running accuracy: 87.8048780487805
Method name: normilized_likelihood, running accuracy: 87.8048780487805
Method name: normilized_entropy, running accuracy: 85.36585365853658
Method name: topk_entropy, running accuracy: 85.36585365853658
Method name: window_entropy, running accuracy: 87.8048780487805

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 40/500 [3:44:28<30:15:04, 236.75s/it, attention_weighted_confidence_acc=87.80%, cer_entropy_weighted_mean_all_acc=87.80%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=87.80%, p_true_acc=87.80%, normilized_likelihood_acc=87.80%, normilized_entropy_acc=85.37%, topk_entropy_acc=85.37%, window_entropy_acc=87.80%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 41/500 [3:44:28<30:56:39, 242.70s/it, attention_weighted_confidence_acc=87.80%, cer_entropy_weighted_mean_all_acc=87.80%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=87.80%, p_true_acc=87.80%, normilized_likelihood_acc=87.80%, normilized_entropy_acc=85.37%, topk_entropy_acc=85.37%, window_entropy_acc=87.80%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 7.914968128568161
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 7.914968128568161
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 15.999959826469421
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 1.0
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 14.3671875
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 1.723361685872078
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 1.7346771210432053
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 1.711852952837944
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Bennett should sell each ear of corn for, let's first calcu...
    Score: 3.735003113746643
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 88.09523809523809
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.09523809523809
Method name: cer_prob_product_log_last, running accuracy: 88.09523809523809
Method name: self_consistency, running accuracy: 88.09523809523809
Method name: p_true, running accuracy: 88.09523809523809
Method name: normilized_likelihood, running accuracy: 88.09523809523809
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 88.09523809523809

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 41/500 [3:48:50<30:56:39, 242.70s/it, attention_weighted_confidence_acc=88.10%, cer_entropy_weighted_mean_all_acc=88.10%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=88.10%, p_true_acc=88.10%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=88.10%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 42/500 [3:48:50<31:36:01, 248.39s/it, attention_weighted_confidence_acc=88.10%, cer_entropy_weighted_mean_all_acc=88.10%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=88.10%, p_true_acc=88.10%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=88.10%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 8.425378411278722
    Answer: 78
    Ground truth:  78
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 8.425378411278722
    Answer: 78
    Ground truth:  78
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 15.99908447265625
    Answer: 78
    Ground truth:  78
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 1.0
    Answer: 78
    Ground truth:  78
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 15.00390625
    Answer: 78
    Ground truth:  78
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 3.8387947231531143
    Answer: 78
    Ground truth:  78
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 2.5950446724891663
    Answer: 78
    Ground truth:  78
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 2.5785862803459167
    Answer: 78
    Ground truth:  78
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. There are 15 red cards, and 60% m...
    Score: 4.570040941238403
    Answer: 78
    Ground truth:  78
Method name: attention_weighted_confidence, running accuracy: 88.37209302325581
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.37209302325581
Method name: cer_prob_product_log_last, running accuracy: 88.37209302325581
Method name: self_consistency, running accuracy: 88.37209302325581
Method name: p_true, running accuracy: 88.37209302325581
Method name: normilized_likelihood, running accuracy: 88.37209302325581
Method name: normilized_entropy, running accuracy: 86.04651162790698
Method name: topk_entropy, running accuracy: 86.04651162790698
Method name: window_entropy, running accuracy: 88.37209302325581

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 42/500 [3:54:14<31:36:01, 248.39s/it, attention_weighted_confidence_acc=88.37%, cer_entropy_weighted_mean_all_acc=88.37%, cer_prob_product_log_last_acc=88.37%, self_consistency_acc=88.37%, p_true_acc=88.37%, normilized_likelihood_acc=88.37%, normilized_entropy_acc=86.05%, topk_entropy_acc=86.05%, window_entropy_acc=88.37%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▊         | 43/500 [3:54:14<34:23:24, 270.91s/it, attention_weighted_confidence_acc=88.37%, cer_entropy_weighted_mean_all_acc=88.37%, cer_prob_product_log_last_acc=88.37%, self_consistency_acc=88.37%, p_true_acc=88.37%, normilized_likelihood_acc=88.37%, normilized_entropy_acc=86.05%, topk_entropy_acc=86.05%, window_entropy_acc=88.37%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 8.181536590999913
    Answer: 243
    Ground truth:  243
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 8.181536590999913
    Answer: 243
    Ground truth:  243
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 14.999595522880554
    Answer: 243
    Ground truth:  243
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 0.9375
    Answer: 243
    Ground truth:  243
Method 5: p_true
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 12.9453125
    Answer: 243
    Ground truth:  243
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 2.65276088565588
    Answer: 243
    Ground truth:  243
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 1.350025825202465
    Answer: 243
    Ground truth:  243
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 1.3480638042092323
    Answer: 243
    Ground truth:  243
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount Mishka spent, we need to calculate the cost of each typ...
    Score: 5.242466181516647
    Answer: 243
    Ground truth:  243
Method name: attention_weighted_confidence, running accuracy: 88.63636363636364
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.63636363636364
Method name: cer_prob_product_log_last, running accuracy: 88.63636363636364
Method name: self_consistency, running accuracy: 88.63636363636364
Method name: p_true, running accuracy: 88.63636363636364
Method name: normilized_likelihood, running accuracy: 88.63636363636364
Method name: normilized_entropy, running accuracy: 86.36363636363636
Method name: topk_entropy, running accuracy: 86.36363636363636
Method name: window_entropy, running accuracy: 88.63636363636364

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▊         | 43/500 [3:58:52<34:23:24, 270.91s/it, attention_weighted_confidence_acc=88.64%, cer_entropy_weighted_mean_all_acc=88.64%, cer_prob_product_log_last_acc=88.64%, self_consistency_acc=88.64%, p_true_acc=88.64%, normilized_likelihood_acc=88.64%, normilized_entropy_acc=86.36%, topk_entropy_acc=86.36%, window_entropy_acc=88.64%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 44/500 [3:58:52<34:35:27, 273.09s/it, attention_weighted_confidence_acc=88.64%, cer_entropy_weighted_mean_all_acc=88.64%, cer_prob_product_log_last_acc=88.64%, self_consistency_acc=88.64%, p_true_acc=88.64%, normilized_likelihood_acc=88.64%, normilized_entropy_acc=86.36%, topk_entropy_acc=86.36%, window_entropy_acc=88.64%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 2.4140825444612966
    Answer: 162
    Ground truth:  342
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 2.4140825444612966
    Answer: 162
    Ground truth:  342
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 4.997619450092316
    Answer: 162
    Ground truth:  342
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 0.3125
    Answer: 162
    Ground truth:  342
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 4.57421875
    Answer: 162
    Ground truth:  342
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 6.2606698870658875
    Answer: 162
    Ground truth:  342
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 5.766821205615997
    Answer: 162
    Ground truth:  342
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 4.928251922130585
    Answer: 162
    Ground truth:  342
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to find out how many plants Pat needs to buy to f...
    Score: 6.583606660366058
    Answer: 162
    Ground truth:  342
Method name: attention_weighted_confidence, running accuracy: 86.66666666666667
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.66666666666667
Method name: cer_prob_product_log_last, running accuracy: 86.66666666666667
Method name: self_consistency, running accuracy: 86.66666666666667
Method name: p_true, running accuracy: 86.66666666666667
Method name: normilized_likelihood, running accuracy: 86.66666666666667
Method name: normilized_entropy, running accuracy: 84.44444444444444
Method name: topk_entropy, running accuracy: 84.44444444444444
Method name: window_entropy, running accuracy: 86.66666666666667

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 44/500 [4:10:04<34:35:27, 273.09s/it, attention_weighted_confidence_acc=86.67%, cer_entropy_weighted_mean_all_acc=86.67%, cer_prob_product_log_last_acc=86.67%, self_consistency_acc=86.67%, p_true_acc=86.67%, normilized_likelihood_acc=86.67%, normilized_entropy_acc=84.44%, topk_entropy_acc=84.44%, window_entropy_acc=86.67%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 45/500 [4:10:04<49:39:29, 392.90s/it, attention_weighted_confidence_acc=86.67%, cer_entropy_weighted_mean_all_acc=86.67%, cer_prob_product_log_last_acc=86.67%, self_consistency_acc=86.67%, p_true_acc=86.67%, normilized_likelihood_acc=86.67%, normilized_entropy_acc=84.44%, topk_entropy_acc=84.44%, window_entropy_acc=86.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 7.9261177710872595
    Answer: 300
    Ground truth:  300
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 7.9261177710872595
    Answer: 300
    Ground truth:  300
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 15.996272683143616
    Answer: 300
    Ground truth:  300
Method 4: self_consistency
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 1.0
    Answer: 300
    Ground truth:  300
Method 5: p_true
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 13.359375
    Answer: 300
    Ground truth:  300
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 3.2495208084583282
    Answer: 300
    Ground truth:  300
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 3.831368088722229
    Answer: 300
    Ground truth:  300
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 3.3715635240077972
    Answer: 300
    Ground truth:  300
Method 9: window_entropy
  Batch 1:
    Text: To find out how many lemons Tim gets in a decade, we need to calculate the total...
    Score: 18.631849974393845
    Answer: 300
    Ground truth:  300
Method name: attention_weighted_confidence, running accuracy: 86.95652173913044
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.95652173913044
Method name: cer_prob_product_log_last, running accuracy: 86.95652173913044
Method name: self_consistency, running accuracy: 86.95652173913044
Method name: p_true, running accuracy: 86.95652173913044
Method name: normilized_likelihood, running accuracy: 86.95652173913044
Method name: normilized_entropy, running accuracy: 84.78260869565217
Method name: topk_entropy, running accuracy: 84.78260869565217
Method name: window_entropy, running accuracy: 86.95652173913044

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 45/500 [4:13:52<49:39:29, 392.90s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=86.96%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=84.78%, topk_entropy_acc=84.78%, window_entropy_acc=86.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 46/500 [4:13:52<43:18:10, 343.37s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=86.96%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=84.78%, topk_entropy_acc=84.78%, window_entropy_acc=86.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 6.922974964616068
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 6.922974964616068
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 14.837960958480835
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 0.9375
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 14.3046875
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 10.817397028207779
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 9.561459600925446
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 8.435422733426094
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how long it will take for Tom to catch up with Bob, we need to under...
    Score: 16.73368811607361
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 87.2340425531915
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.2340425531915
Method name: cer_prob_product_log_last, running accuracy: 87.2340425531915
Method name: self_consistency, running accuracy: 87.2340425531915
Method name: p_true, running accuracy: 87.2340425531915
Method name: normilized_likelihood, running accuracy: 87.2340425531915
Method name: normilized_entropy, running accuracy: 85.1063829787234
Method name: topk_entropy, running accuracy: 85.1063829787234
Method name: window_entropy, running accuracy: 87.2340425531915

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 46/500 [4:18:57<43:18:10, 343.37s/it, attention_weighted_confidence_acc=87.23%, cer_entropy_weighted_mean_all_acc=87.23%, cer_prob_product_log_last_acc=87.23%, self_consistency_acc=87.23%, p_true_acc=87.23%, normilized_likelihood_acc=87.23%, normilized_entropy_acc=85.11%, topk_entropy_acc=85.11%, window_entropy_acc=87.23%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 47/500 [4:18:57<41:44:44, 331.75s/it, attention_weighted_confidence_acc=87.23%, cer_entropy_weighted_mean_all_acc=87.23%, cer_prob_product_log_last_acc=87.23%, self_consistency_acc=87.23%, p_true_acc=87.23%, normilized_likelihood_acc=87.23%, normilized_entropy_acc=85.11%, topk_entropy_acc=85.11%, window_entropy_acc=87.23%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 7.950554629336442
    Answer: 250
    Ground truth:  250
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 7.950554629336442
    Answer: 250
    Ground truth:  250
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 15.999913573265076
    Answer: 250
    Ground truth:  250
Method 4: self_consistency
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 1.0
    Answer: 250
    Ground truth:  250
Method 5: p_true
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 15.41796875
    Answer: 250
    Ground truth:  250
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 4.363578766584396
    Answer: 250
    Ground truth:  250
Method 7: normilized_entropy
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 2.5525064170360565
    Answer: 250
    Ground truth:  250
Method 8: topk_entropy
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 2.5351293981075287
    Answer: 250
    Ground truth:  250
Method 9: window_entropy
  Batch 1:
    Text: To find out John's profit, we need to first calculate his total income from rent...
    Score: 6.22966867685318
    Answer: 250
    Ground truth:  250
Method name: attention_weighted_confidence, running accuracy: 87.5
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.5
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 87.5
Method name: p_true, running accuracy: 87.5
Method name: normilized_likelihood, running accuracy: 87.5
Method name: normilized_entropy, running accuracy: 85.41666666666666
Method name: topk_entropy, running accuracy: 85.41666666666666
Method name: window_entropy, running accuracy: 87.5

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 47/500 [4:22:25<41:44:44, 331.75s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=85.42%, topk_entropy_acc=85.42%, window_entropy_acc=87.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|▉         | 48/500 [4:22:25<37:00:13, 294.72s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=85.42%, topk_entropy_acc=85.42%, window_entropy_acc=87.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 7.480214950814783
    Answer: 1050
    Ground truth:  1050
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 7.480214950814783
    Answer: 1050
    Ground truth:  1050
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 7.477283840455794
    Answer: 1050
    Ground truth:  1050
Method 4: self_consistency
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 0.9375
    Answer: 1050
    Ground truth:  1050
Method 5: p_true
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 13.75390625
    Answer: 1050
    Ground truth:  1050
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 1.9443491697311401
    Answer: 1050
    Ground truth:  1050
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 1.105695366859436
    Answer: 1050
    Ground truth:  1050
Method 8: topk_entropy
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 1.0960576832294464
    Answer: 1050
    Ground truth:  1050
Method 9: window_entropy
  Batch 1:
    Text: To find the total weekly production for the two cashiers, we need to calculate t...
    Score: 4.4450706243515015
    Answer: 1050
    Ground truth:  1050
Method name: attention_weighted_confidence, running accuracy: 87.75510204081633
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.75510204081633
Method name: cer_prob_product_log_last, running accuracy: 87.75510204081633
Method name: self_consistency, running accuracy: 87.75510204081633
Method name: p_true, running accuracy: 87.75510204081633
Method name: normilized_likelihood, running accuracy: 87.75510204081633
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 87.75510204081633

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|▉         | 48/500 [4:27:08<37:00:13, 294.72s/it, attention_weighted_confidence_acc=87.76%, cer_entropy_weighted_mean_all_acc=87.76%, cer_prob_product_log_last_acc=87.76%, self_consistency_acc=87.76%, p_true_acc=87.76%, normilized_likelihood_acc=87.76%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=87.76%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|▉         | 49/500 [4:27:08<36:29:42, 291.31s/it, attention_weighted_confidence_acc=87.76%, cer_entropy_weighted_mean_all_acc=87.76%, cer_prob_product_log_last_acc=87.76%, self_consistency_acc=87.76%, p_true_acc=87.76%, normilized_likelihood_acc=87.76%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=87.76%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 7.308655170128423
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 7.308655170128423
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 13.99909520149231
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 0.875
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 13.46484375
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 2.568144455552101
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 1.788690373301506
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 1.7451087087392807
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let's represent the number of ins...
    Score: 8.983396172523499
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 88.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.0
Method name: cer_prob_product_log_last, running accuracy: 88.0
Method name: self_consistency, running accuracy: 88.0
Method name: p_true, running accuracy: 88.0
Method name: normilized_likelihood, running accuracy: 88.0
Method name: normilized_entropy, running accuracy: 86.0
Method name: topk_entropy, running accuracy: 86.0
Method name: window_entropy, running accuracy: 88.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|▉         | 49/500 [4:33:47<36:29:42, 291.31s/it, attention_weighted_confidence_acc=88.00%, cer_entropy_weighted_mean_all_acc=88.00%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=88.00%, p_true_acc=88.00%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=86.00%, topk_entropy_acc=86.00%, window_entropy_acc=88.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 50/500 [4:33:47<40:27:10, 323.62s/it, attention_weighted_confidence_acc=88.00%, cer_entropy_weighted_mean_all_acc=88.00%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=88.00%, p_true_acc=88.00%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=86.00%, topk_entropy_acc=86.00%, window_entropy_acc=88.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 8.27042275836339
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 8.27042275836339
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 14.411726355552673
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 1.0
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 14.51171875
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 4.02194806933403
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 2.8186871111392975
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 2.7169404476881027
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: For a quadruple batch of brownies, Mark needs to buy a total of 3 * 4 = 12 cups ...
    Score: 10.636026442050934
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 88.23529411764706
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.23529411764706
Method name: cer_prob_product_log_last, running accuracy: 88.23529411764706
Method name: self_consistency, running accuracy: 88.23529411764706
Method name: p_true, running accuracy: 88.23529411764706
Method name: normilized_likelihood, running accuracy: 88.23529411764706
Method name: normilized_entropy, running accuracy: 86.27450980392157
Method name: topk_entropy, running accuracy: 86.27450980392157
Method name: window_entropy, running accuracy: 88.23529411764706

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 50/500 [4:39:25<40:27:10, 323.62s/it, attention_weighted_confidence_acc=88.24%, cer_entropy_weighted_mean_all_acc=88.24%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=88.24%, p_true_acc=88.24%, normilized_likelihood_acc=88.24%, normilized_entropy_acc=86.27%, topk_entropy_acc=86.27%, window_entropy_acc=88.24%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 51/500 [4:39:25<40:54:03, 327.94s/it, attention_weighted_confidence_acc=88.24%, cer_entropy_weighted_mean_all_acc=88.24%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=88.24%, p_true_acc=88.24%, normilized_likelihood_acc=88.24%, normilized_entropy_acc=86.27%, topk_entropy_acc=86.27%, window_entropy_acc=88.24%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 3.5929427581199462
    Answer: 40000
    Ground truth:  45000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 3.5929427581199462
    Answer: 40000
    Ground truth:  45000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the combined salary, we need to first find Valerie's brother's salary. 
...
    Score: 1.5297763137571856
    Answer: 45000
    Ground truth:  45000
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 0.4375
    Answer: 40000
    Ground truth:  45000
Method 5: p_true
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 5.30859375
    Answer: 40000
    Ground truth:  45000
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 1.2973526567220688
    Answer: 40000
    Ground truth:  45000
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 1.002267301082611
    Answer: 40000
    Ground truth:  45000
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 0.9948757886886597
    Answer: 40000
    Ground truth:  45000
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Let's assume that Valerie's brother earns X dollars per month. 
Step 2: ...
    Score: 2.6868746876716614
    Answer: 40000
    Ground truth:  45000
Method name: attention_weighted_confidence, running accuracy: 86.53846153846155
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.53846153846155
Method name: cer_prob_product_log_last, running accuracy: 88.46153846153845
Method name: self_consistency, running accuracy: 86.53846153846155
Method name: p_true, running accuracy: 86.53846153846155
Method name: normilized_likelihood, running accuracy: 86.53846153846155
Method name: normilized_entropy, running accuracy: 84.61538461538461
Method name: topk_entropy, running accuracy: 84.61538461538461
Method name: window_entropy, running accuracy: 86.53846153846155

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 51/500 [4:44:39<40:54:03, 327.94s/it, attention_weighted_confidence_acc=86.54%, cer_entropy_weighted_mean_all_acc=86.54%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=86.54%, p_true_acc=86.54%, normilized_likelihood_acc=86.54%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=86.54%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 52/500 [4:44:39<40:16:45, 323.67s/it, attention_weighted_confidence_acc=86.54%, cer_entropy_weighted_mean_all_acc=86.54%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=86.54%, p_true_acc=86.54%, normilized_likelihood_acc=86.54%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=86.54%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 7.4625541231419685
    Answer: 300
    Ground truth:  240
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 7.4625541231419685
    Answer: 300
    Ground truth:  240
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 14.999827265739441
    Answer: 300
    Ground truth:  240
Method 4: self_consistency
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 0.9375
    Answer: 300
    Ground truth:  240
Method 5: p_true
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 13.15234375
    Answer: 300
    Ground truth:  240
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 2.021570101380348
    Answer: 300
    Ground truth:  240
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 2.7121569514274597
    Answer: 300
    Ground truth:  240
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 2.6889263838529587
    Answer: 300
    Ground truth:  240
Method 9: window_entropy
  Batch 1:
    Text: To find out how much each roommate will pay per year, we need to break it down s...
    Score: 6.5805774331092834
    Answer: 300
    Ground truth:  240
Method name: attention_weighted_confidence, running accuracy: 84.90566037735849
Method name: cer_entropy_weighted_mean_all, running accuracy: 84.90566037735849
Method name: cer_prob_product_log_last, running accuracy: 86.79245283018868
Method name: self_consistency, running accuracy: 84.90566037735849
Method name: p_true, running accuracy: 84.90566037735849
Method name: normilized_likelihood, running accuracy: 84.90566037735849
Method name: normilized_entropy, running accuracy: 83.01886792452831
Method name: topk_entropy, running accuracy: 83.01886792452831
Method name: window_entropy, running accuracy: 84.90566037735849

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 52/500 [4:47:41<40:16:45, 323.67s/it, attention_weighted_confidence_acc=84.91%, cer_entropy_weighted_mean_all_acc=84.91%, cer_prob_product_log_last_acc=86.79%, self_consistency_acc=84.91%, p_true_acc=84.91%, normilized_likelihood_acc=84.91%, normilized_entropy_acc=83.02%, topk_entropy_acc=83.02%, window_entropy_acc=84.91%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 53/500 [4:47:41<34:55:40, 281.30s/it, attention_weighted_confidence_acc=84.91%, cer_entropy_weighted_mean_all_acc=84.91%, cer_prob_product_log_last_acc=86.79%, self_consistency_acc=84.91%, p_true_acc=84.91%, normilized_likelihood_acc=84.91%, normilized_entropy_acc=83.02%, topk_entropy_acc=83.02%, window_entropy_acc=84.91%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 8.317604740891992
    Answer: 70
    Ground truth:  70
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 8.317604740891992
    Answer: 70
    Ground truth:  70
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 15.999763131141663
    Answer: 70
    Ground truth:  70
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 1.0
    Answer: 70
    Ground truth:  70
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 15.234375
    Answer: 70
    Ground truth:  70
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 1.3124337643384933
    Answer: 70
    Ground truth:  70
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 0.7859659045934677
    Answer: 70
    Ground truth:  70
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 0.7832482606172562
    Answer: 70
    Ground truth:  70
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Calculate the time it took C...
    Score: 2.0671432614326477
    Answer: 70
    Ground truth:  70
Method name: attention_weighted_confidence, running accuracy: 85.18518518518519
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.18518518518519
Method name: cer_prob_product_log_last, running accuracy: 87.03703703703704
Method name: self_consistency, running accuracy: 85.18518518518519
Method name: p_true, running accuracy: 85.18518518518519
Method name: normilized_likelihood, running accuracy: 85.18518518518519
Method name: normilized_entropy, running accuracy: 83.33333333333334
Method name: topk_entropy, running accuracy: 83.33333333333334
Method name: window_entropy, running accuracy: 85.18518518518519

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 53/500 [4:51:45<34:55:40, 281.30s/it, attention_weighted_confidence_acc=85.19%, cer_entropy_weighted_mean_all_acc=85.19%, cer_prob_product_log_last_acc=87.04%, self_consistency_acc=85.19%, p_true_acc=85.19%, normilized_likelihood_acc=85.19%, normilized_entropy_acc=83.33%, topk_entropy_acc=83.33%, window_entropy_acc=85.19%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 54/500 [4:51:45<33:27:07, 270.02s/it, attention_weighted_confidence_acc=85.19%, cer_entropy_weighted_mean_all_acc=85.19%, cer_prob_product_log_last_acc=87.04%, self_consistency_acc=85.19%, p_true_acc=85.19%, normilized_likelihood_acc=85.19%, normilized_entropy_acc=83.33%, topk_entropy_acc=83.33%, window_entropy_acc=85.19%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 6.6374183849639286
    Answer: 76
    Ground truth:  76
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 6.6374183849639286
    Answer: 76
    Ground truth:  76
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 12.998874127864838
    Answer: 76
    Ground truth:  76
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 0.8125
    Answer: 76
    Ground truth:  76
Method 5: p_true
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 4.8623046875
    Answer: 76
    Ground truth:  76
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 9.156975612044334
    Answer: 76
    Ground truth:  76
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 7.064346641302109
    Answer: 76
    Ground truth:  76
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 6.144891783595085
    Answer: 76
    Ground truth:  76
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of functioning street lights, we need to find the total...
    Score: 20.177904844284058
    Answer: 76
    Ground truth:  76
Method name: attention_weighted_confidence, running accuracy: 85.45454545454545
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.45454545454545
Method name: cer_prob_product_log_last, running accuracy: 87.27272727272727
Method name: self_consistency, running accuracy: 85.45454545454545
Method name: p_true, running accuracy: 85.45454545454545
Method name: normilized_likelihood, running accuracy: 85.45454545454545
Method name: normilized_entropy, running accuracy: 83.63636363636363
Method name: topk_entropy, running accuracy: 83.63636363636363
Method name: window_entropy, running accuracy: 85.45454545454545

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 54/500 [4:55:53<33:27:07, 270.02s/it, attention_weighted_confidence_acc=85.45%, cer_entropy_weighted_mean_all_acc=85.45%, cer_prob_product_log_last_acc=87.27%, self_consistency_acc=85.45%, p_true_acc=85.45%, normilized_likelihood_acc=85.45%, normilized_entropy_acc=83.64%, topk_entropy_acc=83.64%, window_entropy_acc=85.45%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 55/500 [4:55:53<32:32:36, 263.27s/it, attention_weighted_confidence_acc=85.45%, cer_entropy_weighted_mean_all_acc=85.45%, cer_prob_product_log_last_acc=87.27%, self_consistency_acc=85.45%, p_true_acc=85.45%, normilized_likelihood_acc=85.45%, normilized_entropy_acc=83.64%, topk_entropy_acc=83.64%, window_entropy_acc=85.45%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 6.979838571394868
    Answer: 7300
    Ground truth:  7300
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 6.979838571394868
    Answer: 7300
    Ground truth:  7300
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 7.252110705143437
    Answer: 7300
    Ground truth:  7300
Method 4: self_consistency
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 0.9375
    Answer: 7300
    Ground truth:  7300
Method 5: p_true
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 14.30859375
    Answer: 7300
    Ground truth:  7300
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 7.421021446585655
    Answer: 7300
    Ground truth:  7300
Method 7: normilized_entropy
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 7.204667776823044
    Answer: 7300
    Ground truth:  7300
Method 8: topk_entropy
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 6.332482695579529
    Answer: 7300
    Ground truth:  7300
Method 9: window_entropy
  Batch 1:
    Text: To determine how many more bottles Hortex needs to produce, we first need to fin...
    Score: 17.12376767396927
    Answer: 7300
    Ground truth:  7300
Method name: attention_weighted_confidence, running accuracy: 85.71428571428571
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.71428571428571
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 85.71428571428571
Method name: p_true, running accuracy: 85.71428571428571
Method name: normilized_likelihood, running accuracy: 85.71428571428571
Method name: normilized_entropy, running accuracy: 83.92857142857143
Method name: topk_entropy, running accuracy: 83.92857142857143
Method name: window_entropy, running accuracy: 85.71428571428571

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 55/500 [5:01:22<32:32:36, 263.27s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=83.93%, topk_entropy_acc=83.93%, window_entropy_acc=85.71%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 56/500 [5:01:22<34:55:44, 283.21s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=83.93%, topk_entropy_acc=83.93%, window_entropy_acc=85.71%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 7.533167466834234
    Answer: 18
    Ground truth:  18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 7.533167466834234
    Answer: 18
    Ground truth:  18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 14.986613512039185
    Answer: 18
    Ground truth:  18
Method 4: self_consistency
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 0.9375
    Answer: 18
    Ground truth:  18
Method 5: p_true
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 12.69921875
    Answer: 18
    Ground truth:  18
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 1.4879489243030548
    Answer: 18
    Ground truth:  18
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 1.184806004166603
    Answer: 18
    Ground truth:  18
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 1.1700709909200668
    Answer: 18
    Ground truth:  18
Method 9: window_entropy
  Batch 1:
    Text: To find out how many vlogs Emma still needs to make, we need to first calculate ...
    Score: 6.119334131479263
    Answer: 18
    Ground truth:  18
Method name: attention_weighted_confidence, running accuracy: 85.96491228070175
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.96491228070175
Method name: cer_prob_product_log_last, running accuracy: 87.71929824561403
Method name: self_consistency, running accuracy: 85.96491228070175
Method name: p_true, running accuracy: 85.96491228070175
Method name: normilized_likelihood, running accuracy: 85.96491228070175
Method name: normilized_entropy, running accuracy: 84.21052631578947
Method name: topk_entropy, running accuracy: 84.21052631578947
Method name: window_entropy, running accuracy: 85.96491228070175

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 56/500 [5:05:17<34:55:44, 283.21s/it, attention_weighted_confidence_acc=85.96%, cer_entropy_weighted_mean_all_acc=85.96%, cer_prob_product_log_last_acc=87.72%, self_consistency_acc=85.96%, p_true_acc=85.96%, normilized_likelihood_acc=85.96%, normilized_entropy_acc=84.21%, topk_entropy_acc=84.21%, window_entropy_acc=85.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█▏        | 57/500 [5:05:17<33:04:17, 268.75s/it, attention_weighted_confidence_acc=85.96%, cer_entropy_weighted_mean_all_acc=85.96%, cer_prob_product_log_last_acc=87.72%, self_consistency_acc=85.96%, p_true_acc=85.96%, normilized_likelihood_acc=85.96%, normilized_entropy_acc=84.21%, topk_entropy_acc=84.21%, window_entropy_acc=85.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 5.9228230903367125
    Answer: 11
    Ground truth:  11
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 5.9228230903367125
    Answer: 11
    Ground truth:  11
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 11.998061954975128
    Answer: 11
    Ground truth:  11
Method 4: self_consistency
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 0.75
    Answer: 11
    Ground truth:  11
Method 5: p_true
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 9.42578125
    Answer: 11
    Ground truth:  11
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 4.720017522573471
    Answer: 11
    Ground truth:  11
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 4.824443846940994
    Answer: 11
    Ground truth:  11
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 4.510622203350067
    Answer: 11
    Ground truth:  11
Method 9: window_entropy
  Batch 1:
    Text: To find out how many cups of flour Carla will need, we need to first determine h...
    Score: 13.253152668476105
    Answer: 11
    Ground truth:  11
Method name: attention_weighted_confidence, running accuracy: 86.20689655172413
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.20689655172413
Method name: cer_prob_product_log_last, running accuracy: 87.93103448275862
Method name: self_consistency, running accuracy: 86.20689655172413
Method name: p_true, running accuracy: 86.20689655172413
Method name: normilized_likelihood, running accuracy: 86.20689655172413
Method name: normilized_entropy, running accuracy: 84.48275862068965
Method name: topk_entropy, running accuracy: 84.48275862068965
Method name: window_entropy, running accuracy: 86.20689655172413

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█▏        | 57/500 [5:11:40<33:04:17, 268.75s/it, attention_weighted_confidence_acc=86.21%, cer_entropy_weighted_mean_all_acc=86.21%, cer_prob_product_log_last_acc=87.93%, self_consistency_acc=86.21%, p_true_acc=86.21%, normilized_likelihood_acc=86.21%, normilized_entropy_acc=84.48%, topk_entropy_acc=84.48%, window_entropy_acc=86.21%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 58/500 [5:11:40<37:11:03, 302.86s/it, attention_weighted_confidence_acc=86.21%, cer_entropy_weighted_mean_all_acc=86.21%, cer_prob_product_log_last_acc=87.93%, self_consistency_acc=86.21%, p_true_acc=86.21%, normilized_likelihood_acc=86.21%, normilized_entropy_acc=84.48%, topk_entropy_acc=84.48%, window_entropy_acc=86.21%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 7.310741260419719
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 7.310741260419719
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 14.999807834625244
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 0.9375
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 11.73046875
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 4.461425676941872
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 4.962030082941055
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 4.520654022693634
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Each table has 4 legs, and each l...
    Score: 21.882070243358612
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 86.4406779661017
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.4406779661017
Method name: cer_prob_product_log_last, running accuracy: 88.13559322033898
Method name: self_consistency, running accuracy: 86.4406779661017
Method name: p_true, running accuracy: 86.4406779661017
Method name: normilized_likelihood, running accuracy: 86.4406779661017
Method name: normilized_entropy, running accuracy: 84.7457627118644
Method name: topk_entropy, running accuracy: 84.7457627118644
Method name: window_entropy, running accuracy: 86.4406779661017

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 58/500 [5:15:04<37:11:03, 302.86s/it, attention_weighted_confidence_acc=86.44%, cer_entropy_weighted_mean_all_acc=86.44%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=86.44%, p_true_acc=86.44%, normilized_likelihood_acc=86.44%, normilized_entropy_acc=84.75%, topk_entropy_acc=84.75%, window_entropy_acc=86.44%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 59/500 [5:15:04<33:27:34, 273.14s/it, attention_weighted_confidence_acc=86.44%, cer_entropy_weighted_mean_all_acc=86.44%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=86.44%, p_true_acc=86.44%, normilized_likelihood_acc=86.44%, normilized_entropy_acc=84.75%, topk_entropy_acc=84.75%, window_entropy_acc=86.44%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 5.718459050956188
    Answer: 350
    Ground truth:  350
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 5.718459050956188
    Answer: 350
    Ground truth:  350
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 10.980476140975952
    Answer: 350
    Ground truth:  350
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 0.6875
    Answer: 350
    Ground truth:  350
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 9.61328125
    Answer: 350
    Ground truth:  350
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 2.867239326238632
    Answer: 350
    Ground truth:  350
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 2.698017969727516
    Answer: 350
    Ground truth:  350
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 2.6055570244789124
    Answer: 350
    Ground truth:  350
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll go step by step.

1. Zaid spends 1/4 of his salary ...
    Score: 9.106064140796661
    Answer: 350
    Ground truth:  350
Method name: attention_weighted_confidence, running accuracy: 86.66666666666667
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.66666666666667
Method name: cer_prob_product_log_last, running accuracy: 88.33333333333333
Method name: self_consistency, running accuracy: 86.66666666666667
Method name: p_true, running accuracy: 86.66666666666667
Method name: normilized_likelihood, running accuracy: 86.66666666666667
Method name: normilized_entropy, running accuracy: 85.0
Method name: topk_entropy, running accuracy: 85.0
Method name: window_entropy, running accuracy: 86.66666666666667

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 59/500 [5:21:42<33:27:34, 273.14s/it, attention_weighted_confidence_acc=86.67%, cer_entropy_weighted_mean_all_acc=86.67%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=86.67%, p_true_acc=86.67%, normilized_likelihood_acc=86.67%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=86.67%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 60/500 [5:21:42<37:58:53, 310.76s/it, attention_weighted_confidence_acc=86.67%, cer_entropy_weighted_mean_all_acc=86.67%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=86.67%, p_true_acc=86.67%, normilized_likelihood_acc=86.67%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=86.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 7.992928696762728
    Answer: 14000
    Ground truth:  14.000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 7.992928696762728
    Answer: 14000
    Ground truth:  14.000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 0.021037298371275215
    Answer: 14000
    Ground truth:  14.000
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 0.9375
    Answer: 14000
    Ground truth:  14.000
Method 5: p_true
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 12.65625
    Answer: 14000
    Ground truth:  14.000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 1.7827468812465668
    Answer: 14000
    Ground truth:  14.000
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 2.3851037323474884
    Answer: 14000
    Ground truth:  14.000
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 2.214352548122406
    Answer: 14000
    Ground truth:  14.000
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Boris produces in the fourth week, we need to first find ou...
    Score: 11.148723185062408
    Answer: 14000
    Ground truth:  14.000
Method name: attention_weighted_confidence, running accuracy: 85.24590163934425
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.24590163934425
Method name: cer_prob_product_log_last, running accuracy: 86.88524590163934
Method name: self_consistency, running accuracy: 85.24590163934425
Method name: p_true, running accuracy: 85.24590163934425
Method name: normilized_likelihood, running accuracy: 85.24590163934425
Method name: normilized_entropy, running accuracy: 83.60655737704919
Method name: topk_entropy, running accuracy: 83.60655737704919
Method name: window_entropy, running accuracy: 85.24590163934425

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 60/500 [5:26:59<37:58:53, 310.76s/it, attention_weighted_confidence_acc=85.25%, cer_entropy_weighted_mean_all_acc=85.25%, cer_prob_product_log_last_acc=86.89%, self_consistency_acc=85.25%, p_true_acc=85.25%, normilized_likelihood_acc=85.25%, normilized_entropy_acc=83.61%, topk_entropy_acc=83.61%, window_entropy_acc=85.25%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 61/500 [5:26:59<38:06:40, 312.53s/it, attention_weighted_confidence_acc=85.25%, cer_entropy_weighted_mean_all_acc=85.25%, cer_prob_product_log_last_acc=86.89%, self_consistency_acc=85.25%, p_true_acc=85.25%, normilized_likelihood_acc=85.25%, normilized_entropy_acc=83.61%, topk_entropy_acc=83.61%, window_entropy_acc=85.25%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 3.4219579080991136
    Answer: 14
    Ground truth:  14
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 3.4219579080991136
    Answer: 14
    Ground truth:  14
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 6.255762949585915
    Answer: 14
    Ground truth:  14
Method 4: self_consistency
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 0.5
    Answer: 14
    Ground truth:  14
Method 5: p_true
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 6.73828125
    Answer: 14
    Ground truth:  14
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 5.735438674688339
    Answer: 14
    Ground truth:  14
Method 7: normilized_entropy
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 5.7149136662483215
    Answer: 14
    Ground truth:  14
Method 8: topk_entropy
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 4.825063049793243
    Answer: 14
    Ground truth:  14
Method 9: window_entropy
  Batch 1:
    Text: Let's analyze the situation step by step.

1. Each person can trade 2 old record...
    Score: 6.392305612564087
    Answer: 14
    Ground truth:  14
Method name: attention_weighted_confidence, running accuracy: 85.48387096774194
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.48387096774194
Method name: cer_prob_product_log_last, running accuracy: 87.09677419354838
Method name: self_consistency, running accuracy: 85.48387096774194
Method name: p_true, running accuracy: 85.48387096774194
Method name: normilized_likelihood, running accuracy: 85.48387096774194
Method name: normilized_entropy, running accuracy: 83.87096774193549
Method name: topk_entropy, running accuracy: 83.87096774193549
Method name: window_entropy, running accuracy: 85.48387096774194

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 61/500 [5:35:58<38:06:40, 312.53s/it, attention_weighted_confidence_acc=85.48%, cer_entropy_weighted_mean_all_acc=85.48%, cer_prob_product_log_last_acc=87.10%, self_consistency_acc=85.48%, p_true_acc=85.48%, normilized_likelihood_acc=85.48%, normilized_entropy_acc=83.87%, topk_entropy_acc=83.87%, window_entropy_acc=85.48%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 62/500 [5:35:58<46:16:50, 380.39s/it, attention_weighted_confidence_acc=85.48%, cer_entropy_weighted_mean_all_acc=85.48%, cer_prob_product_log_last_acc=87.10%, self_consistency_acc=85.48%, p_true_acc=85.48%, normilized_likelihood_acc=85.48%, normilized_entropy_acc=83.87%, topk_entropy_acc=83.87%, window_entropy_acc=85.48%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 8.342744305705065
    Answer: 160
    Ground truth:  160
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 8.342744305705065
    Answer: 160
    Ground truth:  160
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 15.999892234802246
    Answer: 160
    Ground truth:  160
Method 4: self_consistency
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 1.0
    Answer: 160
    Ground truth:  160
Method 5: p_true
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 12.96484375
    Answer: 160
    Ground truth:  160
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 1.3838939666748047
    Answer: 160
    Ground truth:  160
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 1.1507613211870193
    Answer: 160
    Ground truth:  160
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 1.1510995626449585
    Answer: 160
    Ground truth:  160
Method 9: window_entropy
  Batch 1:
    Text: To find out how many chairs the restaurant has left, we first need to calculate ...
    Score: 4.653960585594177
    Answer: 160
    Ground truth:  160
Method name: attention_weighted_confidence, running accuracy: 85.71428571428571
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.71428571428571
Method name: cer_prob_product_log_last, running accuracy: 87.3015873015873
Method name: self_consistency, running accuracy: 85.71428571428571
Method name: p_true, running accuracy: 85.71428571428571
Method name: normilized_likelihood, running accuracy: 85.71428571428571
Method name: normilized_entropy, running accuracy: 84.12698412698413
Method name: topk_entropy, running accuracy: 84.12698412698413
Method name: window_entropy, running accuracy: 85.71428571428571

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 62/500 [5:39:39<46:16:50, 380.39s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=87.30%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=84.13%, topk_entropy_acc=84.13%, window_entropy_acc=85.71%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 63/500 [5:39:39<40:23:17, 332.72s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=87.30%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=84.13%, topk_entropy_acc=84.13%, window_entropy_acc=85.71%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 8.343218701810454
    Answer: 220
    Ground truth:  220
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 8.343218701810454
    Answer: 220
    Ground truth:  220
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 15.999792218208313
    Answer: 220
    Ground truth:  220
Method 4: self_consistency
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 1.0
    Answer: 220
    Ground truth:  220
Method 5: p_true
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 14.10546875
    Answer: 220
    Ground truth:  220
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 2.7646660655736923
    Answer: 220
    Ground truth:  220
Method 7: normilized_entropy
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 2.2702751606702805
    Answer: 220
    Ground truth:  220
Method 8: topk_entropy
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 2.246797926723957
    Answer: 220
    Ground truth:  220
Method 9: window_entropy
  Batch 1:
    Text: To find the weight of the Mastiff, we need to work backwards from the Papillon's...
    Score: 5.844756364822388
    Answer: 220
    Ground truth:  220
Method name: attention_weighted_confidence, running accuracy: 85.9375
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.9375
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 85.9375
Method name: p_true, running accuracy: 85.9375
Method name: normilized_likelihood, running accuracy: 85.9375
Method name: normilized_entropy, running accuracy: 84.375
Method name: topk_entropy, running accuracy: 84.375
Method name: window_entropy, running accuracy: 85.9375

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 63/500 [5:43:57<40:23:17, 332.72s/it, attention_weighted_confidence_acc=85.94%, cer_entropy_weighted_mean_all_acc=85.94%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=85.94%, p_true_acc=85.94%, normilized_likelihood_acc=85.94%, normilized_entropy_acc=84.38%, topk_entropy_acc=84.38%, window_entropy_acc=85.94%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 64/500 [5:43:57<37:35:32, 310.40s/it, attention_weighted_confidence_acc=85.94%, cer_entropy_weighted_mean_all_acc=85.94%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=85.94%, p_true_acc=85.94%, normilized_likelihood_acc=85.94%, normilized_entropy_acc=84.38%, topk_entropy_acc=84.38%, window_entropy_acc=85.94%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 4.464518449480097
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 4.464518449480097
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 8.86343652009964
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 0.5625
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 8.41015625
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 4.572292193770409
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 4.600122034549713
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 4.249172329902649
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to find the total number of rotations the tires w...
    Score: 11.391766667366028
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 86.15384615384616
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.15384615384616
Method name: cer_prob_product_log_last, running accuracy: 87.6923076923077
Method name: self_consistency, running accuracy: 86.15384615384616
Method name: p_true, running accuracy: 86.15384615384616
Method name: normilized_likelihood, running accuracy: 86.15384615384616
Method name: normilized_entropy, running accuracy: 84.61538461538461
Method name: topk_entropy, running accuracy: 84.61538461538461
Method name: window_entropy, running accuracy: 86.15384615384616

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 64/500 [5:49:57<37:35:32, 310.40s/it, attention_weighted_confidence_acc=86.15%, cer_entropy_weighted_mean_all_acc=86.15%, cer_prob_product_log_last_acc=87.69%, self_consistency_acc=86.15%, p_true_acc=86.15%, normilized_likelihood_acc=86.15%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=86.15%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 65/500 [5:49:57<39:18:22, 325.29s/it, attention_weighted_confidence_acc=86.15%, cer_entropy_weighted_mean_all_acc=86.15%, cer_prob_product_log_last_acc=87.69%, self_consistency_acc=86.15%, p_true_acc=86.15%, normilized_likelihood_acc=86.15%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=86.15%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 7.888097370283177
    Answer: 120
    Ground truth:  120
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 7.888097370283177
    Answer: 120
    Ground truth:  120
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 15.498486697673798
    Answer: 120
    Ground truth:  120
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 1.0
    Answer: 120
    Ground truth:  120
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 14.0703125
    Answer: 120
    Ground truth:  120
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 2.944941908121109
    Answer: 120
    Ground truth:  120
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 1.2822854667901993
    Answer: 120
    Ground truth:  120
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 1.2763075530529022
    Answer: 120
    Ground truth:  120
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to find out how many friends Amy has and then add...
    Score: 2.5834812223911285
    Answer: 120
    Ground truth:  120
Method name: attention_weighted_confidence, running accuracy: 86.36363636363636
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.36363636363636
Method name: cer_prob_product_log_last, running accuracy: 87.87878787878788
Method name: self_consistency, running accuracy: 86.36363636363636
Method name: p_true, running accuracy: 86.36363636363636
Method name: normilized_likelihood, running accuracy: 86.36363636363636
Method name: normilized_entropy, running accuracy: 84.84848484848484
Method name: topk_entropy, running accuracy: 84.84848484848484
Method name: window_entropy, running accuracy: 86.36363636363636

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 65/500 [5:52:58<39:18:22, 325.29s/it, attention_weighted_confidence_acc=86.36%, cer_entropy_weighted_mean_all_acc=86.36%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=86.36%, p_true_acc=86.36%, normilized_likelihood_acc=86.36%, normilized_entropy_acc=84.85%, topk_entropy_acc=84.85%, window_entropy_acc=86.36%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 66/500 [5:52:58<33:58:41, 281.85s/it, attention_weighted_confidence_acc=86.36%, cer_entropy_weighted_mean_all_acc=86.36%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=86.36%, p_true_acc=86.36%, normilized_likelihood_acc=86.36%, normilized_entropy_acc=84.85%, topk_entropy_acc=84.85%, window_entropy_acc=86.36%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 7.010994595468649
    Answer: 40
    Ground truth:  40
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 7.010994595468649
    Answer: 40
    Ground truth:  40
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 13.989779949188232
    Answer: 40
    Ground truth:  40
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 0.875
    Answer: 40
    Ground truth:  40
Method 5: p_true
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 12.98046875
    Answer: 40
    Ground truth:  40
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 1.9963473230600357
    Answer: 40
    Ground truth:  40
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 1.147853285074234
    Answer: 40
    Ground truth:  40
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 1.144250139594078
    Answer: 40
    Ground truth:  40
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Paige raised, we need to calculate the total number of slic...
    Score: 3.0464656352996826
    Answer: 40
    Ground truth:  40
Method name: attention_weighted_confidence, running accuracy: 86.56716417910447
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.56716417910447
Method name: cer_prob_product_log_last, running accuracy: 88.05970149253731
Method name: self_consistency, running accuracy: 86.56716417910447
Method name: p_true, running accuracy: 86.56716417910447
Method name: normilized_likelihood, running accuracy: 86.56716417910447
Method name: normilized_entropy, running accuracy: 85.07462686567165
Method name: topk_entropy, running accuracy: 85.07462686567165
Method name: window_entropy, running accuracy: 86.56716417910447

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 66/500 [5:58:25<33:58:41, 281.85s/it, attention_weighted_confidence_acc=86.57%, cer_entropy_weighted_mean_all_acc=86.57%, cer_prob_product_log_last_acc=88.06%, self_consistency_acc=86.57%, p_true_acc=86.57%, normilized_likelihood_acc=86.57%, normilized_entropy_acc=85.07%, topk_entropy_acc=85.07%, window_entropy_acc=86.57%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 67/500 [5:58:25<35:31:43, 295.39s/it, attention_weighted_confidence_acc=86.57%, cer_entropy_weighted_mean_all_acc=86.57%, cer_prob_product_log_last_acc=88.06%, self_consistency_acc=86.57%, p_true_acc=86.57%, normilized_likelihood_acc=86.57%, normilized_entropy_acc=85.07%, topk_entropy_acc=85.07%, window_entropy_acc=86.57%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 2.966365382144009
    Answer: 163
    Ground truth:  163
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 2.966365382144009
    Answer: 163
    Ground truth:  163
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 5.6357221603393555
    Answer: 163
    Ground truth:  163
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 0.375
    Answer: 163
    Ground truth:  163
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 5.45703125
    Answer: 163
    Ground truth:  163
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 5.690533220767975
    Answer: 163
    Ground truth:  163
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 5.756247043609619
    Answer: 163
    Ground truth:  163
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 4.758736163377762
    Answer: 163
    Ground truth:  163
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can set up an equation. Let x be the number of Post-it...
    Score: 9.662420928478241
    Answer: 163
    Ground truth:  163
Method name: attention_weighted_confidence, running accuracy: 86.76470588235294
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.76470588235294
Method name: cer_prob_product_log_last, running accuracy: 88.23529411764706
Method name: self_consistency, running accuracy: 86.76470588235294
Method name: p_true, running accuracy: 86.76470588235294
Method name: normilized_likelihood, running accuracy: 86.76470588235294
Method name: normilized_entropy, running accuracy: 85.29411764705883
Method name: topk_entropy, running accuracy: 85.29411764705883
Method name: window_entropy, running accuracy: 86.76470588235294

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 67/500 [6:06:44<35:31:43, 295.39s/it, attention_weighted_confidence_acc=86.76%, cer_entropy_weighted_mean_all_acc=86.76%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=86.76%, p_true_acc=86.76%, normilized_likelihood_acc=86.76%, normilized_entropy_acc=85.29%, topk_entropy_acc=85.29%, window_entropy_acc=86.76%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▎        | 68/500 [6:06:44<42:46:51, 356.51s/it, attention_weighted_confidence_acc=86.76%, cer_entropy_weighted_mean_all_acc=86.76%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=86.76%, p_true_acc=86.76%, normilized_likelihood_acc=86.76%, normilized_entropy_acc=85.29%, topk_entropy_acc=85.29%, window_entropy_acc=86.76%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 3.9200946322236105
    Answer: 255
    Ground truth:  255
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 3.9200946322236105
    Answer: 255
    Ground truth:  255
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 7.999891996383667
    Answer: 255
    Ground truth:  255
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 0.5
    Answer: 255
    Ground truth:  255
Method 5: p_true
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 6.75390625
    Answer: 255
    Ground truth:  255
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 7.642469152808189
    Answer: 255
    Ground truth:  255
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 7.5746926963329315
    Answer: 255
    Ground truth:  255
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 6.625811845064163
    Answer: 255
    Ground truth:  255
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Shiela will pay each month for 3 months, we need to calcula...
    Score: 8.396058142185211
    Answer: 255
    Ground truth:  255
Method name: attention_weighted_confidence, running accuracy: 86.95652173913044
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.95652173913044
Method name: cer_prob_product_log_last, running accuracy: 88.40579710144928
Method name: self_consistency, running accuracy: 86.95652173913044
Method name: p_true, running accuracy: 86.95652173913044
Method name: normilized_likelihood, running accuracy: 86.95652173913044
Method name: normilized_entropy, running accuracy: 85.5072463768116
Method name: topk_entropy, running accuracy: 85.5072463768116
Method name: window_entropy, running accuracy: 86.95652173913044

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▎        | 68/500 [6:12:20<42:46:51, 356.51s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=88.41%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=85.51%, topk_entropy_acc=85.51%, window_entropy_acc=86.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 69/500 [6:12:20<41:56:14, 350.29s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=88.41%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=85.51%, topk_entropy_acc=85.51%, window_entropy_acc=86.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 7.853398824027135
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 7.853398824027135
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 15.99702763557434
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 15.35546875
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 1.3955544531345367
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 1.053254097700119
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 1.047871619462967
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how many days it will take Prince to sell the meat from Bill's bull,...
    Score: 3.142091155052185
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 87.14285714285714
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.14285714285714
Method name: cer_prob_product_log_last, running accuracy: 88.57142857142857
Method name: self_consistency, running accuracy: 87.14285714285714
Method name: p_true, running accuracy: 87.14285714285714
Method name: normilized_likelihood, running accuracy: 87.14285714285714
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 87.14285714285714

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 69/500 [6:16:17<41:56:14, 350.29s/it, attention_weighted_confidence_acc=87.14%, cer_entropy_weighted_mean_all_acc=87.14%, cer_prob_product_log_last_acc=88.57%, self_consistency_acc=87.14%, p_true_acc=87.14%, normilized_likelihood_acc=87.14%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=87.14%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 70/500 [6:16:17<37:47:01, 316.33s/it, attention_weighted_confidence_acc=87.14%, cer_entropy_weighted_mean_all_acc=87.14%, cer_prob_product_log_last_acc=88.57%, self_consistency_acc=87.14%, p_true_acc=87.14%, normilized_likelihood_acc=87.14%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=87.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 7.8357958315455125
    Answer: 18
    Ground truth:  14
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 7.8357958315455125
    Answer: 18
    Ground truth:  14
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 14.999749183654785
    Answer: 18
    Ground truth:  14
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 0.9375
    Answer: 18
    Ground truth:  14
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 14.23828125
    Answer: 18
    Ground truth:  14
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 6.93988835811615
    Answer: 18
    Ground truth:  14
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 7.344538450241089
    Answer: 18
    Ground truth:  14
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 6.845064714550972
    Answer: 18
    Ground truth:  14
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

Step 1: In the first g...
    Score: 17.14304757118225
    Answer: 18
    Ground truth:  14
Method name: attention_weighted_confidence, running accuracy: 85.91549295774648
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.91549295774648
Method name: cer_prob_product_log_last, running accuracy: 87.32394366197182
Method name: self_consistency, running accuracy: 85.91549295774648
Method name: p_true, running accuracy: 85.91549295774648
Method name: normilized_likelihood, running accuracy: 85.91549295774648
Method name: normilized_entropy, running accuracy: 84.50704225352112
Method name: topk_entropy, running accuracy: 84.50704225352112
Method name: window_entropy, running accuracy: 85.91549295774648

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 70/500 [6:20:47<37:47:01, 316.33s/it, attention_weighted_confidence_acc=85.92%, cer_entropy_weighted_mean_all_acc=85.92%, cer_prob_product_log_last_acc=87.32%, self_consistency_acc=85.92%, p_true_acc=85.92%, normilized_likelihood_acc=85.92%, normilized_entropy_acc=84.51%, topk_entropy_acc=84.51%, window_entropy_acc=85.92%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 71/500 [6:20:47<36:02:21, 302.43s/it, attention_weighted_confidence_acc=85.92%, cer_entropy_weighted_mean_all_acc=85.92%, cer_prob_product_log_last_acc=87.32%, self_consistency_acc=85.92%, p_true_acc=85.92%, normilized_likelihood_acc=85.92%, normilized_entropy_acc=84.51%, topk_entropy_acc=84.51%, window_entropy_acc=85.92%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 4.843442400103829
    Answer: 178.75
    Ground truth:  205
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 4.843442400103829
    Answer: 178.75
    Ground truth:  205
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 3.333265026534155
    Answer: 178.75
    Ground truth:  205
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 0.625
    Answer: 178.75
    Ground truth:  205
Method 5: p_true
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 9.3359375
    Answer: 178.75
    Ground truth:  205
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 6.146208539605141
    Answer: 178.75
    Ground truth:  205
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 6.078427895903587
    Answer: 178.75
    Ground truth:  205
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 5.43231301009655
    Answer: 178.75
    Ground truth:  205
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount Patty charged, we need to calculate the labor cost and ...
    Score: 12.438065946102142
    Answer: 178.75
    Ground truth:  205
Method name: attention_weighted_confidence, running accuracy: 84.72222222222221
Method name: cer_entropy_weighted_mean_all, running accuracy: 84.72222222222221
Method name: cer_prob_product_log_last, running accuracy: 86.11111111111111
Method name: self_consistency, running accuracy: 84.72222222222221
Method name: p_true, running accuracy: 84.72222222222221
Method name: normilized_likelihood, running accuracy: 84.72222222222221
Method name: normilized_entropy, running accuracy: 83.33333333333334
Method name: topk_entropy, running accuracy: 83.33333333333334
Method name: window_entropy, running accuracy: 84.72222222222221

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 71/500 [6:25:15<36:02:21, 302.43s/it, attention_weighted_confidence_acc=84.72%, cer_entropy_weighted_mean_all_acc=84.72%, cer_prob_product_log_last_acc=86.11%, self_consistency_acc=84.72%, p_true_acc=84.72%, normilized_likelihood_acc=84.72%, normilized_entropy_acc=83.33%, topk_entropy_acc=83.33%, window_entropy_acc=84.72%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 72/500 [6:25:15<34:44:30, 292.22s/it, attention_weighted_confidence_acc=84.72%, cer_entropy_weighted_mean_all_acc=84.72%, cer_prob_product_log_last_acc=86.11%, self_consistency_acc=84.72%, p_true_acc=84.72%, normilized_likelihood_acc=84.72%, normilized_entropy_acc=83.33%, topk_entropy_acc=83.33%, window_entropy_acc=84.72%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 6.23987965590711
    Answer: 560
    Ground truth:  560
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 6.23987965590711
    Answer: 560
    Ground truth:  560
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 12.996374070644379
    Answer: 560
    Ground truth:  560
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 0.8125
    Answer: 560
    Ground truth:  560
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 12.08984375
    Answer: 560
    Ground truth:  560
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 9.70757320523262
    Answer: 560
    Ground truth:  560
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 9.447638541460037
    Answer: 560
    Ground truth:  560
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 7.847554117441177
    Answer: 560
    Ground truth:  560
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. First, we need to fin...
    Score: 20.54706311225891
    Answer: 560
    Ground truth:  560
Method name: attention_weighted_confidence, running accuracy: 84.93150684931507
Method name: cer_entropy_weighted_mean_all, running accuracy: 84.93150684931507
Method name: cer_prob_product_log_last, running accuracy: 86.3013698630137
Method name: self_consistency, running accuracy: 84.93150684931507
Method name: p_true, running accuracy: 84.93150684931507
Method name: normilized_likelihood, running accuracy: 84.93150684931507
Method name: normilized_entropy, running accuracy: 83.56164383561644
Method name: topk_entropy, running accuracy: 83.56164383561644
Method name: window_entropy, running accuracy: 84.93150684931507

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 72/500 [6:29:59<34:44:30, 292.22s/it, attention_weighted_confidence_acc=84.93%, cer_entropy_weighted_mean_all_acc=84.93%, cer_prob_product_log_last_acc=86.30%, self_consistency_acc=84.93%, p_true_acc=84.93%, normilized_likelihood_acc=84.93%, normilized_entropy_acc=83.56%, topk_entropy_acc=83.56%, window_entropy_acc=84.93%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▍        | 73/500 [6:29:59<34:22:18, 289.79s/it, attention_weighted_confidence_acc=84.93%, cer_entropy_weighted_mean_all_acc=84.93%, cer_prob_product_log_last_acc=86.30%, self_consistency_acc=84.93%, p_true_acc=84.93%, normilized_likelihood_acc=84.93%, normilized_entropy_acc=83.56%, topk_entropy_acc=83.56%, window_entropy_acc=84.93%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 7.054122259876255
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 7.054122259876255
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 13.999906659126282
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 0.875
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 11.68359375
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 7.886927112936974
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 8.42894932627678
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 7.395841255784035
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find out how much James saved, we need to calculate the total cost of picking...
    Score: 22.381555140018463
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 85.13513513513513
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.13513513513513
Method name: cer_prob_product_log_last, running accuracy: 86.48648648648648
Method name: self_consistency, running accuracy: 85.13513513513513
Method name: p_true, running accuracy: 85.13513513513513
Method name: normilized_likelihood, running accuracy: 85.13513513513513
Method name: normilized_entropy, running accuracy: 83.78378378378379
Method name: topk_entropy, running accuracy: 83.78378378378379
Method name: window_entropy, running accuracy: 85.13513513513513

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▍        | 73/500 [6:35:14<34:22:18, 289.79s/it, attention_weighted_confidence_acc=85.14%, cer_entropy_weighted_mean_all_acc=85.14%, cer_prob_product_log_last_acc=86.49%, self_consistency_acc=85.14%, p_true_acc=85.14%, normilized_likelihood_acc=85.14%, normilized_entropy_acc=83.78%, topk_entropy_acc=83.78%, window_entropy_acc=85.14%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▍        | 74/500 [6:35:14<35:11:20, 297.37s/it, attention_weighted_confidence_acc=85.14%, cer_entropy_weighted_mean_all_acc=85.14%, cer_prob_product_log_last_acc=86.49%, self_consistency_acc=85.14%, p_true_acc=85.14%, normilized_likelihood_acc=85.14%, normilized_entropy_acc=83.78%, topk_entropy_acc=83.78%, window_entropy_acc=85.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 8.046655183726312
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 8.046655183726312
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 15.980453252792358
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 1.0
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 13.046875
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 1.3162658661603928
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 1.160698801279068
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 1.1383271515369415
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step:

1. Eve initially had 20...
    Score: 4.883509337902069
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 85.33333333333334
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.33333333333334
Method name: cer_prob_product_log_last, running accuracy: 86.66666666666667
Method name: self_consistency, running accuracy: 85.33333333333334
Method name: p_true, running accuracy: 85.33333333333334
Method name: normilized_likelihood, running accuracy: 85.33333333333334
Method name: normilized_entropy, running accuracy: 84.0
Method name: topk_entropy, running accuracy: 84.0
Method name: window_entropy, running accuracy: 85.33333333333334

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▍        | 74/500 [6:39:23<35:11:20, 297.37s/it, attention_weighted_confidence_acc=85.33%, cer_entropy_weighted_mean_all_acc=85.33%, cer_prob_product_log_last_acc=86.67%, self_consistency_acc=85.33%, p_true_acc=85.33%, normilized_likelihood_acc=85.33%, normilized_entropy_acc=84.00%, topk_entropy_acc=84.00%, window_entropy_acc=85.33%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 75/500 [6:39:23<33:22:49, 282.75s/it, attention_weighted_confidence_acc=85.33%, cer_entropy_weighted_mean_all_acc=85.33%, cer_prob_product_log_last_acc=86.67%, self_consistency_acc=85.33%, p_true_acc=85.33%, normilized_likelihood_acc=85.33%, normilized_entropy_acc=84.00%, topk_entropy_acc=84.00%, window_entropy_acc=85.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 4.125025153096068
    Answer: 20.0
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 4.125025153096068
    Answer: 20.0
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate his total revenue, total cost...
    Score: 2.679581046104431
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 0.5
    Answer: 20.0
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 7.125
    Answer: 20.0
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 9.287079393863678
    Answer: 20.0
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 8.58479768037796
    Answer: 20.0
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 7.3844509571790695
    Answer: 20.0
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To find Charlie's net profit, we need to calculate the cost of making the candle...
    Score: 13.50507390499115
    Answer: 20.0
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 85.52631578947368
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.52631578947368
Method name: cer_prob_product_log_last, running accuracy: 86.8421052631579
Method name: self_consistency, running accuracy: 85.52631578947368
Method name: p_true, running accuracy: 85.52631578947368
Method name: normilized_likelihood, running accuracy: 85.52631578947368
Method name: normilized_entropy, running accuracy: 84.21052631578947
Method name: topk_entropy, running accuracy: 84.21052631578947
Method name: window_entropy, running accuracy: 85.52631578947368

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 75/500 [6:45:49<33:22:49, 282.75s/it, attention_weighted_confidence_acc=85.53%, cer_entropy_weighted_mean_all_acc=85.53%, cer_prob_product_log_last_acc=86.84%, self_consistency_acc=85.53%, p_true_acc=85.53%, normilized_likelihood_acc=85.53%, normilized_entropy_acc=84.21%, topk_entropy_acc=84.21%, window_entropy_acc=85.53%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 76/500 [6:45:49<36:56:34, 313.67s/it, attention_weighted_confidence_acc=85.53%, cer_entropy_weighted_mean_all_acc=85.53%, cer_prob_product_log_last_acc=86.84%, self_consistency_acc=85.53%, p_true_acc=85.53%, normilized_likelihood_acc=85.53%, normilized_entropy_acc=84.21%, topk_entropy_acc=84.21%, window_entropy_acc=85.53%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 8.01551960135135
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 8.01551960135135
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 15.865714192390442
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 14.421875
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 1.8037158399820328
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 2.075539618730545
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 2.0452947318553925
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how many points Joey is winning, we need to calculate the final scor...
    Score: 3.3735448122024536
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 85.71428571428571
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.71428571428571
Method name: cer_prob_product_log_last, running accuracy: 87.01298701298701
Method name: self_consistency, running accuracy: 85.71428571428571
Method name: p_true, running accuracy: 85.71428571428571
Method name: normilized_likelihood, running accuracy: 85.71428571428571
Method name: normilized_entropy, running accuracy: 84.4155844155844
Method name: topk_entropy, running accuracy: 84.4155844155844
Method name: window_entropy, running accuracy: 85.71428571428571

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 76/500 [6:49:35<36:56:34, 313.67s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=87.01%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=84.42%, topk_entropy_acc=84.42%, window_entropy_acc=85.71%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 77/500 [6:49:35<33:46:34, 287.46s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=87.01%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=84.42%, topk_entropy_acc=84.42%, window_entropy_acc=85.71%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 7.423284102450829
    Answer: 90
    Ground truth:  90
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 7.423284102450829
    Answer: 90
    Ground truth:  90
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 14.999146342277527
    Answer: 90
    Ground truth:  90
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 0.9375
    Answer: 90
    Ground truth:  90
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 12.54296875
    Answer: 90
    Ground truth:  90
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 2.0085221379995346
    Answer: 90
    Ground truth:  90
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 1.0446851402521133
    Answer: 90
    Ground truth:  90
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 1.0411449074745178
    Answer: 90
    Ground truth:  90
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Step 1: Celine starts with 120 liter...
    Score: 2.6709226965904236
    Answer: 90
    Ground truth:  90
Method name: attention_weighted_confidence, running accuracy: 85.8974358974359
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.8974358974359
Method name: cer_prob_product_log_last, running accuracy: 87.17948717948718
Method name: self_consistency, running accuracy: 85.8974358974359
Method name: p_true, running accuracy: 85.8974358974359
Method name: normilized_likelihood, running accuracy: 85.8974358974359
Method name: normilized_entropy, running accuracy: 84.61538461538461
Method name: topk_entropy, running accuracy: 84.61538461538461
Method name: window_entropy, running accuracy: 85.8974358974359

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 77/500 [6:53:51<33:46:34, 287.46s/it, attention_weighted_confidence_acc=85.90%, cer_entropy_weighted_mean_all_acc=85.90%, cer_prob_product_log_last_acc=87.18%, self_consistency_acc=85.90%, p_true_acc=85.90%, normilized_likelihood_acc=85.90%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=85.90%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 78/500 [6:53:51<32:35:52, 278.09s/it, attention_weighted_confidence_acc=85.90%, cer_entropy_weighted_mean_all_acc=85.90%, cer_prob_product_log_last_acc=87.18%, self_consistency_acc=85.90%, p_true_acc=85.90%, normilized_likelihood_acc=85.90%, normilized_entropy_acc=84.62%, topk_entropy_acc=84.62%, window_entropy_acc=85.90%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 8.06358250706592
    Answer: 240
    Ground truth:  240
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 8.06358250706592
    Answer: 240
    Ground truth:  240
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 15.997530162334442
    Answer: 240
    Ground truth:  240
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 1.0
    Answer: 240
    Ground truth:  240
Method 5: p_true
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 13.46484375
    Answer: 240
    Ground truth:  240
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 1.4493937343358994
    Answer: 240
    Ground truth:  240
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 0.998139962553978
    Answer: 240
    Ground truth:  240
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 0.9778044521808624
    Answer: 240
    Ground truth:  240
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount of money Stetson gave up, we need to follow these steps...
    Score: 3.1904013752937317
    Answer: 240
    Ground truth:  240
Method name: attention_weighted_confidence, running accuracy: 86.07594936708861
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.07594936708861
Method name: cer_prob_product_log_last, running accuracy: 87.34177215189874
Method name: self_consistency, running accuracy: 86.07594936708861
Method name: p_true, running accuracy: 86.07594936708861
Method name: normilized_likelihood, running accuracy: 86.07594936708861
Method name: normilized_entropy, running accuracy: 84.81012658227847
Method name: topk_entropy, running accuracy: 84.81012658227847
Method name: window_entropy, running accuracy: 86.07594936708861

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 78/500 [6:57:47<32:35:52, 278.09s/it, attention_weighted_confidence_acc=86.08%, cer_entropy_weighted_mean_all_acc=86.08%, cer_prob_product_log_last_acc=87.34%, self_consistency_acc=86.08%, p_true_acc=86.08%, normilized_likelihood_acc=86.08%, normilized_entropy_acc=84.81%, topk_entropy_acc=84.81%, window_entropy_acc=86.08%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 79/500 [6:57:47<31:02:04, 265.38s/it, attention_weighted_confidence_acc=86.08%, cer_entropy_weighted_mean_all_acc=86.08%, cer_prob_product_log_last_acc=87.34%, self_consistency_acc=86.08%, p_true_acc=86.08%, normilized_likelihood_acc=86.08%, normilized_entropy_acc=84.81%, topk_entropy_acc=84.81%, window_entropy_acc=86.08%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 5.7928240864150045
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 5.7928240864150045
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 11.406746685504913
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 0.75
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 10.8359375
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 13.064645946025848
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 11.924850732088089
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 10.55872032046318
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find out how long the rice will last, we first need to find out how much rice...
    Score: 16.843081057071686
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 86.25
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.25
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 86.25
Method name: p_true, running accuracy: 86.25
Method name: normilized_likelihood, running accuracy: 86.25
Method name: normilized_entropy, running accuracy: 85.0
Method name: topk_entropy, running accuracy: 85.0
Method name: window_entropy, running accuracy: 86.25

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 79/500 [7:03:39<31:02:04, 265.38s/it, attention_weighted_confidence_acc=86.25%, cer_entropy_weighted_mean_all_acc=86.25%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=86.25%, p_true_acc=86.25%, normilized_likelihood_acc=86.25%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=86.25%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 80/500 [7:03:39<33:59:24, 291.34s/it, attention_weighted_confidence_acc=86.25%, cer_entropy_weighted_mean_all_acc=86.25%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=86.25%, p_true_acc=86.25%, normilized_likelihood_acc=86.25%, normilized_entropy_acc=85.00%, topk_entropy_acc=85.00%, window_entropy_acc=86.25%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 5.984135813735111
    Answer: 42.0
    Ground truth:  42
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 5.984135813735111
    Answer: 42.0
    Ground truth:  42
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Roy has, we'll break it down into steps:

1. Eva has ...
    Score: 2.999866247177124
    Answer: 42
    Ground truth:  42
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 0.75
    Answer: 42.0
    Ground truth:  42
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 10.95703125
    Answer: 42.0
    Ground truth:  42
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 1.6247296333312988
    Answer: 42.0
    Ground truth:  42
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 1.1453891545534134
    Answer: 42.0
    Ground truth:  42
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 0.9419864267110825
    Answer: 42.0
    Ground truth:  42
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Eva saved $20.00.

Anthony saved $10...
    Score: 11.242592990398407
    Answer: 42.0
    Ground truth:  42
Method name: attention_weighted_confidence, running accuracy: 86.41975308641975
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.41975308641975
Method name: cer_prob_product_log_last, running accuracy: 87.65432098765432
Method name: self_consistency, running accuracy: 86.41975308641975
Method name: p_true, running accuracy: 86.41975308641975
Method name: normilized_likelihood, running accuracy: 86.41975308641975
Method name: normilized_entropy, running accuracy: 85.18518518518519
Method name: topk_entropy, running accuracy: 85.18518518518519
Method name: window_entropy, running accuracy: 86.41975308641975

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 80/500 [7:07:27<33:59:24, 291.34s/it, attention_weighted_confidence_acc=86.42%, cer_entropy_weighted_mean_all_acc=86.42%, cer_prob_product_log_last_acc=87.65%, self_consistency_acc=86.42%, p_true_acc=86.42%, normilized_likelihood_acc=86.42%, normilized_entropy_acc=85.19%, topk_entropy_acc=85.19%, window_entropy_acc=86.42%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 81/500 [7:07:27<31:40:43, 272.18s/it, attention_weighted_confidence_acc=86.42%, cer_entropy_weighted_mean_all_acc=86.42%, cer_prob_product_log_last_acc=87.65%, self_consistency_acc=86.42%, p_true_acc=86.42%, normilized_likelihood_acc=86.42%, normilized_entropy_acc=85.19%, topk_entropy_acc=85.19%, window_entropy_acc=86.42%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 3.288614985897306
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 3.288614985897306
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 6.504603624343872
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 0.4375
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 3.830078125
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 3.7317256927490234
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 3.821241647005081
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 3.291100889444351
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find the time it took James to reach the store, we need to first calculate th...
    Score: 8.686342537403107
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 86.58536585365853
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.58536585365853
Method name: cer_prob_product_log_last, running accuracy: 87.8048780487805
Method name: self_consistency, running accuracy: 86.58536585365853
Method name: p_true, running accuracy: 86.58536585365853
Method name: normilized_likelihood, running accuracy: 86.58536585365853
Method name: normilized_entropy, running accuracy: 85.36585365853658
Method name: topk_entropy, running accuracy: 85.36585365853658
Method name: window_entropy, running accuracy: 86.58536585365853

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 81/500 [7:12:30<31:40:43, 272.18s/it, attention_weighted_confidence_acc=86.59%, cer_entropy_weighted_mean_all_acc=86.59%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=86.59%, p_true_acc=86.59%, normilized_likelihood_acc=86.59%, normilized_entropy_acc=85.37%, topk_entropy_acc=85.37%, window_entropy_acc=86.59%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▋        | 82/500 [7:12:30<32:41:37, 281.57s/it, attention_weighted_confidence_acc=86.59%, cer_entropy_weighted_mean_all_acc=86.59%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=86.59%, p_true_acc=86.59%, normilized_likelihood_acc=86.59%, normilized_entropy_acc=85.37%, topk_entropy_acc=85.37%, window_entropy_acc=86.59%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 7.799358395266154
    Answer: 93000
    Ground truth:  93000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 7.799358395266154
    Answer: 93000
    Ground truth:  93000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 2.746006737491988
    Answer: 93000
    Ground truth:  93000
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 1.0
    Answer: 93000
    Ground truth:  93000
Method 5: p_true
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 13.97265625
    Answer: 93000
    Ground truth:  93000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 3.0509255826473236
    Answer: 93000
    Ground truth:  93000
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 2.2986821979284286
    Answer: 93000
    Ground truth:  93000
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 2.2424905449151993
    Answer: 93000
    Ground truth:  93000
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of posts in the group for March, we need to calculate t...
    Score: 6.104337632656097
    Answer: 93000
    Ground truth:  93000
Method name: attention_weighted_confidence, running accuracy: 86.74698795180723
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.74698795180723
Method name: cer_prob_product_log_last, running accuracy: 87.95180722891565
Method name: self_consistency, running accuracy: 86.74698795180723
Method name: p_true, running accuracy: 86.74698795180723
Method name: normilized_likelihood, running accuracy: 86.74698795180723
Method name: normilized_entropy, running accuracy: 85.54216867469879
Method name: topk_entropy, running accuracy: 85.54216867469879
Method name: window_entropy, running accuracy: 86.74698795180723

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▋        | 82/500 [7:16:18<32:41:37, 281.57s/it, attention_weighted_confidence_acc=86.75%, cer_entropy_weighted_mean_all_acc=86.75%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=86.75%, p_true_acc=86.75%, normilized_likelihood_acc=86.75%, normilized_entropy_acc=85.54%, topk_entropy_acc=85.54%, window_entropy_acc=86.75%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 83/500 [7:16:18<30:46:00, 265.61s/it, attention_weighted_confidence_acc=86.75%, cer_entropy_weighted_mean_all_acc=86.75%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=86.75%, p_true_acc=86.75%, normilized_likelihood_acc=86.75%, normilized_entropy_acc=85.54%, topk_entropy_acc=85.54%, window_entropy_acc=86.75%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 6.404972007990092
    Answer: 8
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 6.404972007990092
    Answer: 8
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 12.861891388893127
    Answer: 8
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 0.8125
    Answer: 8
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 12.6796875
    Answer: 8
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 3.29461532831192
    Answer: 8
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 3.7427228689193726
    Answer: 8
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 3.3879182636737823
    Answer: 8
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to first find the total time John spent on all ac...
    Score: 13.145413279533386
    Answer: 8
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 85.71428571428571
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.71428571428571
Method name: cer_prob_product_log_last, running accuracy: 86.90476190476191
Method name: self_consistency, running accuracy: 85.71428571428571
Method name: p_true, running accuracy: 85.71428571428571
Method name: normilized_likelihood, running accuracy: 85.71428571428571
Method name: normilized_entropy, running accuracy: 84.52380952380952
Method name: topk_entropy, running accuracy: 84.52380952380952
Method name: window_entropy, running accuracy: 85.71428571428571

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 83/500 [7:22:43<30:46:00, 265.61s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=86.90%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=84.52%, topk_entropy_acc=84.52%, window_entropy_acc=85.71%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 84/500 [7:22:43<34:49:50, 301.42s/it, attention_weighted_confidence_acc=85.71%, cer_entropy_weighted_mean_all_acc=85.71%, cer_prob_product_log_last_acc=86.90%, self_consistency_acc=85.71%, p_true_acc=85.71%, normilized_likelihood_acc=85.71%, normilized_entropy_acc=84.52%, topk_entropy_acc=84.52%, window_entropy_acc=85.71%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 3.5659113460077596
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 3.5659113460077596
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 6.996840834617615
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 0.4375
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 5.017578125
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 3.8599108308553696
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 4.009405121207237
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 3.566977694630623
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To find the total length of the glue sticks that are not used, we need to find t...
    Score: 9.478335738182068
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 85.88235294117646
Method name: cer_entropy_weighted_mean_all, running accuracy: 85.88235294117646
Method name: cer_prob_product_log_last, running accuracy: 87.05882352941177
Method name: self_consistency, running accuracy: 85.88235294117646
Method name: p_true, running accuracy: 85.88235294117646
Method name: normilized_likelihood, running accuracy: 85.88235294117646
Method name: normilized_entropy, running accuracy: 84.70588235294117
Method name: topk_entropy, running accuracy: 84.70588235294117
Method name: window_entropy, running accuracy: 85.88235294117646

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 84/500 [7:33:04<34:49:50, 301.42s/it, attention_weighted_confidence_acc=85.88%, cer_entropy_weighted_mean_all_acc=85.88%, cer_prob_product_log_last_acc=87.06%, self_consistency_acc=85.88%, p_true_acc=85.88%, normilized_likelihood_acc=85.88%, normilized_entropy_acc=84.71%, topk_entropy_acc=84.71%, window_entropy_acc=85.88%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 85/500 [7:33:04<45:47:11, 397.18s/it, attention_weighted_confidence_acc=85.88%, cer_entropy_weighted_mean_all_acc=85.88%, cer_prob_product_log_last_acc=87.06%, self_consistency_acc=85.88%, p_true_acc=85.88%, normilized_likelihood_acc=85.88%, normilized_entropy_acc=84.71%, topk_entropy_acc=84.71%, window_entropy_acc=85.88%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 3.493040529284284
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 3.493040529284284
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 6.154472500085831
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 0.4375
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 5.34765625
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 6.954721421003342
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 7.052347779273987
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 5.985027834773064
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of new limbs after fifteen days, we first need to calcu...
    Score: 7.940922915935516
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 86.04651162790698
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.04651162790698
Method name: cer_prob_product_log_last, running accuracy: 87.20930232558139
Method name: self_consistency, running accuracy: 86.04651162790698
Method name: p_true, running accuracy: 86.04651162790698
Method name: normilized_likelihood, running accuracy: 86.04651162790698
Method name: normilized_entropy, running accuracy: 84.88372093023256
Method name: topk_entropy, running accuracy: 84.88372093023256
Method name: window_entropy, running accuracy: 86.04651162790698

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 85/500 [7:39:29<45:47:11, 397.18s/it, attention_weighted_confidence_acc=86.05%, cer_entropy_weighted_mean_all_acc=86.05%, cer_prob_product_log_last_acc=87.21%, self_consistency_acc=86.05%, p_true_acc=86.05%, normilized_likelihood_acc=86.05%, normilized_entropy_acc=84.88%, topk_entropy_acc=84.88%, window_entropy_acc=86.05%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 86/500 [7:39:29<45:16:09, 393.65s/it, attention_weighted_confidence_acc=86.05%, cer_entropy_weighted_mean_all_acc=86.05%, cer_prob_product_log_last_acc=87.21%, self_consistency_acc=86.05%, p_true_acc=86.05%, normilized_likelihood_acc=86.05%, normilized_entropy_acc=84.88%, topk_entropy_acc=84.88%, window_entropy_acc=86.05%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 8.163918115187718
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 8.163918115187718
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 15.999556422233582
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 1.0
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 15.06640625
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 2.4808663576841354
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 1.9692234545946121
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 1.8406336456537247
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: To find out how much longer it will take for the coat of varnish on 6 paintings ...
    Score: 12.785823285579681
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 86.20689655172413
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.20689655172413
Method name: cer_prob_product_log_last, running accuracy: 87.35632183908046
Method name: self_consistency, running accuracy: 86.20689655172413
Method name: p_true, running accuracy: 86.20689655172413
Method name: normilized_likelihood, running accuracy: 86.20689655172413
Method name: normilized_entropy, running accuracy: 85.0574712643678
Method name: topk_entropy, running accuracy: 85.0574712643678
Method name: window_entropy, running accuracy: 86.20689655172413

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 86/500 [7:43:55<45:16:09, 393.65s/it, attention_weighted_confidence_acc=86.21%, cer_entropy_weighted_mean_all_acc=86.21%, cer_prob_product_log_last_acc=87.36%, self_consistency_acc=86.21%, p_true_acc=86.21%, normilized_likelihood_acc=86.21%, normilized_entropy_acc=85.06%, topk_entropy_acc=85.06%, window_entropy_acc=86.21%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 87/500 [7:43:55<40:44:55, 355.19s/it, attention_weighted_confidence_acc=86.21%, cer_entropy_weighted_mean_all_acc=86.21%, cer_prob_product_log_last_acc=87.36%, self_consistency_acc=86.21%, p_true_acc=86.21%, normilized_likelihood_acc=86.21%, normilized_entropy_acc=85.06%, topk_entropy_acc=85.06%, window_entropy_acc=86.21%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 5.571336446843401
    Answer: 34
    Ground truth:  34
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 5.571336446843401
    Answer: 34
    Ground truth:  34
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 10.997563004493713
    Answer: 34
    Ground truth:  34
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 0.6875
    Answer: 34
    Ground truth:  34
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 10.09765625
    Answer: 34
    Ground truth:  34
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 8.839484825730324
    Answer: 34
    Ground truth:  34
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 8.729644641280174
    Answer: 34
    Ground truth:  34
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 7.549031898379326
    Answer: 34
    Ground truth:  34
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

1. They spent $20.25 on...
    Score: 19.392279088497162
    Answer: 34
    Ground truth:  34
Method name: attention_weighted_confidence, running accuracy: 86.36363636363636
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.36363636363636
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 86.36363636363636
Method name: p_true, running accuracy: 86.36363636363636
Method name: normilized_likelihood, running accuracy: 86.36363636363636
Method name: normilized_entropy, running accuracy: 85.22727272727273
Method name: topk_entropy, running accuracy: 85.22727272727273
Method name: window_entropy, running accuracy: 86.36363636363636

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 87/500 [7:49:16<40:44:55, 355.19s/it, attention_weighted_confidence_acc=86.36%, cer_entropy_weighted_mean_all_acc=86.36%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=86.36%, p_true_acc=86.36%, normilized_likelihood_acc=86.36%, normilized_entropy_acc=85.23%, topk_entropy_acc=85.23%, window_entropy_acc=86.36%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 88/500 [7:49:16<39:29:08, 345.02s/it, attention_weighted_confidence_acc=86.36%, cer_entropy_weighted_mean_all_acc=86.36%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=86.36%, p_true_acc=86.36%, normilized_likelihood_acc=86.36%, normilized_entropy_acc=85.23%, topk_entropy_acc=85.23%, window_entropy_acc=86.36%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 6.885369851414748
    Answer: 130000
    Ground truth:  130000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 6.885369851414748
    Answer: 130000
    Ground truth:  130000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total population in South America, we need to find the total number ...
    Score: 0.9875884652137756
    Answer: 12
    Ground truth:  130000
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 0.875
    Answer: 130000
    Ground truth:  130000
Method 5: p_true
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 1.86083984375
    Answer: 130000
    Ground truth:  130000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 7.271112576127052
    Answer: 130000
    Ground truth:  130000
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 5.49629108607769
    Answer: 130000
    Ground truth:  130000
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 5.189333662390709
    Answer: 130000
    Ground truth:  130000
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of people in South America, we need to multiply the num...
    Score: 17.291469365358353
    Answer: 130000
    Ground truth:  130000
Method name: attention_weighted_confidence, running accuracy: 86.51685393258427
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.51685393258427
Method name: cer_prob_product_log_last, running accuracy: 86.51685393258427
Method name: self_consistency, running accuracy: 86.51685393258427
Method name: p_true, running accuracy: 86.51685393258427
Method name: normilized_likelihood, running accuracy: 86.51685393258427
Method name: normilized_entropy, running accuracy: 85.39325842696628
Method name: topk_entropy, running accuracy: 85.39325842696628
Method name: window_entropy, running accuracy: 86.51685393258427

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 88/500 [7:52:46<39:29:08, 345.02s/it, attention_weighted_confidence_acc=86.52%, cer_entropy_weighted_mean_all_acc=86.52%, cer_prob_product_log_last_acc=86.52%, self_consistency_acc=86.52%, p_true_acc=86.52%, normilized_likelihood_acc=86.52%, normilized_entropy_acc=85.39%, topk_entropy_acc=85.39%, window_entropy_acc=86.52%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 89/500 [7:52:46<34:46:00, 304.53s/it, attention_weighted_confidence_acc=86.52%, cer_entropy_weighted_mean_all_acc=86.52%, cer_prob_product_log_last_acc=86.52%, self_consistency_acc=86.52%, p_true_acc=86.52%, normilized_likelihood_acc=86.52%, normilized_entropy_acc=85.39%, topk_entropy_acc=85.39%, window_entropy_acc=86.52%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 1.412292746428653
    Answer: 318
    Ground truth:  319
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 1.412292746428653
    Answer: 318
    Ground truth:  319
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 2.999900221824646
    Answer: 318
    Ground truth:  319
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 0.1875
    Answer: 318
    Ground truth:  319
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 2.21484375
    Answer: 318
    Ground truth:  319
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 3.031812608242035
    Answer: 318
    Ground truth:  319
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 2.776745080947876
    Answer: 318
    Ground truth:  319
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 2.390559136867523
    Answer: 318
    Ground truth:  319
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Alex invites 100 pe...
    Score: 3.5412707924842834
    Answer: 318
    Ground truth:  319
Method name: attention_weighted_confidence, running accuracy: 86.66666666666667
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.66666666666667
Method name: cer_prob_product_log_last, running accuracy: 86.66666666666667
Method name: self_consistency, running accuracy: 86.66666666666667
Method name: p_true, running accuracy: 86.66666666666667
Method name: normilized_likelihood, running accuracy: 86.66666666666667
Method name: normilized_entropy, running accuracy: 85.55555555555556
Method name: topk_entropy, running accuracy: 85.55555555555556
Method name: window_entropy, running accuracy: 86.66666666666667

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 89/500 [7:59:32<34:46:00, 304.53s/it, attention_weighted_confidence_acc=86.67%, cer_entropy_weighted_mean_all_acc=86.67%, cer_prob_product_log_last_acc=86.67%, self_consistency_acc=86.67%, p_true_acc=86.67%, normilized_likelihood_acc=86.67%, normilized_entropy_acc=85.56%, topk_entropy_acc=85.56%, window_entropy_acc=86.67%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 90/500 [7:59:32<38:08:15, 334.87s/it, attention_weighted_confidence_acc=86.67%, cer_entropy_weighted_mean_all_acc=86.67%, cer_prob_product_log_last_acc=86.67%, self_consistency_acc=86.67%, p_true_acc=86.67%, normilized_likelihood_acc=86.67%, normilized_entropy_acc=85.56%, topk_entropy_acc=85.56%, window_entropy_acc=86.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 7.358205176534522
    Answer: 50
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 7.358205176534522
    Answer: 50
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 14.285371720790863
    Answer: 50
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 0.9375
    Answer: 50
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 14.05859375
    Answer: 50
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 2.6609634160995483
    Answer: 50
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 1.9850650429725647
    Answer: 50
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 1.8756411969661713
    Answer: 50
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

First, we need to dete...
    Score: 9.365418314933777
    Answer: 50
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 86.81318681318682
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.81318681318682
Method name: cer_prob_product_log_last, running accuracy: 86.81318681318682
Method name: self_consistency, running accuracy: 86.81318681318682
Method name: p_true, running accuracy: 86.81318681318682
Method name: normilized_likelihood, running accuracy: 86.81318681318682
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 86.81318681318682

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 90/500 [8:05:06<38:08:15, 334.87s/it, attention_weighted_confidence_acc=86.81%, cer_entropy_weighted_mean_all_acc=86.81%, cer_prob_product_log_last_acc=86.81%, self_consistency_acc=86.81%, p_true_acc=86.81%, normilized_likelihood_acc=86.81%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=86.81%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 91/500 [8:05:06<38:00:29, 334.55s/it, attention_weighted_confidence_acc=86.81%, cer_entropy_weighted_mean_all_acc=86.81%, cer_prob_product_log_last_acc=86.81%, self_consistency_acc=86.81%, p_true_acc=86.81%, normilized_likelihood_acc=86.81%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=86.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 2.23131473251695
    Answer: 64
    Ground truth:  64
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 2.23131473251695
    Answer: 64
    Ground truth:  64
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 3.9683065861463547
    Answer: 64
    Ground truth:  64
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 0.3125
    Answer: 64
    Ground truth:  64
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 4.3984375
    Answer: 64
    Ground truth:  64
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 1.9501609206199646
    Answer: 64
    Ground truth:  64
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 1.94767364859581
    Answer: 64
    Ground truth:  64
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 1.5832685232162476
    Answer: 64
    Ground truth:  64
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Aaron was able to r...
    Score: 4.225346207618713
    Answer: 64
    Ground truth:  64
Method name: attention_weighted_confidence, running accuracy: 86.95652173913044
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.95652173913044
Method name: cer_prob_product_log_last, running accuracy: 86.95652173913044
Method name: self_consistency, running accuracy: 86.95652173913044
Method name: p_true, running accuracy: 86.95652173913044
Method name: normilized_likelihood, running accuracy: 86.95652173913044
Method name: normilized_entropy, running accuracy: 85.86956521739131
Method name: topk_entropy, running accuracy: 85.86956521739131
Method name: window_entropy, running accuracy: 86.95652173913044

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 91/500 [8:12:25<38:00:29, 334.55s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=86.96%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=85.87%, topk_entropy_acc=85.87%, window_entropy_acc=86.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 92/500 [8:12:25<41:29:43, 366.14s/it, attention_weighted_confidence_acc=86.96%, cer_entropy_weighted_mean_all_acc=86.96%, cer_prob_product_log_last_acc=86.96%, self_consistency_acc=86.96%, p_true_acc=86.96%, normilized_likelihood_acc=86.96%, normilized_entropy_acc=85.87%, topk_entropy_acc=85.87%, window_entropy_acc=86.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 8.199112604176706
    Answer: 1
    Ground truth:  1
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 8.199112604176706
    Answer: 1
    Ground truth:  1
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 13.533969223499298
    Answer: 1
    Ground truth:  1
Method 4: self_consistency
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 1.0
    Answer: 1
    Ground truth:  1
Method 5: p_true
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 13.6953125
    Answer: 1
    Ground truth:  1
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 2.8969716280698776
    Answer: 1
    Ground truth:  1
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 1.1632330417633057
    Answer: 1
    Ground truth:  1
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 1.1564486026763916
    Answer: 1
    Ground truth:  1
Method 9: window_entropy
  Batch 1:
    Text: To find out how much more expensive the boots are on eBay, we need to calculate ...
    Score: 3.905225932598114
    Answer: 1
    Ground truth:  1
Method name: attention_weighted_confidence, running accuracy: 87.09677419354838
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.09677419354838
Method name: cer_prob_product_log_last, running accuracy: 87.09677419354838
Method name: self_consistency, running accuracy: 87.09677419354838
Method name: p_true, running accuracy: 87.09677419354838
Method name: normilized_likelihood, running accuracy: 87.09677419354838
Method name: normilized_entropy, running accuracy: 86.02150537634408
Method name: topk_entropy, running accuracy: 86.02150537634408
Method name: window_entropy, running accuracy: 87.09677419354838

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 92/500 [8:16:12<41:29:43, 366.14s/it, attention_weighted_confidence_acc=87.10%, cer_entropy_weighted_mean_all_acc=87.10%, cer_prob_product_log_last_acc=87.10%, self_consistency_acc=87.10%, p_true_acc=87.10%, normilized_likelihood_acc=87.10%, normilized_entropy_acc=86.02%, topk_entropy_acc=86.02%, window_entropy_acc=87.10%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▊        | 93/500 [8:16:12<36:39:07, 324.20s/it, attention_weighted_confidence_acc=87.10%, cer_entropy_weighted_mean_all_acc=87.10%, cer_prob_product_log_last_acc=87.10%, self_consistency_acc=87.10%, p_true_acc=87.10%, normilized_likelihood_acc=87.10%, normilized_entropy_acc=86.02%, topk_entropy_acc=86.02%, window_entropy_acc=87.10%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 4.532698234137981
    Answer: 11
    Ground truth:  11
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 4.532698234137981
    Answer: 11
    Ground truth:  11
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 8.996744215488434
    Answer: 11
    Ground truth:  11
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 0.5625
    Answer: 11
    Ground truth:  11
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 8.66796875
    Answer: 11
    Ground truth:  11
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 10.118186265230179
    Answer: 11
    Ground truth:  11
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 9.445652216672897
    Answer: 11
    Ground truth:  11
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 7.99832820892334
    Answer: 11
    Ground truth:  11
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. On the first day, th...
    Score: 12.74128770828247
    Answer: 11
    Ground truth:  11
Method name: attention_weighted_confidence, running accuracy: 87.2340425531915
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.2340425531915
Method name: cer_prob_product_log_last, running accuracy: 87.2340425531915
Method name: self_consistency, running accuracy: 87.2340425531915
Method name: p_true, running accuracy: 87.2340425531915
Method name: normilized_likelihood, running accuracy: 87.2340425531915
Method name: normilized_entropy, running accuracy: 86.17021276595744
Method name: topk_entropy, running accuracy: 86.17021276595744
Method name: window_entropy, running accuracy: 87.2340425531915

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▊        | 93/500 [8:23:10<36:39:07, 324.20s/it, attention_weighted_confidence_acc=87.23%, cer_entropy_weighted_mean_all_acc=87.23%, cer_prob_product_log_last_acc=87.23%, self_consistency_acc=87.23%, p_true_acc=87.23%, normilized_likelihood_acc=87.23%, normilized_entropy_acc=86.17%, topk_entropy_acc=86.17%, window_entropy_acc=87.23%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 94/500 [8:23:10<39:44:52, 352.44s/it, attention_weighted_confidence_acc=87.23%, cer_entropy_weighted_mean_all_acc=87.23%, cer_prob_product_log_last_acc=87.23%, self_consistency_acc=87.23%, p_true_acc=87.23%, normilized_likelihood_acc=87.23%, normilized_entropy_acc=86.17%, topk_entropy_acc=86.17%, window_entropy_acc=87.23%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 7.1529316067656366
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 7.1529316067656366
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 14.998924374580383
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 0.9375
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 14.51953125
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 6.034942060709
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 6.481920033693314
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 6.060644268989563
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To find out how much more money Jackie will have if she hires the accountant, we...
    Score: 13.565362751483917
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 87.36842105263159
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.36842105263159
Method name: cer_prob_product_log_last, running accuracy: 87.36842105263159
Method name: self_consistency, running accuracy: 87.36842105263159
Method name: p_true, running accuracy: 87.36842105263159
Method name: normilized_likelihood, running accuracy: 87.36842105263159
Method name: normilized_entropy, running accuracy: 86.31578947368422
Method name: topk_entropy, running accuracy: 86.31578947368422
Method name: window_entropy, running accuracy: 87.36842105263159

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 94/500 [8:27:13<39:44:52, 352.44s/it, attention_weighted_confidence_acc=87.37%, cer_entropy_weighted_mean_all_acc=87.37%, cer_prob_product_log_last_acc=87.37%, self_consistency_acc=87.37%, p_true_acc=87.37%, normilized_likelihood_acc=87.37%, normilized_entropy_acc=86.32%, topk_entropy_acc=86.32%, window_entropy_acc=87.37%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 95/500 [8:27:13<35:56:24, 319.47s/it, attention_weighted_confidence_acc=87.37%, cer_entropy_weighted_mean_all_acc=87.37%, cer_prob_product_log_last_acc=87.37%, self_consistency_acc=87.37%, p_true_acc=87.37%, normilized_likelihood_acc=87.37%, normilized_entropy_acc=86.32%, topk_entropy_acc=86.32%, window_entropy_acc=87.37%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 7.752972341451875
    Answer: 11050
    Ground truth:  11050
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 7.752972341451875
    Answer: 11050
    Ground truth:  11050
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 0.10379863705244752
    Answer: 11050
    Ground truth:  11050
Method 4: self_consistency
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 1.0
    Answer: 11050
    Ground truth:  11050
Method 5: p_true
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 14.80078125
    Answer: 11050
    Ground truth:  11050
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 1.8234233409166336
    Answer: 11050
    Ground truth:  11050
Method 7: normilized_entropy
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 1.4116938561201096
    Answer: 11050
    Ground truth:  11050
Method 8: topk_entropy
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 1.3992902636528015
    Answer: 11050
    Ground truth:  11050
Method 9: window_entropy
  Batch 1:
    Text: To find the cost of filling 170 balloons after the price increase, we need to fi...
    Score: 2.4120715856552124
    Answer: 11050
    Ground truth:  11050
Method name: attention_weighted_confidence, running accuracy: 87.5
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.5
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 87.5
Method name: p_true, running accuracy: 87.5
Method name: normilized_likelihood, running accuracy: 87.5
Method name: normilized_entropy, running accuracy: 86.45833333333334
Method name: topk_entropy, running accuracy: 86.45833333333334
Method name: window_entropy, running accuracy: 87.5

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 95/500 [8:31:36<35:56:24, 319.47s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.46%, topk_entropy_acc=86.46%, window_entropy_acc=87.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 96/500 [8:31:36<33:58:25, 302.74s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=87.50%, p_true_acc=87.50%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.46%, topk_entropy_acc=86.46%, window_entropy_acc=87.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how far off Tim's guess is, we need to first calculate the total num...
    Score: 1.8868745094511776
    Answer: 900
    Ground truth:  36
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how far off Tim's guess is, we need to first calculate the total num...
    Score: 1.8868745094511776
    Answer: 900
    Ground truth:  36
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how far off Tim's guess is, we need to first calculate the total num...
    Score: 3.9980016350746155
    Answer: 900
    Ground truth:  36
Method 4: self_consistency
  Batch 1:
    Text: To find the number of jelly beans in the jar, we need to first find the volume o...
    Score: 0.25
    Answer: 36
    Ground truth:  36
Method 5: p_true
  Batch 1:
    Text: To find the number of jelly beans in the jar, we need to first find the volume o...
    Score: 2.8798828125
    Answer: 36
    Ground truth:  36
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how far off Tim's guess is, we need to first calculate the total num...
    Score: 4.762209355831146
    Answer: 900
    Ground truth:  36
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how far off Tim's guess is, we need to first calculate the total num...
    Score: 4.597263991832733
    Answer: 900
    Ground truth:  36
Method 8: topk_entropy
  Batch 1:
    Text: To find out how far off Tim's guess is, we need to first calculate the total num...
    Score: 3.8005969524383545
    Answer: 900
    Ground truth:  36
Method 9: window_entropy
  Batch 1:
    Text: To find out how far off Tim's guess is, we need to first calculate the total num...
    Score: 4.719348669052124
    Answer: 900
    Ground truth:  36
Method name: attention_weighted_confidence, running accuracy: 86.5979381443299
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.5979381443299
Method name: cer_prob_product_log_last, running accuracy: 86.5979381443299
Method name: self_consistency, running accuracy: 87.62886597938144
Method name: p_true, running accuracy: 87.62886597938144
Method name: normilized_likelihood, running accuracy: 86.5979381443299
Method name: normilized_entropy, running accuracy: 85.56701030927834
Method name: topk_entropy, running accuracy: 85.56701030927834
Method name: window_entropy, running accuracy: 86.5979381443299

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 96/500 [8:44:01<33:58:25, 302.74s/it, attention_weighted_confidence_acc=86.60%, cer_entropy_weighted_mean_all_acc=86.60%, cer_prob_product_log_last_acc=86.60%, self_consistency_acc=87.63%, p_true_acc=87.63%, normilized_likelihood_acc=86.60%, normilized_entropy_acc=85.57%, topk_entropy_acc=85.57%, window_entropy_acc=86.60%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 97/500 [8:44:01<48:43:55, 435.32s/it, attention_weighted_confidence_acc=86.60%, cer_entropy_weighted_mean_all_acc=86.60%, cer_prob_product_log_last_acc=86.60%, self_consistency_acc=87.63%, p_true_acc=87.63%, normilized_likelihood_acc=86.60%, normilized_entropy_acc=85.57%, topk_entropy_acc=85.57%, window_entropy_acc=86.60%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 7.619703225541242
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 7.619703225541242
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 15.997406005859375
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 14.953125
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 2.606343299150467
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 1.0173909664154053
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 1.0218427777290344
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Each of the two gir...
    Score: 3.0176392793655396
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 86.73469387755102
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.73469387755102
Method name: cer_prob_product_log_last, running accuracy: 86.73469387755102
Method name: self_consistency, running accuracy: 87.75510204081633
Method name: p_true, running accuracy: 87.75510204081633
Method name: normilized_likelihood, running accuracy: 86.73469387755102
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 85.71428571428571
Method name: window_entropy, running accuracy: 86.73469387755102

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 97/500 [8:48:12<48:43:55, 435.32s/it, attention_weighted_confidence_acc=86.73%, cer_entropy_weighted_mean_all_acc=86.73%, cer_prob_product_log_last_acc=86.73%, self_consistency_acc=87.76%, p_true_acc=87.76%, normilized_likelihood_acc=86.73%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=86.73%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|█▉        | 98/500 [8:48:12<42:26:35, 380.09s/it, attention_weighted_confidence_acc=86.73%, cer_entropy_weighted_mean_all_acc=86.73%, cer_prob_product_log_last_acc=86.73%, self_consistency_acc=87.76%, p_true_acc=87.76%, normilized_likelihood_acc=86.73%, normilized_entropy_acc=85.71%, topk_entropy_acc=85.71%, window_entropy_acc=86.73%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 6.679428153611469
    Answer: 160
    Ground truth:  160
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 6.679428153611469
    Answer: 160
    Ground truth:  160
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 12.999961733818054
    Answer: 160
    Ground truth:  160
Method 4: self_consistency
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 0.8125
    Answer: 160
    Ground truth:  160
Method 5: p_true
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 11.5703125
    Answer: 160
    Ground truth:  160
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 1.6309484243392944
    Answer: 160
    Ground truth:  160
Method 7: normilized_entropy
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 1.1240073889493942
    Answer: 160
    Ground truth:  160
Method 8: topk_entropy
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 1.1222999542951584
    Answer: 160
    Ground truth:  160
Method 9: window_entropy
  Batch 1:
    Text: To find the profit, let's break it down into steps:

1. First, we need to find t...
    Score: 2.2723649740219116
    Answer: 160
    Ground truth:  160
Method name: attention_weighted_confidence, running accuracy: 86.86868686868688
Method name: cer_entropy_weighted_mean_all, running accuracy: 86.86868686868688
Method name: cer_prob_product_log_last, running accuracy: 86.86868686868688
Method name: self_consistency, running accuracy: 87.87878787878788
Method name: p_true, running accuracy: 87.87878787878788
Method name: normilized_likelihood, running accuracy: 86.86868686868688
Method name: normilized_entropy, running accuracy: 85.85858585858585
Method name: topk_entropy, running accuracy: 85.85858585858585
Method name: window_entropy, running accuracy: 86.86868686868688

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|█▉        | 98/500 [8:52:29<42:26:35, 380.09s/it, attention_weighted_confidence_acc=86.87%, cer_entropy_weighted_mean_all_acc=86.87%, cer_prob_product_log_last_acc=86.87%, self_consistency_acc=87.88%, p_true_acc=87.88%, normilized_likelihood_acc=86.87%, normilized_entropy_acc=85.86%, topk_entropy_acc=85.86%, window_entropy_acc=86.87%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|█▉        | 99/500 [8:52:29<38:13:18, 343.14s/it, attention_weighted_confidence_acc=86.87%, cer_entropy_weighted_mean_all_acc=86.87%, cer_prob_product_log_last_acc=86.87%, self_consistency_acc=87.88%, p_true_acc=87.88%, normilized_likelihood_acc=86.87%, normilized_entropy_acc=85.86%, topk_entropy_acc=85.86%, window_entropy_acc=86.87%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 8.17340267952009
    Answer: 91
    Ground truth:  91
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 8.17340267952009
    Answer: 91
    Ground truth:  91
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 15.978989064693451
    Answer: 91
    Ground truth:  91
Method 4: self_consistency
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 1.0
    Answer: 91
    Ground truth:  91
Method 5: p_true
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 15.0703125
    Answer: 91
    Ground truth:  91
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 5.077178448438644
    Answer: 91
    Ground truth:  91
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 3.745313882827759
    Answer: 91
    Ground truth:  91
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 3.5552310198545456
    Answer: 91
    Ground truth:  91
Method 9: window_entropy
  Batch 1:
    Text: To find out how many eggs the family will consume in a week, we need to calculat...
    Score: 12.137475907802582
    Answer: 91
    Ground truth:  91
Method name: attention_weighted_confidence, running accuracy: 87.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.0
Method name: cer_prob_product_log_last, running accuracy: 87.0
Method name: self_consistency, running accuracy: 88.0
Method name: p_true, running accuracy: 88.0
Method name: normilized_likelihood, running accuracy: 87.0
Method name: normilized_entropy, running accuracy: 86.0
Method name: topk_entropy, running accuracy: 86.0
Method name: window_entropy, running accuracy: 87.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|█▉        | 99/500 [8:57:44<38:13:18, 343.14s/it, attention_weighted_confidence_acc=87.00%, cer_entropy_weighted_mean_all_acc=87.00%, cer_prob_product_log_last_acc=87.00%, self_consistency_acc=88.00%, p_true_acc=88.00%, normilized_likelihood_acc=87.00%, normilized_entropy_acc=86.00%, topk_entropy_acc=86.00%, window_entropy_acc=87.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 100/500 [8:57:44<37:10:05, 334.51s/it, attention_weighted_confidence_acc=87.00%, cer_entropy_weighted_mean_all_acc=87.00%, cer_prob_product_log_last_acc=87.00%, self_consistency_acc=88.00%, p_true_acc=88.00%, normilized_likelihood_acc=87.00%, normilized_entropy_acc=86.00%, topk_entropy_acc=86.00%, window_entropy_acc=87.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 8.688361784796747
    Answer: 34
    Ground truth:  34
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 8.688361784796747
    Answer: 34
    Ground truth:  34
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 15.998565912246704
    Answer: 34
    Ground truth:  34
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 1.0
    Answer: 34
    Ground truth:  34
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 14.82421875
    Answer: 34
    Ground truth:  34
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 0.7564201056957245
    Answer: 34
    Ground truth:  34
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 0.9600705951452255
    Answer: 34
    Ground truth:  34
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 0.9605401083827019
    Answer: 34
    Ground truth:  34
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step.

1. They have 9 red sticks.
2. They have 5 mor...
    Score: 2.757954925298691
    Answer: 34
    Ground truth:  34
Method name: attention_weighted_confidence, running accuracy: 87.12871287128714
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.12871287128714
Method name: cer_prob_product_log_last, running accuracy: 87.12871287128714
Method name: self_consistency, running accuracy: 88.11881188118812
Method name: p_true, running accuracy: 88.11881188118812
Method name: normilized_likelihood, running accuracy: 87.12871287128714
Method name: normilized_entropy, running accuracy: 86.13861386138613
Method name: topk_entropy, running accuracy: 86.13861386138613
Method name: window_entropy, running accuracy: 87.12871287128714

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 100/500 [9:01:34<37:10:05, 334.51s/it, attention_weighted_confidence_acc=87.13%, cer_entropy_weighted_mean_all_acc=87.13%, cer_prob_product_log_last_acc=87.13%, self_consistency_acc=88.12%, p_true_acc=88.12%, normilized_likelihood_acc=87.13%, normilized_entropy_acc=86.14%, topk_entropy_acc=86.14%, window_entropy_acc=87.13%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 101/500 [9:01:34<33:37:02, 303.31s/it, attention_weighted_confidence_acc=87.13%, cer_entropy_weighted_mean_all_acc=87.13%, cer_prob_product_log_last_acc=87.13%, self_consistency_acc=88.12%, p_true_acc=88.12%, normilized_likelihood_acc=87.13%, normilized_entropy_acc=86.14%, topk_entropy_acc=86.14%, window_entropy_acc=87.13%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 7.284391888873635
    Answer: 1
    Ground truth:  1
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 7.284391888873635
    Answer: 1
    Ground truth:  1
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 12.539765983819962
    Answer: 1
    Ground truth:  1
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 0.875
    Answer: 1
    Ground truth:  1
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 9.666015625
    Answer: 1
    Ground truth:  1
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 2.932268738746643
    Answer: 1
    Ground truth:  1
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 2.3355794847011566
    Answer: 1
    Ground truth:  1
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 1.9468939751386642
    Answer: 1
    Ground truth:  1
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. First, we need to find out how ma...
    Score: 18.988421261310577
    Answer: 1
    Ground truth:  1
Method name: attention_weighted_confidence, running accuracy: 87.25490196078431
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.25490196078431
Method name: cer_prob_product_log_last, running accuracy: 87.25490196078431
Method name: self_consistency, running accuracy: 88.23529411764706
Method name: p_true, running accuracy: 88.23529411764706
Method name: normilized_likelihood, running accuracy: 87.25490196078431
Method name: normilized_entropy, running accuracy: 86.27450980392157
Method name: topk_entropy, running accuracy: 86.27450980392157
Method name: window_entropy, running accuracy: 87.25490196078431

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 101/500 [9:07:27<33:37:02, 303.31s/it, attention_weighted_confidence_acc=87.25%, cer_entropy_weighted_mean_all_acc=87.25%, cer_prob_product_log_last_acc=87.25%, self_consistency_acc=88.24%, p_true_acc=88.24%, normilized_likelihood_acc=87.25%, normilized_entropy_acc=86.27%, topk_entropy_acc=86.27%, window_entropy_acc=87.25%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 102/500 [9:07:27<35:10:06, 318.11s/it, attention_weighted_confidence_acc=87.25%, cer_entropy_weighted_mean_all_acc=87.25%, cer_prob_product_log_last_acc=87.25%, self_consistency_acc=88.24%, p_true_acc=88.24%, normilized_likelihood_acc=87.25%, normilized_entropy_acc=86.27%, topk_entropy_acc=86.27%, window_entropy_acc=87.25%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 6.107371876087781
    Answer: 220
    Ground truth:  220
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 6.107371876087781
    Answer: 220
    Ground truth:  220
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 12.995583772659302
    Answer: 220
    Ground truth:  220
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 0.8125
    Answer: 220
    Ground truth:  220
Method 5: p_true
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 8.41162109375
    Answer: 220
    Ground truth:  220
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 13.310997128486633
    Answer: 220
    Ground truth:  220
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 13.581539943814278
    Answer: 220
    Ground truth:  220
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 11.194988384842873
    Answer: 220
    Ground truth:  220
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Calculate the total amount Janeth needs to return with an additional 10%...
    Score: 16.804151713848114
    Answer: 220
    Ground truth:  220
Method name: attention_weighted_confidence, running accuracy: 87.37864077669903
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.37864077669903
Method name: cer_prob_product_log_last, running accuracy: 87.37864077669903
Method name: self_consistency, running accuracy: 88.3495145631068
Method name: p_true, running accuracy: 88.3495145631068
Method name: normilized_likelihood, running accuracy: 87.37864077669903
Method name: normilized_entropy, running accuracy: 86.40776699029125
Method name: topk_entropy, running accuracy: 86.40776699029125
Method name: window_entropy, running accuracy: 87.37864077669903

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 102/500 [9:14:00<35:10:06, 318.11s/it, attention_weighted_confidence_acc=87.38%, cer_entropy_weighted_mean_all_acc=87.38%, cer_prob_product_log_last_acc=87.38%, self_consistency_acc=88.35%, p_true_acc=88.35%, normilized_likelihood_acc=87.38%, normilized_entropy_acc=86.41%, topk_entropy_acc=86.41%, window_entropy_acc=87.38%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 103/500 [9:14:00<37:33:58, 340.65s/it, attention_weighted_confidence_acc=87.38%, cer_entropy_weighted_mean_all_acc=87.38%, cer_prob_product_log_last_acc=87.38%, self_consistency_acc=88.35%, p_true_acc=88.35%, normilized_likelihood_acc=87.38%, normilized_entropy_acc=86.41%, topk_entropy_acc=86.41%, window_entropy_acc=87.38%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 6.273033889248423
    Answer: 280
    Ground truth:  280
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 6.273033889248423
    Answer: 280
    Ground truth:  280
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 12.823681771755219
    Answer: 280
    Ground truth:  280
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 0.8125
    Answer: 280
    Ground truth:  280
Method 5: p_true
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 6.427734375
    Answer: 280
    Ground truth:  280
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 5.703210532665253
    Answer: 280
    Ground truth:  280
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 6.525525510311127
    Answer: 280
    Ground truth:  280
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 5.958906516432762
    Answer: 280
    Ground truth:  280
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of children in the drill, we need to multiply the numbe...
    Score: 14.661746054887772
    Answer: 280
    Ground truth:  280
Method name: attention_weighted_confidence, running accuracy: 87.5
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.5
Method name: cer_prob_product_log_last, running accuracy: 87.5
Method name: self_consistency, running accuracy: 88.46153846153845
Method name: p_true, running accuracy: 88.46153846153845
Method name: normilized_likelihood, running accuracy: 87.5
Method name: normilized_entropy, running accuracy: 86.53846153846155
Method name: topk_entropy, running accuracy: 86.53846153846155
Method name: window_entropy, running accuracy: 87.5

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 103/500 [9:17:34<37:33:58, 340.65s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=88.46%, p_true_acc=88.46%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.54%, topk_entropy_acc=86.54%, window_entropy_acc=87.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 104/500 [9:17:34<33:18:22, 302.78s/it, attention_weighted_confidence_acc=87.50%, cer_entropy_weighted_mean_all_acc=87.50%, cer_prob_product_log_last_acc=87.50%, self_consistency_acc=88.46%, p_true_acc=88.46%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.54%, topk_entropy_acc=86.54%, window_entropy_acc=87.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 7.7726553792127975
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 7.7726553792127975
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 13.687492787837982
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 0.9375
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 11.58203125
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 2.524794355034828
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 1.7759892493486404
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 1.7265092730522156
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Tobias howls for a total of 20 s...
    Score: 7.341430902481079
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 87.61904761904762
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.61904761904762
Method name: cer_prob_product_log_last, running accuracy: 87.61904761904762
Method name: self_consistency, running accuracy: 88.57142857142857
Method name: p_true, running accuracy: 88.57142857142857
Method name: normilized_likelihood, running accuracy: 87.61904761904762
Method name: normilized_entropy, running accuracy: 86.66666666666667
Method name: topk_entropy, running accuracy: 86.66666666666667
Method name: window_entropy, running accuracy: 87.61904761904762

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 104/500 [9:23:01<33:18:22, 302.78s/it, attention_weighted_confidence_acc=87.62%, cer_entropy_weighted_mean_all_acc=87.62%, cer_prob_product_log_last_acc=87.62%, self_consistency_acc=88.57%, p_true_acc=88.57%, normilized_likelihood_acc=87.62%, normilized_entropy_acc=86.67%, topk_entropy_acc=86.67%, window_entropy_acc=87.62%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 105/500 [9:23:01<34:00:31, 309.95s/it, attention_weighted_confidence_acc=87.62%, cer_entropy_weighted_mean_all_acc=87.62%, cer_prob_product_log_last_acc=87.62%, self_consistency_acc=88.57%, p_true_acc=88.57%, normilized_likelihood_acc=87.62%, normilized_entropy_acc=86.67%, topk_entropy_acc=86.67%, window_entropy_acc=87.62%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 8.554765836061224
    Answer: 880
    Ground truth:  880
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 8.554765836061224
    Answer: 880
    Ground truth:  880
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 15.967460334300995
    Answer: 880
    Ground truth:  880
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 1.0
    Answer: 880
    Ground truth:  880
Method 5: p_true
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 10.99609375
    Answer: 880
    Ground truth:  880
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 1.188931442797184
    Answer: 880
    Ground truth:  880
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 0.7772621363401413
    Answer: 880
    Ground truth:  880
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 0.7770283818244934
    Answer: 880
    Ground truth:  880
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount of money Dillon will spend, we need to first determine ...
    Score: 8.128632962703705
    Answer: 880
    Ground truth:  880
Method name: attention_weighted_confidence, running accuracy: 87.73584905660378
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.73584905660378
Method name: cer_prob_product_log_last, running accuracy: 87.73584905660378
Method name: self_consistency, running accuracy: 88.67924528301887
Method name: p_true, running accuracy: 88.67924528301887
Method name: normilized_likelihood, running accuracy: 87.73584905660378
Method name: normilized_entropy, running accuracy: 86.79245283018868
Method name: topk_entropy, running accuracy: 86.79245283018868
Method name: window_entropy, running accuracy: 87.73584905660378

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 105/500 [9:28:44<34:00:31, 309.95s/it, attention_weighted_confidence_acc=87.74%, cer_entropy_weighted_mean_all_acc=87.74%, cer_prob_product_log_last_acc=87.74%, self_consistency_acc=88.68%, p_true_acc=88.68%, normilized_likelihood_acc=87.74%, normilized_entropy_acc=86.79%, topk_entropy_acc=86.79%, window_entropy_acc=87.74%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 106/500 [9:28:44<34:59:44, 319.76s/it, attention_weighted_confidence_acc=87.74%, cer_entropy_weighted_mean_all_acc=87.74%, cer_prob_product_log_last_acc=87.74%, self_consistency_acc=88.68%, p_true_acc=88.68%, normilized_likelihood_acc=87.74%, normilized_entropy_acc=86.79%, topk_entropy_acc=86.79%, window_entropy_acc=87.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 7.601049847741984
    Answer: 40
    Ground truth:  40
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 7.601049847741984
    Answer: 40
    Ground truth:  40
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 14.999978065490723
    Answer: 40
    Ground truth:  40
Method 4: self_consistency
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 0.9375
    Answer: 40
    Ground truth:  40
Method 5: p_true
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 12.341796875
    Answer: 40
    Ground truth:  40
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 6.687185823917389
    Answer: 40
    Ground truth:  40
Method 7: normilized_entropy
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 6.098636493086815
    Answer: 40
    Ground truth:  40
Method 8: topk_entropy
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 5.376262933015823
    Answer: 40
    Ground truth:  40
Method 9: window_entropy
  Batch 1:
    Text: To find the revenue from Thursday, we need to calculate the total charged for tr...
    Score: 18.17799997329712
    Answer: 40
    Ground truth:  40
Method name: attention_weighted_confidence, running accuracy: 87.85046728971963
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.85046728971963
Method name: cer_prob_product_log_last, running accuracy: 87.85046728971963
Method name: self_consistency, running accuracy: 88.78504672897196
Method name: p_true, running accuracy: 88.78504672897196
Method name: normilized_likelihood, running accuracy: 87.85046728971963
Method name: normilized_entropy, running accuracy: 86.91588785046729
Method name: topk_entropy, running accuracy: 86.91588785046729
Method name: window_entropy, running accuracy: 87.85046728971963

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 106/500 [9:33:44<34:59:44, 319.76s/it, attention_weighted_confidence_acc=87.85%, cer_entropy_weighted_mean_all_acc=87.85%, cer_prob_product_log_last_acc=87.85%, self_consistency_acc=88.79%, p_true_acc=88.79%, normilized_likelihood_acc=87.85%, normilized_entropy_acc=86.92%, topk_entropy_acc=86.92%, window_entropy_acc=87.85%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██▏       | 107/500 [9:33:44<34:15:44, 313.85s/it, attention_weighted_confidence_acc=87.85%, cer_entropy_weighted_mean_all_acc=87.85%, cer_prob_product_log_last_acc=87.85%, self_consistency_acc=88.79%, p_true_acc=88.79%, normilized_likelihood_acc=87.85%, normilized_entropy_acc=86.92%, topk_entropy_acc=86.92%, window_entropy_acc=87.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 8.405669227673728
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 8.405669227673728
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 15.999322414398193
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 14.79296875
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 2.151592254638672
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 1.1058734208345413
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 1.0997823923826218
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To find out how many more weeks Melanie will need to collect 200 toothpicks, we ...
    Score: 4.911733031272888
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 87.96296296296296
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.96296296296296
Method name: cer_prob_product_log_last, running accuracy: 87.96296296296296
Method name: self_consistency, running accuracy: 88.88888888888889
Method name: p_true, running accuracy: 88.88888888888889
Method name: normilized_likelihood, running accuracy: 87.96296296296296
Method name: normilized_entropy, running accuracy: 87.03703703703704
Method name: topk_entropy, running accuracy: 87.03703703703704
Method name: window_entropy, running accuracy: 87.96296296296296

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██▏       | 107/500 [9:38:58<34:15:44, 313.85s/it, attention_weighted_confidence_acc=87.96%, cer_entropy_weighted_mean_all_acc=87.96%, cer_prob_product_log_last_acc=87.96%, self_consistency_acc=88.89%, p_true_acc=88.89%, normilized_likelihood_acc=87.96%, normilized_entropy_acc=87.04%, topk_entropy_acc=87.04%, window_entropy_acc=87.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 108/500 [9:38:58<34:10:38, 313.87s/it, attention_weighted_confidence_acc=87.96%, cer_entropy_weighted_mean_all_acc=87.96%, cer_prob_product_log_last_acc=87.96%, self_consistency_acc=88.89%, p_true_acc=88.89%, normilized_likelihood_acc=87.96%, normilized_entropy_acc=87.04%, topk_entropy_acc=87.04%, window_entropy_acc=87.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 7.710543923103999
    Answer: 385000
    Ground truth:  385000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 7.710543923103999
    Answer: 385000
    Ground truth:  385000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 0.31002254852078437
    Answer: 385000
    Ground truth:  385000
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 1.0
    Answer: 385000
    Ground truth:  385000
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 14.42578125
    Answer: 385000
    Ground truth:  385000
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 3.373375430703163
    Answer: 385000
    Ground truth:  385000
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 2.1352849900722504
    Answer: 385000
    Ground truth:  385000
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 2.1316055357456207
    Answer: 385000
    Ground truth:  385000
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll follow these steps:

1. Determine the original cost...
    Score: 5.011639714241028
    Answer: 385000
    Ground truth:  385000
Method name: attention_weighted_confidence, running accuracy: 88.07339449541286
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.07339449541286
Method name: cer_prob_product_log_last, running accuracy: 88.07339449541286
Method name: self_consistency, running accuracy: 88.9908256880734
Method name: p_true, running accuracy: 88.9908256880734
Method name: normilized_likelihood, running accuracy: 88.07339449541286
Method name: normilized_entropy, running accuracy: 87.1559633027523
Method name: topk_entropy, running accuracy: 87.1559633027523
Method name: window_entropy, running accuracy: 88.07339449541286

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 108/500 [9:43:31<34:10:38, 313.87s/it, attention_weighted_confidence_acc=88.07%, cer_entropy_weighted_mean_all_acc=88.07%, cer_prob_product_log_last_acc=88.07%, self_consistency_acc=88.99%, p_true_acc=88.99%, normilized_likelihood_acc=88.07%, normilized_entropy_acc=87.16%, topk_entropy_acc=87.16%, window_entropy_acc=88.07%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 109/500 [9:43:31<32:45:32, 301.62s/it, attention_weighted_confidence_acc=88.07%, cer_entropy_weighted_mean_all_acc=88.07%, cer_prob_product_log_last_acc=88.07%, self_consistency_acc=88.99%, p_true_acc=88.99%, normilized_likelihood_acc=88.07%, normilized_entropy_acc=87.16%, topk_entropy_acc=87.16%, window_entropy_acc=88.07%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 8.03180234624972
    Answer: 120
    Ground truth:  120
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 8.03180234624972
    Answer: 120
    Ground truth:  120
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 15.999195277690887
    Answer: 120
    Ground truth:  120
Method 4: self_consistency
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 1.0
    Answer: 120
    Ground truth:  120
Method 5: p_true
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 13.421875
    Answer: 120
    Ground truth:  120
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 2.538604751229286
    Answer: 120
    Ground truth:  120
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 1.568109393119812
    Answer: 120
    Ground truth:  120
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 1.5454154163599014
    Answer: 120
    Ground truth:  120
Method 9: window_entropy
  Batch 1:
    Text: To find out how many miles Pancho walks in a week, we need to calculate the tota...
    Score: 3.6330941915512085
    Answer: 120
    Ground truth:  120
Method name: attention_weighted_confidence, running accuracy: 88.18181818181819
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.18181818181819
Method name: cer_prob_product_log_last, running accuracy: 88.18181818181819
Method name: self_consistency, running accuracy: 89.0909090909091
Method name: p_true, running accuracy: 89.0909090909091
Method name: normilized_likelihood, running accuracy: 88.18181818181819
Method name: normilized_entropy, running accuracy: 87.27272727272727
Method name: topk_entropy, running accuracy: 87.27272727272727
Method name: window_entropy, running accuracy: 88.18181818181819

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 109/500 [9:47:16<32:45:32, 301.62s/it, attention_weighted_confidence_acc=88.18%, cer_entropy_weighted_mean_all_acc=88.18%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=89.09%, p_true_acc=89.09%, normilized_likelihood_acc=88.18%, normilized_entropy_acc=87.27%, topk_entropy_acc=87.27%, window_entropy_acc=88.18%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 110/500 [9:47:16<30:11:10, 278.64s/it, attention_weighted_confidence_acc=88.18%, cer_entropy_weighted_mean_all_acc=88.18%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=89.09%, p_true_acc=89.09%, normilized_likelihood_acc=88.18%, normilized_entropy_acc=87.27%, topk_entropy_acc=87.27%, window_entropy_acc=88.18%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 5.209274959280666
    Answer: 50.0
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 5.209274959280666
    Answer: 50.0
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine how much money Tyson will spend on the meat and cheese, we need to ...
    Score: 4.999957919120789
    Answer: 50
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 0.625
    Answer: 50.0
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 6.71484375
    Answer: 50.0
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 2.437330536544323
    Answer: 50.0
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 2.516290009021759
    Answer: 50.0
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 2.3469079583883286
    Answer: 50.0
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Tyson would spend on the meat and cheese, we need to ...
    Score: 11.402427554130554
    Answer: 50.0
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 88.28828828828829
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.28828828828829
Method name: cer_prob_product_log_last, running accuracy: 88.28828828828829
Method name: self_consistency, running accuracy: 89.1891891891892
Method name: p_true, running accuracy: 89.1891891891892
Method name: normilized_likelihood, running accuracy: 88.28828828828829
Method name: normilized_entropy, running accuracy: 87.38738738738738
Method name: topk_entropy, running accuracy: 87.38738738738738
Method name: window_entropy, running accuracy: 88.28828828828829

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 110/500 [9:54:03<30:11:10, 278.64s/it, attention_weighted_confidence_acc=88.29%, cer_entropy_weighted_mean_all_acc=88.29%, cer_prob_product_log_last_acc=88.29%, self_consistency_acc=89.19%, p_true_acc=89.19%, normilized_likelihood_acc=88.29%, normilized_entropy_acc=87.39%, topk_entropy_acc=87.39%, window_entropy_acc=88.29%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 111/500 [9:54:03<34:16:58, 317.27s/it, attention_weighted_confidence_acc=88.29%, cer_entropy_weighted_mean_all_acc=88.29%, cer_prob_product_log_last_acc=88.29%, self_consistency_acc=89.19%, p_true_acc=89.19%, normilized_likelihood_acc=88.29%, normilized_entropy_acc=87.39%, topk_entropy_acc=87.39%, window_entropy_acc=88.29%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 6.955256858912437
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 6.955256858912437
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 13.852481186389923
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 0.875
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 11.359375
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 3.343737542629242
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 1.620055466890335
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 1.594321608543396
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: To find the amount of money Brittany's mom gave the cashier, we need to first de...
    Score: 4.047482252120972
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 88.39285714285714
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.39285714285714
Method name: cer_prob_product_log_last, running accuracy: 88.39285714285714
Method name: self_consistency, running accuracy: 89.28571428571429
Method name: p_true, running accuracy: 89.28571428571429
Method name: normilized_likelihood, running accuracy: 88.39285714285714
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 88.39285714285714

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 111/500 [9:57:47<34:16:58, 317.27s/it, attention_weighted_confidence_acc=88.39%, cer_entropy_weighted_mean_all_acc=88.39%, cer_prob_product_log_last_acc=88.39%, self_consistency_acc=89.29%, p_true_acc=89.29%, normilized_likelihood_acc=88.39%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.39%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 112/500 [9:57:47<31:11:17, 289.37s/it, attention_weighted_confidence_acc=88.39%, cer_entropy_weighted_mean_all_acc=88.39%, cer_prob_product_log_last_acc=88.39%, self_consistency_acc=89.29%, p_true_acc=89.29%, normilized_likelihood_acc=88.39%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.39%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 6.399068359024616
    Answer: 75
    Ground truth:  75
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 6.399068359024616
    Answer: 75
    Ground truth:  75
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 12.999524056911469
    Answer: 75
    Ground truth:  75
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 0.8125
    Answer: 75
    Ground truth:  75
Method 5: p_true
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 10.70703125
    Answer: 75
    Ground truth:  75
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 5.163585796952248
    Answer: 75
    Ground truth:  75
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 4.096467047929764
    Answer: 75
    Ground truth:  75
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 3.9077246487140656
    Answer: 75
    Ground truth:  75
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Susan earns, we need to calculate the total time she ...
    Score: 5.908477783203125
    Answer: 75
    Ground truth:  75
Method name: attention_weighted_confidence, running accuracy: 88.49557522123894
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.49557522123894
Method name: cer_prob_product_log_last, running accuracy: 88.49557522123894
Method name: self_consistency, running accuracy: 89.38053097345133
Method name: p_true, running accuracy: 89.38053097345133
Method name: normilized_likelihood, running accuracy: 88.49557522123894
Method name: normilized_entropy, running accuracy: 87.61061946902655
Method name: topk_entropy, running accuracy: 87.61061946902655
Method name: window_entropy, running accuracy: 88.49557522123894

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 112/500 [10:02:34<31:11:17, 289.37s/it, attention_weighted_confidence_acc=88.50%, cer_entropy_weighted_mean_all_acc=88.50%, cer_prob_product_log_last_acc=88.50%, self_consistency_acc=89.38%, p_true_acc=89.38%, normilized_likelihood_acc=88.50%, normilized_entropy_acc=87.61%, topk_entropy_acc=87.61%, window_entropy_acc=88.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 113/500 [10:02:34<31:01:38, 288.63s/it, attention_weighted_confidence_acc=88.50%, cer_entropy_weighted_mean_all_acc=88.50%, cer_prob_product_log_last_acc=88.50%, self_consistency_acc=89.38%, p_true_acc=89.38%, normilized_likelihood_acc=88.50%, normilized_entropy_acc=87.61%, topk_entropy_acc=87.61%, window_entropy_acc=88.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 7.190509990048399
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 7.190509990048399
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 13.344832181930542
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 0.875
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 12.8359375
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 3.634296953678131
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 2.3196679800748825
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 2.2217700630426407
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: Step 1: First, calculate the initial number of occupied beds in the hospital.

S...
    Score: 6.187067210674286
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 88.59649122807018
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.59649122807018
Method name: cer_prob_product_log_last, running accuracy: 88.59649122807018
Method name: self_consistency, running accuracy: 89.47368421052632
Method name: p_true, running accuracy: 89.47368421052632
Method name: normilized_likelihood, running accuracy: 88.59649122807018
Method name: normilized_entropy, running accuracy: 87.71929824561403
Method name: topk_entropy, running accuracy: 87.71929824561403
Method name: window_entropy, running accuracy: 88.59649122807018

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 113/500 [10:08:16<31:01:38, 288.63s/it, attention_weighted_confidence_acc=88.60%, cer_entropy_weighted_mean_all_acc=88.60%, cer_prob_product_log_last_acc=88.60%, self_consistency_acc=89.47%, p_true_acc=89.47%, normilized_likelihood_acc=88.60%, normilized_entropy_acc=87.72%, topk_entropy_acc=87.72%, window_entropy_acc=88.60%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 114/500 [10:08:16<32:39:25, 304.57s/it, attention_weighted_confidence_acc=88.60%, cer_entropy_weighted_mean_all_acc=88.60%, cer_prob_product_log_last_acc=88.60%, self_consistency_acc=89.47%, p_true_acc=89.47%, normilized_likelihood_acc=88.60%, normilized_entropy_acc=87.72%, topk_entropy_acc=87.72%, window_entropy_acc=88.60%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 8.512909920899716
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 8.512909920899716
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 15.999868512153625
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 12.21484375
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 2.9146716073155403
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 2.2143719643354416
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 2.1356621459126472
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

Initially, Tate has:
- Male guppies:...
    Score: 8.45564466714859
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 88.69565217391305
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.69565217391305
Method name: cer_prob_product_log_last, running accuracy: 88.69565217391305
Method name: self_consistency, running accuracy: 89.56521739130436
Method name: p_true, running accuracy: 89.56521739130436
Method name: normilized_likelihood, running accuracy: 88.69565217391305
Method name: normilized_entropy, running accuracy: 87.82608695652175
Method name: topk_entropy, running accuracy: 87.82608695652175
Method name: window_entropy, running accuracy: 88.69565217391305

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 114/500 [10:14:31<32:39:25, 304.57s/it, attention_weighted_confidence_acc=88.70%, cer_entropy_weighted_mean_all_acc=88.70%, cer_prob_product_log_last_acc=88.70%, self_consistency_acc=89.57%, p_true_acc=89.57%, normilized_likelihood_acc=88.70%, normilized_entropy_acc=87.83%, topk_entropy_acc=87.83%, window_entropy_acc=88.70%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 115/500 [10:14:31<34:50:05, 325.73s/it, attention_weighted_confidence_acc=88.70%, cer_entropy_weighted_mean_all_acc=88.70%, cer_prob_product_log_last_acc=88.70%, self_consistency_acc=89.57%, p_true_acc=89.57%, normilized_likelihood_acc=88.70%, normilized_entropy_acc=87.83%, topk_entropy_acc=87.83%, window_entropy_acc=88.70%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 5.442106512815326
    Answer: 40
    Ground truth:  40
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 5.442106512815326
    Answer: 40
    Ground truth:  40
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 10.999749302864075
    Answer: 40
    Ground truth:  40
Method 4: self_consistency
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 0.6875
    Answer: 40
    Ground truth:  40
Method 5: p_true
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 9.92578125
    Answer: 40
    Ground truth:  40
Method 6: normilized_likelihood
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 9.925892293453217
    Answer: 40
    Ground truth:  40
Method 7: normilized_entropy
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 9.439553767442703
    Answer: 40
    Ground truth:  40
Method 8: topk_entropy
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 8.576397269964218
    Answer: 40
    Ground truth:  40
Method 9: window_entropy
  Batch 1:
    Text: 1. First, we need to find out how many video games Brian had before losing 5.
  ...
    Score: 17.08443307876587
    Answer: 40
    Ground truth:  40
Method name: attention_weighted_confidence, running accuracy: 88.79310344827587
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.79310344827587
Method name: cer_prob_product_log_last, running accuracy: 88.79310344827587
Method name: self_consistency, running accuracy: 89.65517241379311
Method name: p_true, running accuracy: 89.65517241379311
Method name: normilized_likelihood, running accuracy: 88.79310344827587
Method name: normilized_entropy, running accuracy: 87.93103448275862
Method name: topk_entropy, running accuracy: 87.93103448275862
Method name: window_entropy, running accuracy: 88.79310344827587

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 115/500 [10:18:48<34:50:05, 325.73s/it, attention_weighted_confidence_acc=88.79%, cer_entropy_weighted_mean_all_acc=88.79%, cer_prob_product_log_last_acc=88.79%, self_consistency_acc=89.66%, p_true_acc=89.66%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=87.93%, topk_entropy_acc=87.93%, window_entropy_acc=88.79%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 116/500 [10:18:48<32:31:29, 304.92s/it, attention_weighted_confidence_acc=88.79%, cer_entropy_weighted_mean_all_acc=88.79%, cer_prob_product_log_last_acc=88.79%, self_consistency_acc=89.66%, p_true_acc=89.66%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=87.93%, topk_entropy_acc=87.93%, window_entropy_acc=88.79%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 7.662525282959207
    Answer: 623
    Ground truth:  623
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 7.662525282959207
    Answer: 623
    Ground truth:  623
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 15.999797344207764
    Answer: 623
    Ground truth:  623
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 1.0
    Answer: 623
    Ground truth:  623
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 13.671875
    Answer: 623
    Ground truth:  623
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 1.9254483580589294
    Answer: 623
    Ground truth:  623
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 1.5633766651153564
    Answer: 623
    Ground truth:  623
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 1.5599271059036255
    Answer: 623
    Ground truth:  623
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Grace's weight is 125 pounds.

2....
    Score: 4.013630032539368
    Answer: 623
    Ground truth:  623
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 89.74358974358975
Method name: p_true, running accuracy: 89.74358974358975
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.03418803418803
Method name: topk_entropy, running accuracy: 88.03418803418803
Method name: window_entropy, running accuracy: 88.88888888888889

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 116/500 [10:21:42<32:31:29, 304.92s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=89.74%, p_true_acc=89.74%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.03%, topk_entropy_acc=88.03%, window_entropy_acc=88.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 117/500 [10:21:42<28:15:34, 265.62s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=89.74%, p_true_acc=89.74%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.03%, topk_entropy_acc=88.03%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 8.420064100425
    Answer: 50
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 8.420064100425
    Answer: 50
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 15.999809861183167
    Answer: 50
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 1.0
    Answer: 50
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 14.50390625
    Answer: 50
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 3.3437202125787735
    Answer: 50
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 2.293646529316902
    Answer: 50
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 2.2895642146468163
    Answer: 50
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: To find the total distance Rosie can run, we need to calculate her distance in t...
    Score: 4.091319918632507
    Answer: 50
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 88.98305084745762
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.98305084745762
Method name: cer_prob_product_log_last, running accuracy: 88.98305084745762
Method name: self_consistency, running accuracy: 89.83050847457628
Method name: p_true, running accuracy: 89.83050847457628
Method name: normilized_likelihood, running accuracy: 88.98305084745762
Method name: normilized_entropy, running accuracy: 88.13559322033898
Method name: topk_entropy, running accuracy: 88.13559322033898
Method name: window_entropy, running accuracy: 88.98305084745762

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 117/500 [10:26:46<28:15:34, 265.62s/it, attention_weighted_confidence_acc=88.98%, cer_entropy_weighted_mean_all_acc=88.98%, cer_prob_product_log_last_acc=88.98%, self_consistency_acc=89.83%, p_true_acc=89.83%, normilized_likelihood_acc=88.98%, normilized_entropy_acc=88.14%, topk_entropy_acc=88.14%, window_entropy_acc=88.98%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▎       | 118/500 [10:26:46<29:26:03, 277.39s/it, attention_weighted_confidence_acc=88.98%, cer_entropy_weighted_mean_all_acc=88.98%, cer_prob_product_log_last_acc=88.98%, self_consistency_acc=89.83%, p_true_acc=89.83%, normilized_likelihood_acc=88.98%, normilized_entropy_acc=88.14%, topk_entropy_acc=88.14%, window_entropy_acc=88.98%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 7.649722619441613
    Answer: 27000
    Ground truth:  27000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 7.649722619441613
    Answer: 27000
    Ground truth:  27000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 1.4995813378341794
    Answer: 27000
    Ground truth:  27000
Method 4: self_consistency
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 0.9375
    Answer: 27000
    Ground truth:  27000
Method 5: p_true
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 13.12109375
    Answer: 27000
    Ground truth:  27000
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 4.186466336250305
    Answer: 27000
    Ground truth:  27000
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 4.32765719294548
    Answer: 27000
    Ground truth:  27000
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 3.980872392654419
    Answer: 27000
    Ground truth:  27000
Method 9: window_entropy
  Batch 1:
    Text: To calculate the total amount Jean paid for the makeup, we'll break down the ste...
    Score: 13.491920113563538
    Answer: 27000
    Ground truth:  27000
Method name: attention_weighted_confidence, running accuracy: 89.07563025210085
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.07563025210085
Method name: cer_prob_product_log_last, running accuracy: 89.07563025210085
Method name: self_consistency, running accuracy: 89.91596638655463
Method name: p_true, running accuracy: 89.91596638655463
Method name: normilized_likelihood, running accuracy: 89.07563025210085
Method name: normilized_entropy, running accuracy: 88.23529411764706
Method name: topk_entropy, running accuracy: 88.23529411764706
Method name: window_entropy, running accuracy: 89.07563025210085

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▎       | 118/500 [10:31:47<29:26:03, 277.39s/it, attention_weighted_confidence_acc=89.08%, cer_entropy_weighted_mean_all_acc=89.08%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=89.92%, p_true_acc=89.92%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.08%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 119/500 [10:31:47<30:05:58, 284.41s/it, attention_weighted_confidence_acc=89.08%, cer_entropy_weighted_mean_all_acc=89.08%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=89.92%, p_true_acc=89.92%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.08%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 4.210658860644209
    Answer: 10
    Ground truth:  18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 4.210658860644209
    Answer: 10
    Ground truth:  18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 8.974089562892914
    Answer: 10
    Ground truth:  18
Method 4: self_consistency
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 0.5625
    Answer: 10
    Ground truth:  18
Method 5: p_true
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 5.98583984375
    Answer: 10
    Ground truth:  18
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 6.957895487546921
    Answer: 10
    Ground truth:  18
Method 7: normilized_entropy
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 5.67387792468071
    Answer: 10
    Ground truth:  18
Method 8: topk_entropy
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 4.853003889322281
    Answer: 10
    Ground truth:  18
Method 9: window_entropy
  Batch 1:
    Text: Let's solve the problem step by step.

1.  Sara scored 8 points.
2.  Erin now ha...
    Score: 11.5335174202919
    Answer: 10
    Ground truth:  18
Method name: attention_weighted_confidence, running accuracy: 88.33333333333333
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.33333333333333
Method name: cer_prob_product_log_last, running accuracy: 88.33333333333333
Method name: self_consistency, running accuracy: 89.16666666666667
Method name: p_true, running accuracy: 89.16666666666667
Method name: normilized_likelihood, running accuracy: 88.33333333333333
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 88.33333333333333

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 119/500 [10:35:31<30:05:58, 284.41s/it, attention_weighted_confidence_acc=88.33%, cer_entropy_weighted_mean_all_acc=88.33%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=89.17%, p_true_acc=89.17%, normilized_likelihood_acc=88.33%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.33%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 120/500 [10:35:31<28:05:54, 266.20s/it, attention_weighted_confidence_acc=88.33%, cer_entropy_weighted_mean_all_acc=88.33%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=89.17%, p_true_acc=89.17%, normilized_likelihood_acc=88.33%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 4.2610595371522795
    Answer: 114200
    Ground truth:  114.200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 4.2610595371522795
    Answer: 114200
    Ground truth:  114.200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 1.4899417512382787
    Answer: 114200
    Ground truth:  114.200
Method 4: self_consistency
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 0.5625
    Answer: 114200
    Ground truth:  114.200
Method 5: p_true
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 6.29296875
    Answer: 114200
    Ground truth:  114.200
Method 6: normilized_likelihood
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 4.826440453529358
    Answer: 114200
    Ground truth:  114.200
Method 7: normilized_entropy
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 4.945585280656815
    Answer: 114200
    Ground truth:  114.200
Method 8: topk_entropy
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 4.413406848907471
    Answer: 114200
    Ground truth:  114.200
Method 9: window_entropy
  Batch 1:
    Text: To find John's total earnings for the year after winning the award, we need to c...
    Score: 10.201849579811096
    Answer: 114200
    Ground truth:  114.200
Method name: attention_weighted_confidence, running accuracy: 87.60330578512396
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.60330578512396
Method name: cer_prob_product_log_last, running accuracy: 87.60330578512396
Method name: self_consistency, running accuracy: 88.42975206611571
Method name: p_true, running accuracy: 88.42975206611571
Method name: normilized_likelihood, running accuracy: 87.60330578512396
Method name: normilized_entropy, running accuracy: 86.77685950413223
Method name: topk_entropy, running accuracy: 86.77685950413223
Method name: window_entropy, running accuracy: 87.60330578512396

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 120/500 [10:40:57<28:05:54, 266.20s/it, attention_weighted_confidence_acc=87.60%, cer_entropy_weighted_mean_all_acc=87.60%, cer_prob_product_log_last_acc=87.60%, self_consistency_acc=88.43%, p_true_acc=88.43%, normilized_likelihood_acc=87.60%, normilized_entropy_acc=86.78%, topk_entropy_acc=86.78%, window_entropy_acc=87.60%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 121/500 [10:40:57<29:55:04, 284.18s/it, attention_weighted_confidence_acc=87.60%, cer_entropy_weighted_mean_all_acc=87.60%, cer_prob_product_log_last_acc=87.60%, self_consistency_acc=88.43%, p_true_acc=88.43%, normilized_likelihood_acc=87.60%, normilized_entropy_acc=86.78%, topk_entropy_acc=86.78%, window_entropy_acc=87.60%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 7.205161112144002
    Answer: 85000
    Ground truth:  85000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 7.205161112144002
    Answer: 85000
    Ground truth:  85000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 3.787881500500326
    Answer: 85000
    Ground truth:  85000
Method 4: self_consistency
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 0.875
    Answer: 85000
    Ground truth:  85000
Method 5: p_true
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 12.140625
    Answer: 85000
    Ground truth:  85000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 3.722595989704132
    Answer: 85000
    Ground truth:  85000
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 3.5000635236501694
    Answer: 85000
    Ground truth:  85000
Method 8: topk_entropy
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 3.151658907532692
    Answer: 85000
    Ground truth:  85000
Method 9: window_entropy
  Batch 1:
    Text: To find the total calories of the pastries, we need to first find the calories o...
    Score: 16.22443437576294
    Answer: 85000
    Ground truth:  85000
Method name: attention_weighted_confidence, running accuracy: 87.70491803278688
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.70491803278688
Method name: cer_prob_product_log_last, running accuracy: 87.70491803278688
Method name: self_consistency, running accuracy: 88.52459016393442
Method name: p_true, running accuracy: 88.52459016393442
Method name: normilized_likelihood, running accuracy: 87.70491803278688
Method name: normilized_entropy, running accuracy: 86.88524590163934
Method name: topk_entropy, running accuracy: 86.88524590163934
Method name: window_entropy, running accuracy: 87.70491803278688

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 121/500 [10:46:44<29:55:04, 284.18s/it, attention_weighted_confidence_acc=87.70%, cer_entropy_weighted_mean_all_acc=87.70%, cer_prob_product_log_last_acc=87.70%, self_consistency_acc=88.52%, p_true_acc=88.52%, normilized_likelihood_acc=87.70%, normilized_entropy_acc=86.89%, topk_entropy_acc=86.89%, window_entropy_acc=87.70%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 122/500 [10:46:44<31:49:14, 303.05s/it, attention_weighted_confidence_acc=87.70%, cer_entropy_weighted_mean_all_acc=87.70%, cer_prob_product_log_last_acc=87.70%, self_consistency_acc=88.52%, p_true_acc=88.52%, normilized_likelihood_acc=87.70%, normilized_entropy_acc=86.89%, topk_entropy_acc=86.89%, window_entropy_acc=87.70%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 7.995296597639033
    Answer: 1430
    Ground truth:  1430
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 7.995296597639033
    Answer: 1430
    Ground truth:  1430
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 5.98092345270565
    Answer: 1430
    Ground truth:  1430
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 1.0
    Answer: 1430
    Ground truth:  1430
Method 5: p_true
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 12.61328125
    Answer: 1430
    Ground truth:  1430
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 1.9106453210115433
    Answer: 1430
    Ground truth:  1430
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 0.7750025391578674
    Answer: 1430
    Ground truth:  1430
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 0.7689013183116913
    Answer: 1430
    Ground truth:  1430
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount Janet paid, we need to follow the steps:

1. She pays $...
    Score: 3.4568673968315125
    Answer: 1430
    Ground truth:  1430
Method name: attention_weighted_confidence, running accuracy: 87.8048780487805
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.8048780487805
Method name: cer_prob_product_log_last, running accuracy: 87.8048780487805
Method name: self_consistency, running accuracy: 88.6178861788618
Method name: p_true, running accuracy: 88.6178861788618
Method name: normilized_likelihood, running accuracy: 87.8048780487805
Method name: normilized_entropy, running accuracy: 86.99186991869918
Method name: topk_entropy, running accuracy: 86.99186991869918
Method name: window_entropy, running accuracy: 87.8048780487805

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 122/500 [10:50:33<31:49:14, 303.05s/it, attention_weighted_confidence_acc=87.80%, cer_entropy_weighted_mean_all_acc=87.80%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=88.62%, p_true_acc=88.62%, normilized_likelihood_acc=87.80%, normilized_entropy_acc=86.99%, topk_entropy_acc=86.99%, window_entropy_acc=87.80%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▍       | 123/500 [10:50:33<29:23:46, 280.71s/it, attention_weighted_confidence_acc=87.80%, cer_entropy_weighted_mean_all_acc=87.80%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=88.62%, p_true_acc=88.62%, normilized_likelihood_acc=87.80%, normilized_entropy_acc=86.99%, topk_entropy_acc=86.99%, window_entropy_acc=87.80%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 3.118596675883256
    Answer: 40
    Ground truth:  40
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 3.118596675883256
    Answer: 40
    Ground truth:  40
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 5.997653901576996
    Answer: 40
    Ground truth:  40
Method 4: self_consistency
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 0.375
    Answer: 40
    Ground truth:  40
Method 5: p_true
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 5.38671875
    Answer: 40
    Ground truth:  40
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 2.733751580119133
    Answer: 40
    Ground truth:  40
Method 7: normilized_entropy
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 3.130232125520706
    Answer: 40
    Ground truth:  40
Method 8: topk_entropy
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 2.8133576810359955
    Answer: 40
    Ground truth:  40
Method 9: window_entropy
  Batch 1:
    Text: To find the average number of birds Mack saw per day, we need to calculate the t...
    Score: 7.692187666893005
    Answer: 40
    Ground truth:  40
Method name: attention_weighted_confidence, running accuracy: 87.90322580645162
Method name: cer_entropy_weighted_mean_all, running accuracy: 87.90322580645162
Method name: cer_prob_product_log_last, running accuracy: 87.90322580645162
Method name: self_consistency, running accuracy: 88.70967741935483
Method name: p_true, running accuracy: 88.70967741935483
Method name: normilized_likelihood, running accuracy: 87.90322580645162
Method name: normilized_entropy, running accuracy: 87.09677419354838
Method name: topk_entropy, running accuracy: 87.09677419354838
Method name: window_entropy, running accuracy: 87.90322580645162

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▍       | 123/500 [10:56:25<29:23:46, 280.71s/it, attention_weighted_confidence_acc=87.90%, cer_entropy_weighted_mean_all_acc=87.90%, cer_prob_product_log_last_acc=87.90%, self_consistency_acc=88.71%, p_true_acc=88.71%, normilized_likelihood_acc=87.90%, normilized_entropy_acc=87.10%, topk_entropy_acc=87.10%, window_entropy_acc=87.90%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▍       | 124/500 [10:56:25<31:33:57, 302.23s/it, attention_weighted_confidence_acc=87.90%, cer_entropy_weighted_mean_all_acc=87.90%, cer_prob_product_log_last_acc=87.90%, self_consistency_acc=88.71%, p_true_acc=88.71%, normilized_likelihood_acc=87.90%, normilized_entropy_acc=87.10%, topk_entropy_acc=87.10%, window_entropy_acc=87.90%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 8.152986588750103
    Answer: -3
    Ground truth:  -3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 8.152986588750103
    Answer: -3
    Ground truth:  -3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 7.991740300726358
    Answer: -3
    Ground truth:  -3
Method 4: self_consistency
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 1.0
    Answer: -3
    Ground truth:  -3
Method 5: p_true
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 14.47265625
    Answer: -3
    Ground truth:  -3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 1.9059252887964249
    Answer: -3
    Ground truth:  -3
Method 7: normilized_entropy
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 1.6653601229190826
    Answer: -3
    Ground truth:  -3
Method 8: topk_entropy
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 1.6511621475219727
    Answer: -3
    Ground truth:  -3
Method 9: window_entropy
  Batch 1:
    Text: To find the temperature in the morning, we first need to determine the temperatu...
    Score: 5.874322235584259
    Answer: -3
    Ground truth:  -3
Method name: attention_weighted_confidence, running accuracy: 88.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.0
Method name: cer_prob_product_log_last, running accuracy: 88.0
Method name: self_consistency, running accuracy: 88.8
Method name: p_true, running accuracy: 88.8
Method name: normilized_likelihood, running accuracy: 88.0
Method name: normilized_entropy, running accuracy: 87.2
Method name: topk_entropy, running accuracy: 87.2
Method name: window_entropy, running accuracy: 88.0

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▍       | 124/500 [10:59:16<31:33:57, 302.23s/it, attention_weighted_confidence_acc=88.00%, cer_entropy_weighted_mean_all_acc=88.00%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=88.80%, p_true_acc=88.80%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.20%, topk_entropy_acc=87.20%, window_entropy_acc=88.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 125/500 [10:59:16<27:22:22, 262.78s/it, attention_weighted_confidence_acc=88.00%, cer_entropy_weighted_mean_all_acc=88.00%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=88.80%, p_true_acc=88.80%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.20%, topk_entropy_acc=87.20%, window_entropy_acc=88.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 7.176075625966068
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 7.176075625966068
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 13.320859253406525
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 0.875
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 13.46484375
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 5.6597758531570435
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 5.672360986471176
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 5.019469857215881
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: Let's denote the number of large stuffed animals sold as 'L' and the number of s...
    Score: 19.31827747821808
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 88.09523809523809
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.09523809523809
Method name: cer_prob_product_log_last, running accuracy: 88.09523809523809
Method name: self_consistency, running accuracy: 88.88888888888889
Method name: p_true, running accuracy: 88.88888888888889
Method name: normilized_likelihood, running accuracy: 88.09523809523809
Method name: normilized_entropy, running accuracy: 87.3015873015873
Method name: topk_entropy, running accuracy: 87.3015873015873
Method name: window_entropy, running accuracy: 88.09523809523809

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 125/500 [11:06:20<27:22:22, 262.78s/it, attention_weighted_confidence_acc=88.10%, cer_entropy_weighted_mean_all_acc=88.10%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=88.89%, p_true_acc=88.89%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=87.30%, topk_entropy_acc=87.30%, window_entropy_acc=88.10%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 126/500 [11:06:20<32:19:55, 311.22s/it, attention_weighted_confidence_acc=88.10%, cer_entropy_weighted_mean_all_acc=88.10%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=88.89%, p_true_acc=88.89%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=87.30%, topk_entropy_acc=87.30%, window_entropy_acc=88.10%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 6.130005335267738
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 6.130005335267738
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 11.998196244239807
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 0.75
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 11.55078125
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 2.6520522385835648
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 2.5833933502435684
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 2.478887066245079
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to find the speed of the car in the slow lane and...
    Score: 9.217843383550644
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 88.18897637795276
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.18897637795276
Method name: cer_prob_product_log_last, running accuracy: 88.18897637795276
Method name: self_consistency, running accuracy: 88.9763779527559
Method name: p_true, running accuracy: 88.9763779527559
Method name: normilized_likelihood, running accuracy: 88.18897637795276
Method name: normilized_entropy, running accuracy: 87.4015748031496
Method name: topk_entropy, running accuracy: 87.4015748031496
Method name: window_entropy, running accuracy: 88.18897637795276

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 126/500 [11:11:35<32:19:55, 311.22s/it, attention_weighted_confidence_acc=88.19%, cer_entropy_weighted_mean_all_acc=88.19%, cer_prob_product_log_last_acc=88.19%, self_consistency_acc=88.98%, p_true_acc=88.98%, normilized_likelihood_acc=88.19%, normilized_entropy_acc=87.40%, topk_entropy_acc=87.40%, window_entropy_acc=88.19%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 127/500 [11:11:35<32:22:05, 312.40s/it, attention_weighted_confidence_acc=88.19%, cer_entropy_weighted_mean_all_acc=88.19%, cer_prob_product_log_last_acc=88.19%, self_consistency_acc=88.98%, p_true_acc=88.98%, normilized_likelihood_acc=88.19%, normilized_entropy_acc=87.40%, topk_entropy_acc=87.40%, window_entropy_acc=88.19%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 1.2677239009754322
    Answer: 68
    Ground truth:  68
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 1.2677239009754322
    Answer: 68
    Ground truth:  68
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 2.805980384349823
    Answer: 68
    Ground truth:  68
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 0.1875
    Answer: 68
    Ground truth:  68
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 2.8671875
    Answer: 68
    Ground truth:  68
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 3.598922848701477
    Answer: 68
    Ground truth:  68
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 3.3992887139320374
    Answer: 68
    Ground truth:  68
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 2.765367031097412
    Answer: 68
    Ground truth:  68
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to determine the total volume and temperature of ...
    Score: 2.9915931224823
    Answer: 68
    Ground truth:  68
Method name: attention_weighted_confidence, running accuracy: 88.28125
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.28125
Method name: cer_prob_product_log_last, running accuracy: 88.28125
Method name: self_consistency, running accuracy: 89.0625
Method name: p_true, running accuracy: 89.0625
Method name: normilized_likelihood, running accuracy: 88.28125
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 88.28125

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 127/500 [11:22:44<32:22:05, 312.40s/it, attention_weighted_confidence_acc=88.28%, cer_entropy_weighted_mean_all_acc=88.28%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=89.06%, p_true_acc=89.06%, normilized_likelihood_acc=88.28%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.28%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 128/500 [11:22:44<43:19:28, 419.27s/it, attention_weighted_confidence_acc=88.28%, cer_entropy_weighted_mean_all_acc=88.28%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=89.06%, p_true_acc=89.06%, normilized_likelihood_acc=88.28%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.28%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 6.730678551168125
    Answer: 17
    Ground truth:  17
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 6.730678551168125
    Answer: 17
    Ground truth:  17
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 12.993715286254883
    Answer: 17
    Ground truth:  17
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 0.8125
    Answer: 17
    Ground truth:  17
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 11.0
    Answer: 17
    Ground truth:  17
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 4.574245780706406
    Answer: 17
    Ground truth:  17
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 3.494083344936371
    Answer: 17
    Ground truth:  17
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 3.3436024636030197
    Answer: 17
    Ground truth:  17
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down step by step.

1. Total number o...
    Score: 10.523968279361725
    Answer: 17
    Ground truth:  17
Method name: attention_weighted_confidence, running accuracy: 88.37209302325581
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.37209302325581
Method name: cer_prob_product_log_last, running accuracy: 88.37209302325581
Method name: self_consistency, running accuracy: 89.14728682170544
Method name: p_true, running accuracy: 89.14728682170544
Method name: normilized_likelihood, running accuracy: 88.37209302325581
Method name: normilized_entropy, running accuracy: 87.59689922480621
Method name: topk_entropy, running accuracy: 87.59689922480621
Method name: window_entropy, running accuracy: 88.37209302325581

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 128/500 [11:26:46<43:19:28, 419.27s/it, attention_weighted_confidence_acc=88.37%, cer_entropy_weighted_mean_all_acc=88.37%, cer_prob_product_log_last_acc=88.37%, self_consistency_acc=89.15%, p_true_acc=89.15%, normilized_likelihood_acc=88.37%, normilized_entropy_acc=87.60%, topk_entropy_acc=87.60%, window_entropy_acc=88.37%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 129/500 [11:26:46<37:43:14, 366.02s/it, attention_weighted_confidence_acc=88.37%, cer_entropy_weighted_mean_all_acc=88.37%, cer_prob_product_log_last_acc=88.37%, self_consistency_acc=89.15%, p_true_acc=89.15%, normilized_likelihood_acc=88.37%, normilized_entropy_acc=87.60%, topk_entropy_acc=87.60%, window_entropy_acc=88.37%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 8.347883544944699
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 8.347883544944699
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 15.461306869983673
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 13.625
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 3.243532881140709
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 2.499289035797119
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 2.4794642329216003
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the length of one of Bobby's shoes, we need to break down the informatio...
    Score: 4.966674208641052
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 88.46153846153845
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.46153846153845
Method name: cer_prob_product_log_last, running accuracy: 88.46153846153845
Method name: self_consistency, running accuracy: 89.23076923076924
Method name: p_true, running accuracy: 89.23076923076924
Method name: normilized_likelihood, running accuracy: 88.46153846153845
Method name: normilized_entropy, running accuracy: 87.6923076923077
Method name: topk_entropy, running accuracy: 87.6923076923077
Method name: window_entropy, running accuracy: 88.46153846153845

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 129/500 [11:32:23<37:43:14, 366.02s/it, attention_weighted_confidence_acc=88.46%, cer_entropy_weighted_mean_all_acc=88.46%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=89.23%, p_true_acc=89.23%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=87.69%, topk_entropy_acc=87.69%, window_entropy_acc=88.46%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 130/500 [11:32:23<36:43:28, 357.32s/it, attention_weighted_confidence_acc=88.46%, cer_entropy_weighted_mean_all_acc=88.46%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=89.23%, p_true_acc=89.23%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=87.69%, topk_entropy_acc=87.69%, window_entropy_acc=88.46%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 7.866855969876777
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 7.866855969876777
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 15.997392117977142
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 14.83203125
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 2.194130226969719
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 2.052566960453987
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 2.0304724872112274
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 11.101626336574554
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 88.54961832061069
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.54961832061069
Method name: cer_prob_product_log_last, running accuracy: 88.54961832061069
Method name: self_consistency, running accuracy: 89.31297709923665
Method name: p_true, running accuracy: 89.31297709923665
Method name: normilized_likelihood, running accuracy: 88.54961832061069
Method name: normilized_entropy, running accuracy: 87.78625954198473
Method name: topk_entropy, running accuracy: 87.78625954198473
Method name: window_entropy, running accuracy: 88.54961832061069

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 130/500 [11:35:49<36:43:28, 357.32s/it, attention_weighted_confidence_acc=88.55%, cer_entropy_weighted_mean_all_acc=88.55%, cer_prob_product_log_last_acc=88.55%, self_consistency_acc=89.31%, p_true_acc=89.31%, normilized_likelihood_acc=88.55%, normilized_entropy_acc=87.79%, topk_entropy_acc=87.79%, window_entropy_acc=88.55%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 131/500 [11:35:49<31:58:33, 311.96s/it, attention_weighted_confidence_acc=88.55%, cer_entropy_weighted_mean_all_acc=88.55%, cer_prob_product_log_last_acc=88.55%, self_consistency_acc=89.31%, p_true_acc=89.31%, normilized_likelihood_acc=88.55%, normilized_entropy_acc=87.79%, topk_entropy_acc=87.79%, window_entropy_acc=88.55%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 4.718096242469989
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 4.718096242469989
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 9.963110268115997
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 0.625
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 8.4765625
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 8.698486238718033
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 8.690866559743881
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 7.6909752786159515
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the speed at which John runs, we first need to find out how many hours h...
    Score: 13.577990770339966
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 88.63636363636364
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.63636363636364
Method name: cer_prob_product_log_last, running accuracy: 88.63636363636364
Method name: self_consistency, running accuracy: 89.39393939393939
Method name: p_true, running accuracy: 89.39393939393939
Method name: normilized_likelihood, running accuracy: 88.63636363636364
Method name: normilized_entropy, running accuracy: 87.87878787878788
Method name: topk_entropy, running accuracy: 87.87878787878788
Method name: window_entropy, running accuracy: 88.63636363636364

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 131/500 [11:41:46<31:58:33, 311.96s/it, attention_weighted_confidence_acc=88.64%, cer_entropy_weighted_mean_all_acc=88.64%, cer_prob_product_log_last_acc=88.64%, self_consistency_acc=89.39%, p_true_acc=89.39%, normilized_likelihood_acc=88.64%, normilized_entropy_acc=87.88%, topk_entropy_acc=87.88%, window_entropy_acc=88.64%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▋       | 132/500 [11:41:46<33:17:00, 325.60s/it, attention_weighted_confidence_acc=88.64%, cer_entropy_weighted_mean_all_acc=88.64%, cer_prob_product_log_last_acc=88.64%, self_consistency_acc=89.39%, p_true_acc=89.39%, normilized_likelihood_acc=88.64%, normilized_entropy_acc=87.88%, topk_entropy_acc=87.88%, window_entropy_acc=88.64%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 7.543651544602077
    Answer: 51
    Ground truth:  51
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 7.543651544602077
    Answer: 51
    Ground truth:  51
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 13.925647497177124
    Answer: 51
    Ground truth:  51
Method 4: self_consistency
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 0.875
    Answer: 51
    Ground truth:  51
Method 5: p_true
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 11.00390625
    Answer: 51
    Ground truth:  51
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 15.324750363826752
    Answer: 51
    Ground truth:  51
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 15.39568743109703
    Answer: 51
    Ground truth:  51
Method 8: topk_entropy
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 13.374418705701828
    Answer: 51
    Ground truth:  51
Method 9: window_entropy
  Batch 1:
    Text: To find the total age of the four birds, let's break down the information given:...
    Score: 24.824648827314377
    Answer: 51
    Ground truth:  51
Method name: attention_weighted_confidence, running accuracy: 88.7218045112782
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.7218045112782
Method name: cer_prob_product_log_last, running accuracy: 88.7218045112782
Method name: self_consistency, running accuracy: 89.47368421052632
Method name: p_true, running accuracy: 89.47368421052632
Method name: normilized_likelihood, running accuracy: 88.7218045112782
Method name: normilized_entropy, running accuracy: 87.96992481203007
Method name: topk_entropy, running accuracy: 87.96992481203007
Method name: window_entropy, running accuracy: 88.7218045112782

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▋       | 132/500 [11:47:26<33:17:00, 325.60s/it, attention_weighted_confidence_acc=88.72%, cer_entropy_weighted_mean_all_acc=88.72%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=89.47%, p_true_acc=89.47%, normilized_likelihood_acc=88.72%, normilized_entropy_acc=87.97%, topk_entropy_acc=87.97%, window_entropy_acc=88.72%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 133/500 [11:47:26<33:37:51, 329.90s/it, attention_weighted_confidence_acc=88.72%, cer_entropy_weighted_mean_all_acc=88.72%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=89.47%, p_true_acc=89.47%, normilized_likelihood_acc=88.72%, normilized_entropy_acc=87.97%, topk_entropy_acc=87.97%, window_entropy_acc=88.72%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 8.22574661204915
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 8.22574661204915
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 15.999461054801941
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 1.0
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 15.49609375
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 3.2337990403175354
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 2.334787741303444
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 2.2812011539936066
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

Step 1: Sarah does 400 ...
    Score: 11.364028304815292
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 88.80597014925374
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.80597014925374
Method name: cer_prob_product_log_last, running accuracy: 88.80597014925374
Method name: self_consistency, running accuracy: 89.55223880597015
Method name: p_true, running accuracy: 89.55223880597015
Method name: normilized_likelihood, running accuracy: 88.80597014925374
Method name: normilized_entropy, running accuracy: 88.05970149253731
Method name: topk_entropy, running accuracy: 88.05970149253731
Method name: window_entropy, running accuracy: 88.80597014925374

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 133/500 [11:51:38<33:37:51, 329.90s/it, attention_weighted_confidence_acc=88.81%, cer_entropy_weighted_mean_all_acc=88.81%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=89.55%, p_true_acc=89.55%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.06%, topk_entropy_acc=88.06%, window_entropy_acc=88.81%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 134/500 [11:51:38<31:08:58, 306.39s/it, attention_weighted_confidence_acc=88.81%, cer_entropy_weighted_mean_all_acc=88.81%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=89.55%, p_true_acc=89.55%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.06%, topk_entropy_acc=88.06%, window_entropy_acc=88.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 7.925702061240106
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 7.925702061240106
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 15.999980330467224
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 1.0
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 14.8515625
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 1.9963460266590118
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 1.7944165170192719
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 1.7877892404794693
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: To find the cost of May's haircut and color, we need to calculate the cost of th...
    Score: 4.108518660068512
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 89.62962962962962
Method name: p_true, running accuracy: 89.62962962962962
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.14814814814815
Method name: topk_entropy, running accuracy: 88.14814814814815
Method name: window_entropy, running accuracy: 88.88888888888889

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 134/500 [11:55:06<31:08:58, 306.39s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=89.63%, p_true_acc=89.63%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.15%, topk_entropy_acc=88.15%, window_entropy_acc=88.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 135/500 [11:55:06<28:04:05, 276.84s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=89.63%, p_true_acc=89.63%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.15%, topk_entropy_acc=88.15%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 8.230756288395227
    Answer: 56
    Ground truth:  56
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 8.230756288395227
    Answer: 56
    Ground truth:  56
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 15.998923003673553
    Answer: 56
    Ground truth:  56
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 1.0
    Answer: 56
    Ground truth:  56
Method 5: p_true
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 14.51953125
    Answer: 56
    Ground truth:  56
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 3.8631129264831543
    Answer: 56
    Ground truth:  56
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 2.8889365792274475
    Answer: 56
    Ground truth:  56
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 2.6969687938690186
    Answer: 56
    Ground truth:  56
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of students, we first need to determine the number of s...
    Score: 10.025226533412933
    Answer: 56
    Ground truth:  56
Method name: attention_weighted_confidence, running accuracy: 88.97058823529412
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.97058823529412
Method name: cer_prob_product_log_last, running accuracy: 88.97058823529412
Method name: self_consistency, running accuracy: 89.70588235294117
Method name: p_true, running accuracy: 89.70588235294117
Method name: normilized_likelihood, running accuracy: 88.97058823529412
Method name: normilized_entropy, running accuracy: 88.23529411764706
Method name: topk_entropy, running accuracy: 88.23529411764706
Method name: window_entropy, running accuracy: 88.97058823529412

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 135/500 [11:58:44<28:04:05, 276.84s/it, attention_weighted_confidence_acc=88.97%, cer_entropy_weighted_mean_all_acc=88.97%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=89.71%, p_true_acc=89.71%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=88.97%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 136/500 [11:58:44<26:12:20, 259.18s/it, attention_weighted_confidence_acc=88.97%, cer_entropy_weighted_mean_all_acc=88.97%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=89.71%, p_true_acc=89.71%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=88.97%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 6.129685746447033
    Answer: 112
    Ground truth:  112
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 6.129685746447033
    Answer: 112
    Ground truth:  112
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 11.999164402484894
    Answer: 112
    Ground truth:  112
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 0.75
    Answer: 112
    Ground truth:  112
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 10.9765625
    Answer: 112
    Ground truth:  112
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 2.369587689638138
    Answer: 112
    Ground truth:  112
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 1.5994991660118103
    Answer: 112
    Ground truth:  112
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 1.5870643705129623
    Answer: 112
    Ground truth:  112
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 2.5150833129882812
    Answer: 112
    Ground truth:  112
Method name: attention_weighted_confidence, running accuracy: 89.05109489051095
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.05109489051095
Method name: cer_prob_product_log_last, running accuracy: 89.05109489051095
Method name: self_consistency, running accuracy: 89.78102189781022
Method name: p_true, running accuracy: 89.78102189781022
Method name: normilized_likelihood, running accuracy: 89.05109489051095
Method name: normilized_entropy, running accuracy: 88.32116788321169
Method name: topk_entropy, running accuracy: 88.32116788321169
Method name: window_entropy, running accuracy: 89.05109489051095

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 136/500 [12:03:10<26:12:20, 259.18s/it, attention_weighted_confidence_acc=89.05%, cer_entropy_weighted_mean_all_acc=89.05%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=89.78%, p_true_acc=89.78%, normilized_likelihood_acc=89.05%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.05%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 137/500 [12:03:10<26:20:30, 261.24s/it, attention_weighted_confidence_acc=89.05%, cer_entropy_weighted_mean_all_acc=89.05%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=89.78%, p_true_acc=89.78%, normilized_likelihood_acc=89.05%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.05%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 6.453973198803817
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 6.453973198803817
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 12.534410417079926
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 0.8125
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 10.39501953125
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 15.575748294591904
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 16.03267851471901
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 13.917333662509918
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, there ar...
    Score: 20.828919112682343
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 89.13043478260869
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.13043478260869
Method name: cer_prob_product_log_last, running accuracy: 89.13043478260869
Method name: self_consistency, running accuracy: 89.85507246376811
Method name: p_true, running accuracy: 89.85507246376811
Method name: normilized_likelihood, running accuracy: 89.13043478260869
Method name: normilized_entropy, running accuracy: 88.40579710144928
Method name: topk_entropy, running accuracy: 88.40579710144928
Method name: window_entropy, running accuracy: 89.13043478260869

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 137/500 [12:10:14<26:20:30, 261.24s/it, attention_weighted_confidence_acc=89.13%, cer_entropy_weighted_mean_all_acc=89.13%, cer_prob_product_log_last_acc=89.13%, self_consistency_acc=89.86%, p_true_acc=89.86%, normilized_likelihood_acc=89.13%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.13%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 138/500 [12:10:14<31:12:00, 310.28s/it, attention_weighted_confidence_acc=89.13%, cer_entropy_weighted_mean_all_acc=89.13%, cer_prob_product_log_last_acc=89.13%, self_consistency_acc=89.86%, p_true_acc=89.86%, normilized_likelihood_acc=89.13%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.13%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 8.29288421253432
    Answer: 105
    Ground truth:  105
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 8.29288421253432
    Answer: 105
    Ground truth:  105
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 15.998594641685486
    Answer: 105
    Ground truth:  105
Method 4: self_consistency
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 1.0
    Answer: 105
    Ground truth:  105
Method 5: p_true
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 14.6171875
    Answer: 105
    Ground truth:  105
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 5.253646269440651
    Answer: 105
    Ground truth:  105
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 2.2528915256261826
    Answer: 105
    Ground truth:  105
Method 8: topk_entropy
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 2.2221319526433945
    Answer: 105
    Ground truth:  105
Method 9: window_entropy
  Batch 1:
    Text: To find the total weight of Elijah and Kory's dogs, we need to calculate the wei...
    Score: 3.6394442319869995
    Answer: 105
    Ground truth:  105
Method name: attention_weighted_confidence, running accuracy: 89.20863309352518
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.20863309352518
Method name: cer_prob_product_log_last, running accuracy: 89.20863309352518
Method name: self_consistency, running accuracy: 89.92805755395683
Method name: p_true, running accuracy: 89.92805755395683
Method name: normilized_likelihood, running accuracy: 89.20863309352518
Method name: normilized_entropy, running accuracy: 88.48920863309353
Method name: topk_entropy, running accuracy: 88.48920863309353
Method name: window_entropy, running accuracy: 89.20863309352518

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 138/500 [12:14:52<31:12:00, 310.28s/it, attention_weighted_confidence_acc=89.21%, cer_entropy_weighted_mean_all_acc=89.21%, cer_prob_product_log_last_acc=89.21%, self_consistency_acc=89.93%, p_true_acc=89.93%, normilized_likelihood_acc=89.21%, normilized_entropy_acc=88.49%, topk_entropy_acc=88.49%, window_entropy_acc=89.21%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 139/500 [12:14:52<30:07:50, 300.47s/it, attention_weighted_confidence_acc=89.21%, cer_entropy_weighted_mean_all_acc=89.21%, cer_prob_product_log_last_acc=89.21%, self_consistency_acc=89.93%, p_true_acc=89.93%, normilized_likelihood_acc=89.21%, normilized_entropy_acc=88.49%, topk_entropy_acc=88.49%, window_entropy_acc=89.21%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 2.793140732635457
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 2.793140732635457
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 5.999653935432434
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Erin has a total of...
    Score: 0.375
    Answer: 4
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 4.75390625
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 5.126784682273865
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 5.423243045806885
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 4.596866548061371
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Erin wants to bu...
    Score: 7.706565260887146
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 89.28571428571429
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.28571428571429
Method name: cer_prob_product_log_last, running accuracy: 89.28571428571429
Method name: self_consistency, running accuracy: 89.28571428571429
Method name: p_true, running accuracy: 90.0
Method name: normilized_likelihood, running accuracy: 89.28571428571429
Method name: normilized_entropy, running accuracy: 88.57142857142857
Method name: topk_entropy, running accuracy: 88.57142857142857
Method name: window_entropy, running accuracy: 89.28571428571429

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 139/500 [12:21:58<30:07:50, 300.47s/it, attention_weighted_confidence_acc=89.29%, cer_entropy_weighted_mean_all_acc=89.29%, cer_prob_product_log_last_acc=89.29%, self_consistency_acc=89.29%, p_true_acc=90.00%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=89.29%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 140/500 [12:21:58<33:49:13, 338.20s/it, attention_weighted_confidence_acc=89.29%, cer_entropy_weighted_mean_all_acc=89.29%, cer_prob_product_log_last_acc=89.29%, self_consistency_acc=89.29%, p_true_acc=90.00%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=89.29%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 2.2966598716462894
    Answer: 1
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 2.2966598716462894
    Answer: 1
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 4.2097442746162415
    Answer: 1
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 0.3125
    Answer: 1
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 3.4296875
    Answer: 1
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 4.292397990822792
    Answer: 1
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 4.129935085773468
    Answer: 1
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 3.547422766685486
    Answer: 1
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. 90 single-use contacts will last ...
    Score: 5.959928393363953
    Answer: 1
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 88.65248226950354
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.65248226950354
Method name: cer_prob_product_log_last, running accuracy: 88.65248226950354
Method name: self_consistency, running accuracy: 88.65248226950354
Method name: p_true, running accuracy: 89.36170212765957
Method name: normilized_likelihood, running accuracy: 88.65248226950354
Method name: normilized_entropy, running accuracy: 87.94326241134752
Method name: topk_entropy, running accuracy: 87.94326241134752
Method name: window_entropy, running accuracy: 88.65248226950354

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 140/500 [12:29:33<33:49:13, 338.20s/it, attention_weighted_confidence_acc=88.65%, cer_entropy_weighted_mean_all_acc=88.65%, cer_prob_product_log_last_acc=88.65%, self_consistency_acc=88.65%, p_true_acc=89.36%, normilized_likelihood_acc=88.65%, normilized_entropy_acc=87.94%, topk_entropy_acc=87.94%, window_entropy_acc=88.65%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 141/500 [12:29:33<37:12:11, 373.07s/it, attention_weighted_confidence_acc=88.65%, cer_entropy_weighted_mean_all_acc=88.65%, cer_prob_product_log_last_acc=88.65%, self_consistency_acc=88.65%, p_true_acc=89.36%, normilized_likelihood_acc=88.65%, normilized_entropy_acc=87.94%, topk_entropy_acc=87.94%, window_entropy_acc=88.65%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 7.865734438858951
    Answer: 80
    Ground truth:  80
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 7.865734438858951
    Answer: 80
    Ground truth:  80
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 14.999367713928223
    Answer: 80
    Ground truth:  80
Method 4: self_consistency
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 0.9375
    Answer: 80
    Ground truth:  80
Method 5: p_true
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 14.1171875
    Answer: 80
    Ground truth:  80
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 2.025699704885483
    Answer: 80
    Ground truth:  80
Method 7: normilized_entropy
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 1.5899810045957565
    Answer: 80
    Ground truth:  80
Method 8: topk_entropy
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 1.5706382989883423
    Answer: 80
    Ground truth:  80
Method 9: window_entropy
  Batch 1:
    Text: To find the average guess, we need to calculate each friend's guess and then add...
    Score: 13.5760258436203
    Answer: 80
    Ground truth:  80
Method name: attention_weighted_confidence, running accuracy: 88.73239436619718
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.73239436619718
Method name: cer_prob_product_log_last, running accuracy: 88.73239436619718
Method name: self_consistency, running accuracy: 88.73239436619718
Method name: p_true, running accuracy: 89.43661971830986
Method name: normilized_likelihood, running accuracy: 88.73239436619718
Method name: normilized_entropy, running accuracy: 88.02816901408451
Method name: topk_entropy, running accuracy: 88.02816901408451
Method name: window_entropy, running accuracy: 88.73239436619718

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 141/500 [12:34:19<37:12:11, 373.07s/it, attention_weighted_confidence_acc=88.73%, cer_entropy_weighted_mean_all_acc=88.73%, cer_prob_product_log_last_acc=88.73%, self_consistency_acc=88.73%, p_true_acc=89.44%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.03%, topk_entropy_acc=88.03%, window_entropy_acc=88.73%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 142/500 [12:34:19<34:31:24, 347.16s/it, attention_weighted_confidence_acc=88.73%, cer_entropy_weighted_mean_all_acc=88.73%, cer_prob_product_log_last_acc=88.73%, self_consistency_acc=88.73%, p_true_acc=89.44%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.03%, topk_entropy_acc=88.03%, window_entropy_acc=88.73%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

1. Calculate the numbe...
    Score: 2.594649977054935
    Answer: 224000
    Ground truth:  224000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

1. Calculate the numbe...
    Score: 2.594649977054935
    Answer: 224000
    Ground truth:  224000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount of money the company pays to all the programmers per mo...
    Score: 1.0086696257340009
    Answer: 208000
    Ground truth:  224000
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

1. Calculate the numbe...
    Score: 0.3125
    Answer: 224000
    Ground truth:  224000
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

1. Calculate the numbe...
    Score: 4.6328125
    Answer: 224000
    Ground truth:  224000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount of money the company pays to all the programmers per mo...
    Score: 0.6327902376651764
    Answer: 208000
    Ground truth:  224000
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount of money the company pays to all the programmers per mo...
    Score: 0.5950943529605865
    Answer: 208000
    Ground truth:  224000
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount of money the company pays to all the programmers per mo...
    Score: 0.5919350981712341
    Answer: 208000
    Ground truth:  224000
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

1. Calculate the numbe...
    Score: 0.8081989288330078
    Answer: 224000
    Ground truth:  224000
Method name: attention_weighted_confidence, running accuracy: 88.81118881118881
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.81118881118881
Method name: cer_prob_product_log_last, running accuracy: 88.11188811188812
Method name: self_consistency, running accuracy: 88.81118881118881
Method name: p_true, running accuracy: 89.5104895104895
Method name: normilized_likelihood, running accuracy: 88.11188811188812
Method name: normilized_entropy, running accuracy: 87.41258741258741
Method name: topk_entropy, running accuracy: 87.41258741258741
Method name: window_entropy, running accuracy: 88.81118881118881

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 142/500 [12:42:40<34:31:24, 347.16s/it, attention_weighted_confidence_acc=88.81%, cer_entropy_weighted_mean_all_acc=88.81%, cer_prob_product_log_last_acc=88.11%, self_consistency_acc=88.81%, p_true_acc=89.51%, normilized_likelihood_acc=88.11%, normilized_entropy_acc=87.41%, topk_entropy_acc=87.41%, window_entropy_acc=88.81%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▊       | 143/500 [12:42:40<38:59:36, 393.21s/it, attention_weighted_confidence_acc=88.81%, cer_entropy_weighted_mean_all_acc=88.81%, cer_prob_product_log_last_acc=88.11%, self_consistency_acc=88.81%, p_true_acc=89.51%, normilized_likelihood_acc=88.11%, normilized_entropy_acc=87.41%, topk_entropy_acc=87.41%, window_entropy_acc=88.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 7.657606717446838
    Answer: 1110
    Ground truth:  1110
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 7.657606717446838
    Answer: 1110
    Ground truth:  1110
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 7.496921479701996
    Answer: 1110
    Ground truth:  1110
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 0.9375
    Answer: 1110
    Ground truth:  1110
Method 5: p_true
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 13.84765625
    Answer: 1110
    Ground truth:  1110
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 1.9563563019037247
    Answer: 1110
    Ground truth:  1110
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 1.3810075521469116
    Answer: 1110
    Ground truth:  1110
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 1.3772007375955582
    Answer: 1110
    Ground truth:  1110
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount spent by Tim, we need to calculate the cost of the fire...
    Score: 4.46676367521286
    Answer: 1110
    Ground truth:  1110
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 88.19444444444444
Method name: self_consistency, running accuracy: 88.88888888888889
Method name: p_true, running accuracy: 89.58333333333334
Method name: normilized_likelihood, running accuracy: 88.19444444444444
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 88.88888888888889

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▊       | 143/500 [12:47:50<38:59:36, 393.21s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.19%, self_consistency_acc=88.89%, p_true_acc=89.58%, normilized_likelihood_acc=88.19%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 144/500 [12:47:50<36:25:16, 368.31s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.19%, self_consistency_acc=88.89%, p_true_acc=89.58%, normilized_likelihood_acc=88.19%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 7.635960319707287
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 7.635960319707287
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 15.999573290348053
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 14.203125
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 2.0955549627542496
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 2.253638818860054
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 2.2117983549833298
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of the crayons and Violetta's change, let's break down th...
    Score: 10.386846542358398
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 88.96551724137932
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.96551724137932
Method name: cer_prob_product_log_last, running accuracy: 88.27586206896552
Method name: self_consistency, running accuracy: 88.96551724137932
Method name: p_true, running accuracy: 89.65517241379311
Method name: normilized_likelihood, running accuracy: 88.27586206896552
Method name: normilized_entropy, running accuracy: 87.58620689655172
Method name: topk_entropy, running accuracy: 87.58620689655172
Method name: window_entropy, running accuracy: 88.96551724137932

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 144/500 [12:50:55<36:25:16, 368.31s/it, attention_weighted_confidence_acc=88.97%, cer_entropy_weighted_mean_all_acc=88.97%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=88.97%, p_true_acc=89.66%, normilized_likelihood_acc=88.28%, normilized_entropy_acc=87.59%, topk_entropy_acc=87.59%, window_entropy_acc=88.97%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 145/500 [12:50:55<30:54:00, 313.35s/it, attention_weighted_confidence_acc=88.97%, cer_entropy_weighted_mean_all_acc=88.97%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=88.97%, p_true_acc=89.66%, normilized_likelihood_acc=88.28%, normilized_entropy_acc=87.59%, topk_entropy_acc=87.59%, window_entropy_acc=88.97%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 8.112102557230955
    Answer: 60
    Ground truth:  60
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 8.112102557230955
    Answer: 60
    Ground truth:  60
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 15.993325412273407
    Answer: 60
    Ground truth:  60
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 1.0
    Answer: 60
    Ground truth:  60
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 14.703125
    Answer: 60
    Ground truth:  60
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 1.3130858391523361
    Answer: 60
    Ground truth:  60
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 0.8929189443588257
    Answer: 60
    Ground truth:  60
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 0.8924982696771622
    Answer: 60
    Ground truth:  60
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Bailey receives a weekly allowanc...
    Score: 4.803578317165375
    Answer: 60
    Ground truth:  60
Method name: attention_weighted_confidence, running accuracy: 89.04109589041096
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.04109589041096
Method name: cer_prob_product_log_last, running accuracy: 88.35616438356165
Method name: self_consistency, running accuracy: 89.04109589041096
Method name: p_true, running accuracy: 89.72602739726028
Method name: normilized_likelihood, running accuracy: 88.35616438356165
Method name: normilized_entropy, running accuracy: 87.67123287671232
Method name: topk_entropy, running accuracy: 87.67123287671232
Method name: window_entropy, running accuracy: 89.04109589041096

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 145/500 [12:54:44<30:54:00, 313.35s/it, attention_weighted_confidence_acc=89.04%, cer_entropy_weighted_mean_all_acc=89.04%, cer_prob_product_log_last_acc=88.36%, self_consistency_acc=89.04%, p_true_acc=89.73%, normilized_likelihood_acc=88.36%, normilized_entropy_acc=87.67%, topk_entropy_acc=87.67%, window_entropy_acc=89.04%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 146/500 [12:54:44<28:18:40, 287.91s/it, attention_weighted_confidence_acc=89.04%, cer_entropy_weighted_mean_all_acc=89.04%, cer_prob_product_log_last_acc=88.36%, self_consistency_acc=89.04%, p_true_acc=89.73%, normilized_likelihood_acc=88.36%, normilized_entropy_acc=87.67%, topk_entropy_acc=87.67%, window_entropy_acc=89.04%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 8.372649837033972
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 8.372649837033972
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 15.999900221824646
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 1.0
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 14.25
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 2.280261356383562
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 2.071614556014538
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 2.0648969635367393
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To find out how old Jackson is, let's break it down into steps.

1. James is 10 ...
    Score: 7.210551142692566
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.1156462585034
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.1156462585034
Method name: cer_prob_product_log_last, running accuracy: 88.43537414965986
Method name: self_consistency, running accuracy: 89.1156462585034
Method name: p_true, running accuracy: 89.79591836734694
Method name: normilized_likelihood, running accuracy: 88.43537414965986
Method name: normilized_entropy, running accuracy: 87.75510204081633
Method name: topk_entropy, running accuracy: 87.75510204081633
Method name: window_entropy, running accuracy: 89.1156462585034

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 146/500 [12:57:49<28:18:40, 287.91s/it, attention_weighted_confidence_acc=89.12%, cer_entropy_weighted_mean_all_acc=89.12%, cer_prob_product_log_last_acc=88.44%, self_consistency_acc=89.12%, p_true_acc=89.80%, normilized_likelihood_acc=88.44%, normilized_entropy_acc=87.76%, topk_entropy_acc=87.76%, window_entropy_acc=89.12%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 147/500 [12:57:49<25:12:02, 257.00s/it, attention_weighted_confidence_acc=89.12%, cer_entropy_weighted_mean_all_acc=89.12%, cer_prob_product_log_last_acc=88.44%, self_consistency_acc=89.12%, p_true_acc=89.80%, normilized_likelihood_acc=88.44%, normilized_entropy_acc=87.76%, topk_entropy_acc=87.76%, window_entropy_acc=89.12%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total time it takes Richard to set off all the soda fountains, we ne...
    Score: 3.6826756233250375
    Answer: 35
    Ground truth:  35
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total time it takes Richard to set off all the soda fountains, we ne...
    Score: 3.6826756233250375
    Answer: 35
    Ground truth:  35
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total time it takes Richard to set off all the soda fountains, we ne...
    Score: 7.956594228744507
    Answer: 35
    Ground truth:  35
Method 4: self_consistency
  Batch 1:
    Text: To find the total time it takes Richard to set off all the soda fountains, we ne...
    Score: 0.5
    Answer: 35
    Ground truth:  35
Method 5: p_true
  Batch 1:
    Text: To find the total time it takes Richard to set off all the soda fountains, we ne...
    Score: 6.5390625
    Answer: 35
    Ground truth:  35
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many seconds it will take Richard to set off all the soda founta...
    Score: 4.972746551036835
    Answer: 40
    Ground truth:  35
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many seconds it will take Richard to set off all the soda founta...
    Score: 4.9664832055568695
    Answer: 40
    Ground truth:  35
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many seconds it will take Richard to set off all the soda founta...
    Score: 4.405498027801514
    Answer: 40
    Ground truth:  35
Method 9: window_entropy
  Batch 1:
    Text: To find out how many seconds it will take Richard to set off all the soda founta...
    Score: 9.512030482292175
    Answer: 40
    Ground truth:  35
Method name: attention_weighted_confidence, running accuracy: 89.1891891891892
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.1891891891892
Method name: cer_prob_product_log_last, running accuracy: 88.51351351351352
Method name: self_consistency, running accuracy: 89.1891891891892
Method name: p_true, running accuracy: 89.86486486486487
Method name: normilized_likelihood, running accuracy: 87.83783783783784
Method name: normilized_entropy, running accuracy: 87.16216216216216
Method name: topk_entropy, running accuracy: 87.16216216216216
Method name: window_entropy, running accuracy: 88.51351351351352

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 147/500 [13:03:11<25:12:02, 257.00s/it, attention_weighted_confidence_acc=89.19%, cer_entropy_weighted_mean_all_acc=89.19%, cer_prob_product_log_last_acc=88.51%, self_consistency_acc=89.19%, p_true_acc=89.86%, normilized_likelihood_acc=87.84%, normilized_entropy_acc=87.16%, topk_entropy_acc=87.16%, window_entropy_acc=88.51%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|██▉       | 148/500 [13:03:11<27:02:47, 276.61s/it, attention_weighted_confidence_acc=89.19%, cer_entropy_weighted_mean_all_acc=89.19%, cer_prob_product_log_last_acc=88.51%, self_consistency_acc=89.19%, p_true_acc=89.86%, normilized_likelihood_acc=87.84%, normilized_entropy_acc=87.16%, topk_entropy_acc=87.16%, window_entropy_acc=88.51%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 7.919061362953032
    Answer: 120
    Ground truth:  120
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 7.919061362953032
    Answer: 120
    Ground truth:  120
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 15.996232986450195
    Answer: 120
    Ground truth:  120
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 1.0
    Answer: 120
    Ground truth:  120
Method 5: p_true
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 14.12890625
    Answer: 120
    Ground truth:  120
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 1.891762152314186
    Answer: 120
    Ground truth:  120
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 0.8801929354667664
    Answer: 120
    Ground truth:  120
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 0.8811672180891037
    Answer: 120
    Ground truth:  120
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount of water Hannah needs to drink, we first need to find t...
    Score: 3.639583110809326
    Answer: 120
    Ground truth:  120
Method name: attention_weighted_confidence, running accuracy: 89.26174496644296
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.26174496644296
Method name: cer_prob_product_log_last, running accuracy: 88.59060402684564
Method name: self_consistency, running accuracy: 89.26174496644296
Method name: p_true, running accuracy: 89.93288590604027
Method name: normilized_likelihood, running accuracy: 87.91946308724832
Method name: normilized_entropy, running accuracy: 87.24832214765101
Method name: topk_entropy, running accuracy: 87.24832214765101
Method name: window_entropy, running accuracy: 88.59060402684564

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|██▉       | 148/500 [13:06:49<27:02:47, 276.61s/it, attention_weighted_confidence_acc=89.26%, cer_entropy_weighted_mean_all_acc=89.26%, cer_prob_product_log_last_acc=88.59%, self_consistency_acc=89.26%, p_true_acc=89.93%, normilized_likelihood_acc=87.92%, normilized_entropy_acc=87.25%, topk_entropy_acc=87.25%, window_entropy_acc=88.59%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|██▉       | 149/500 [13:06:49<25:15:45, 259.10s/it, attention_weighted_confidence_acc=89.26%, cer_entropy_weighted_mean_all_acc=89.26%, cer_prob_product_log_last_acc=88.59%, self_consistency_acc=89.26%, p_true_acc=89.93%, normilized_likelihood_acc=87.92%, normilized_entropy_acc=87.25%, topk_entropy_acc=87.25%, window_entropy_acc=88.59%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 7.4245170113706624
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 7.4245170113706624
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 14.999075591564178
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 0.9375
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 10.82421875
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 8.823853313922882
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 8.893273010849953
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 8.196329712867737
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find out how many pieces of fruit are left, we need to first calculate the to...
    Score: 25.36650162935257
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 89.33333333333333
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.33333333333333
Method name: cer_prob_product_log_last, running accuracy: 88.66666666666667
Method name: self_consistency, running accuracy: 89.33333333333333
Method name: p_true, running accuracy: 90.0
Method name: normilized_likelihood, running accuracy: 88.0
Method name: normilized_entropy, running accuracy: 87.33333333333333
Method name: topk_entropy, running accuracy: 87.33333333333333
Method name: window_entropy, running accuracy: 88.66666666666667

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|██▉       | 149/500 [13:10:12<25:15:45, 259.10s/it, attention_weighted_confidence_acc=89.33%, cer_entropy_weighted_mean_all_acc=89.33%, cer_prob_product_log_last_acc=88.67%, self_consistency_acc=89.33%, p_true_acc=90.00%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.33%, topk_entropy_acc=87.33%, window_entropy_acc=88.67%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 150/500 [13:10:12<23:32:07, 242.08s/it, attention_weighted_confidence_acc=89.33%, cer_entropy_weighted_mean_all_acc=89.33%, cer_prob_product_log_last_acc=88.67%, self_consistency_acc=89.33%, p_true_acc=90.00%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.33%, topk_entropy_acc=87.33%, window_entropy_acc=88.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 5.1359533579572565
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 5.1359533579572565
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 10.022047847509384
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 0.6875
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 8.45703125
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 11.015220552682877
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 9.53952768445015
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 7.822940081357956
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: To find the number of eggs the friends found, we need to subtract the number of ...
    Score: 15.102738499641418
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 89.40397350993378
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.40397350993378
Method name: cer_prob_product_log_last, running accuracy: 88.74172185430463
Method name: self_consistency, running accuracy: 89.40397350993378
Method name: p_true, running accuracy: 90.06622516556291
Method name: normilized_likelihood, running accuracy: 88.0794701986755
Method name: normilized_entropy, running accuracy: 87.41721854304636
Method name: topk_entropy, running accuracy: 87.41721854304636
Method name: window_entropy, running accuracy: 88.74172185430463

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 150/500 [13:14:57<23:32:07, 242.08s/it, attention_weighted_confidence_acc=89.40%, cer_entropy_weighted_mean_all_acc=89.40%, cer_prob_product_log_last_acc=88.74%, self_consistency_acc=89.40%, p_true_acc=90.07%, normilized_likelihood_acc=88.08%, normilized_entropy_acc=87.42%, topk_entropy_acc=87.42%, window_entropy_acc=88.74%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 151/500 [13:14:57<24:43:00, 254.96s/it, attention_weighted_confidence_acc=89.40%, cer_entropy_weighted_mean_all_acc=89.40%, cer_prob_product_log_last_acc=88.74%, self_consistency_acc=89.40%, p_true_acc=90.07%, normilized_likelihood_acc=88.08%, normilized_entropy_acc=87.42%, topk_entropy_acc=87.42%, window_entropy_acc=88.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 3.93419777876775
    Answer: 113
    Ground truth:  98
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 3.93419777876775
    Answer: 113
    Ground truth:  98
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 7.999839425086975
    Answer: 113
    Ground truth:  98
Method 4: self_consistency
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 0.5
    Answer: 113
    Ground truth:  98
Method 5: p_true
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 7.73828125
    Answer: 113
    Ground truth:  98
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 0.6850412786006927
    Answer: 113
    Ground truth:  98
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 0.7974835634231567
    Answer: 113
    Ground truth:  98
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of guests at the end of the day, let's break down the events ...
    Score: 0.7880814969539642
    Answer: 113
    Ground truth:  98
Method 9: window_entropy
  Batch 1:
    Text: To find the current number of guests in the hotel, we need to follow the sequenc...
    Score: 3.3970276713371277
    Answer: 98
    Ground truth:  98
Method name: attention_weighted_confidence, running accuracy: 88.81578947368422
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.81578947368422
Method name: cer_prob_product_log_last, running accuracy: 88.1578947368421
Method name: self_consistency, running accuracy: 88.81578947368422
Method name: p_true, running accuracy: 89.47368421052632
Method name: normilized_likelihood, running accuracy: 87.5
Method name: normilized_entropy, running accuracy: 86.8421052631579
Method name: topk_entropy, running accuracy: 86.8421052631579
Method name: window_entropy, running accuracy: 88.81578947368422

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 151/500 [13:19:10<24:43:00, 254.96s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=88.16%, self_consistency_acc=88.82%, p_true_acc=89.47%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.84%, topk_entropy_acc=86.84%, window_entropy_acc=88.82%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 152/500 [13:19:10<24:36:04, 254.50s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=88.16%, self_consistency_acc=88.82%, p_true_acc=89.47%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.84%, topk_entropy_acc=86.84%, window_entropy_acc=88.82%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 7.995308531808418
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 7.995308531808418
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 15.999700427055359
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 14.30859375
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 2.3378793597221375
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 0.9258358180522919
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 0.9044264554977417
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: First, let's calculate the total number of diaper changes per day. Each child re...
    Score: 5.121691942214966
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 88.23529411764706
Method name: self_consistency, running accuracy: 88.88888888888889
Method name: p_true, running accuracy: 89.54248366013073
Method name: normilized_likelihood, running accuracy: 87.58169934640523
Method name: normilized_entropy, running accuracy: 86.9281045751634
Method name: topk_entropy, running accuracy: 86.9281045751634
Method name: window_entropy, running accuracy: 88.88888888888889

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 152/500 [13:23:01<24:36:04, 254.50s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=88.89%, p_true_acc=89.54%, normilized_likelihood_acc=87.58%, normilized_entropy_acc=86.93%, topk_entropy_acc=86.93%, window_entropy_acc=88.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 153/500 [13:23:01<23:50:32, 247.36s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=88.89%, p_true_acc=89.54%, normilized_likelihood_acc=87.58%, normilized_entropy_acc=86.93%, topk_entropy_acc=86.93%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 6.865349761613274
    Answer: 7425
    Ground truth:  7425
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 6.865349761613274
    Answer: 7425
    Ground truth:  7425
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 3.0194734015686273
    Answer: 7425
    Ground truth:  7425
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 0.875
    Answer: 7425
    Ground truth:  7425
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 12.09765625
    Answer: 7425
    Ground truth:  7425
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 1.3452754318714142
    Answer: 7425
    Ground truth:  7425
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 0.9344069361686707
    Answer: 7425
    Ground truth:  7425
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 0.9297631829977036
    Answer: 7425
    Ground truth:  7425
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 3.7766672372817993
    Answer: 7425
    Ground truth:  7425
Method name: attention_weighted_confidence, running accuracy: 88.96103896103897
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.96103896103897
Method name: cer_prob_product_log_last, running accuracy: 88.31168831168831
Method name: self_consistency, running accuracy: 88.96103896103897
Method name: p_true, running accuracy: 89.6103896103896
Method name: normilized_likelihood, running accuracy: 87.66233766233766
Method name: normilized_entropy, running accuracy: 87.01298701298701
Method name: topk_entropy, running accuracy: 87.01298701298701
Method name: window_entropy, running accuracy: 88.96103896103897

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 153/500 [13:27:40<23:50:32, 247.36s/it, attention_weighted_confidence_acc=88.96%, cer_entropy_weighted_mean_all_acc=88.96%, cer_prob_product_log_last_acc=88.31%, self_consistency_acc=88.96%, p_true_acc=89.61%, normilized_likelihood_acc=87.66%, normilized_entropy_acc=87.01%, topk_entropy_acc=87.01%, window_entropy_acc=88.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 154/500 [13:27:40<24:41:04, 256.83s/it, attention_weighted_confidence_acc=88.96%, cer_entropy_weighted_mean_all_acc=88.96%, cer_prob_product_log_last_acc=88.31%, self_consistency_acc=88.96%, p_true_acc=89.61%, normilized_likelihood_acc=87.66%, normilized_entropy_acc=87.01%, topk_entropy_acc=87.01%, window_entropy_acc=88.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 7.9649173568460006
    Answer: 70.0
    Ground truth:  70
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 7.9649173568460006
    Answer: 70.0
    Ground truth:  70
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of filling all 5 hanging baskets, we need to break it dow...
    Score: 0.9999964237213135
    Answer: 70
    Ground truth:  70
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 0.9375
    Answer: 70.0
    Ground truth:  70
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 11.3359375
    Answer: 70.0
    Ground truth:  70
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 1.4230135306715965
    Answer: 70.0
    Ground truth:  70
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 1.214040383696556
    Answer: 70.0
    Ground truth:  70
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 1.2109078913927078
    Answer: 70.0
    Ground truth:  70
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the co...
    Score: 2.8992122411727905
    Answer: 70.0
    Ground truth:  70
Method name: attention_weighted_confidence, running accuracy: 89.03225806451613
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.03225806451613
Method name: cer_prob_product_log_last, running accuracy: 88.38709677419355
Method name: self_consistency, running accuracy: 89.03225806451613
Method name: p_true, running accuracy: 89.6774193548387
Method name: normilized_likelihood, running accuracy: 87.74193548387098
Method name: normilized_entropy, running accuracy: 87.09677419354838
Method name: topk_entropy, running accuracy: 87.09677419354838
Method name: window_entropy, running accuracy: 89.03225806451613

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 154/500 [13:33:34<24:41:04, 256.83s/it, attention_weighted_confidence_acc=89.03%, cer_entropy_weighted_mean_all_acc=89.03%, cer_prob_product_log_last_acc=88.39%, self_consistency_acc=89.03%, p_true_acc=89.68%, normilized_likelihood_acc=87.74%, normilized_entropy_acc=87.10%, topk_entropy_acc=87.10%, window_entropy_acc=89.03%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 155/500 [13:33:34<27:24:44, 286.04s/it, attention_weighted_confidence_acc=89.03%, cer_entropy_weighted_mean_all_acc=89.03%, cer_prob_product_log_last_acc=88.39%, self_consistency_acc=89.03%, p_true_acc=89.68%, normilized_likelihood_acc=87.74%, normilized_entropy_acc=87.10%, topk_entropy_acc=87.10%, window_entropy_acc=89.03%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much each pump will cost, we need to calculate the total cost of...
    Score: 1.8962209495468056
    Answer: 20
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much each pump will cost, we need to calculate the total cost of...
    Score: 1.8962209495468056
    Answer: 20
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much each pump will cost, we need to calculate the total cost of...
    Score: 3.973335087299347
    Answer: 20
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find out how much each pump costs, we need to first find out how many pumps i...
    Score: 0.25
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find out how much each pump costs, we need to first find out how many pumps i...
    Score: 2.9794921875
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much each pump will cost, we need to calculate the total cost of...
    Score: 3.553365707397461
    Answer: 20
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much each pump will cost, we need to calculate the total cost of...
    Score: 3.349146753549576
    Answer: 20
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much each pump will cost, we need to calculate the total cost of...
    Score: 2.9620804488658905
    Answer: 20
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find out how much each pump will cost, we need to calculate the total cost of...
    Score: 5.6218849420547485
    Answer: 20
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 88.46153846153845
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.46153846153845
Method name: cer_prob_product_log_last, running accuracy: 87.82051282051282
Method name: self_consistency, running accuracy: 89.1025641025641
Method name: p_true, running accuracy: 89.74358974358975
Method name: normilized_likelihood, running accuracy: 87.17948717948718
Method name: normilized_entropy, running accuracy: 86.53846153846155
Method name: topk_entropy, running accuracy: 86.53846153846155
Method name: window_entropy, running accuracy: 88.46153846153845

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 155/500 [13:39:00<27:24:44, 286.04s/it, attention_weighted_confidence_acc=88.46%, cer_entropy_weighted_mean_all_acc=88.46%, cer_prob_product_log_last_acc=87.82%, self_consistency_acc=89.10%, p_true_acc=89.74%, normilized_likelihood_acc=87.18%, normilized_entropy_acc=86.54%, topk_entropy_acc=86.54%, window_entropy_acc=88.46%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 156/500 [13:39:00<28:28:21, 297.97s/it, attention_weighted_confidence_acc=88.46%, cer_entropy_weighted_mean_all_acc=88.46%, cer_prob_product_log_last_acc=87.82%, self_consistency_acc=89.10%, p_true_acc=89.74%, normilized_likelihood_acc=87.18%, normilized_entropy_acc=86.54%, topk_entropy_acc=86.54%, window_entropy_acc=88.46%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 7.712637596563321
    Answer: 26
    Ground truth:  26
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 7.712637596563321
    Answer: 26
    Ground truth:  26
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 15.994400680065155
    Answer: 26
    Ground truth:  26
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 1.0
    Answer: 26
    Ground truth:  26
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 12.64453125
    Answer: 26
    Ground truth:  26
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 2.382670298218727
    Answer: 26
    Ground truth:  26
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 1.7544493228197098
    Answer: 26
    Ground truth:  26
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 1.7442469596862793
    Answer: 26
    Ground truth:  26
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we first need to find the total number of pieces of pie i...
    Score: 3.8828116059303284
    Answer: 26
    Ground truth:  26
Method name: attention_weighted_confidence, running accuracy: 88.53503184713377
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.53503184713377
Method name: cer_prob_product_log_last, running accuracy: 87.89808917197452
Method name: self_consistency, running accuracy: 89.171974522293
Method name: p_true, running accuracy: 89.80891719745223
Method name: normilized_likelihood, running accuracy: 87.26114649681529
Method name: normilized_entropy, running accuracy: 86.62420382165605
Method name: topk_entropy, running accuracy: 86.62420382165605
Method name: window_entropy, running accuracy: 88.53503184713377

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 156/500 [13:42:26<28:28:21, 297.97s/it, attention_weighted_confidence_acc=88.54%, cer_entropy_weighted_mean_all_acc=88.54%, cer_prob_product_log_last_acc=87.90%, self_consistency_acc=89.17%, p_true_acc=89.81%, normilized_likelihood_acc=87.26%, normilized_entropy_acc=86.62%, topk_entropy_acc=86.62%, window_entropy_acc=88.54%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███▏      | 157/500 [13:42:26<25:45:49, 270.41s/it, attention_weighted_confidence_acc=88.54%, cer_entropy_weighted_mean_all_acc=88.54%, cer_prob_product_log_last_acc=87.90%, self_consistency_acc=89.17%, p_true_acc=89.81%, normilized_likelihood_acc=87.26%, normilized_entropy_acc=86.62%, topk_entropy_acc=86.62%, window_entropy_acc=88.54%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 5.385565131755094
    Answer: 6400
    Ground truth:  6400
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 5.385565131755094
    Answer: 6400
    Ground truth:  6400
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 5.392234184527421
    Answer: 6400
    Ground truth:  6400
Method 4: self_consistency
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 0.6875
    Answer: 6400
    Ground truth:  6400
Method 5: p_true
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 4.4315185546875
    Answer: 6400
    Ground truth:  6400
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 8.116281598806381
    Answer: 6400
    Ground truth:  6400
Method 7: normilized_entropy
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 8.02734586596489
    Answer: 6400
    Ground truth:  6400
Method 8: topk_entropy
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 6.969611316919327
    Answer: 6400
    Ground truth:  6400
Method 9: window_entropy
  Batch 1:
    Text: To find the value of all Jenna's jewels, we need to determine the total worth of...
    Score: 11.388109624385834
    Answer: 6400
    Ground truth:  6400
Method name: attention_weighted_confidence, running accuracy: 88.60759493670885
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.60759493670885
Method name: cer_prob_product_log_last, running accuracy: 87.9746835443038
Method name: self_consistency, running accuracy: 89.24050632911393
Method name: p_true, running accuracy: 89.87341772151899
Method name: normilized_likelihood, running accuracy: 87.34177215189874
Method name: normilized_entropy, running accuracy: 86.70886075949366
Method name: topk_entropy, running accuracy: 86.70886075949366
Method name: window_entropy, running accuracy: 88.60759493670885

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███▏      | 157/500 [13:49:35<25:45:49, 270.41s/it, attention_weighted_confidence_acc=88.61%, cer_entropy_weighted_mean_all_acc=88.61%, cer_prob_product_log_last_acc=87.97%, self_consistency_acc=89.24%, p_true_acc=89.87%, normilized_likelihood_acc=87.34%, normilized_entropy_acc=86.71%, topk_entropy_acc=86.71%, window_entropy_acc=88.61%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 158/500 [13:49:35<30:12:43, 318.02s/it, attention_weighted_confidence_acc=88.61%, cer_entropy_weighted_mean_all_acc=88.61%, cer_prob_product_log_last_acc=87.97%, self_consistency_acc=89.24%, p_true_acc=89.87%, normilized_likelihood_acc=87.34%, normilized_entropy_acc=86.71%, topk_entropy_acc=86.71%, window_entropy_acc=88.61%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 8.797244683962731
    Answer: 34
    Ground truth:  34
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 8.797244683962731
    Answer: 34
    Ground truth:  34
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 15.931655049324036
    Answer: 34
    Ground truth:  34
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 1.0
    Answer: 34
    Ground truth:  34
Method 5: p_true
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 12.51953125
    Answer: 34
    Ground truth:  34
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 3.4849248826503754
    Answer: 34
    Ground truth:  34
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 2.9827622398734093
    Answer: 34
    Ground truth:  34
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 2.9132755249738693
    Answer: 34
    Ground truth:  34
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Jen will earn, we need to calculate the total revenue from ...
    Score: 15.07463401556015
    Answer: 34
    Ground truth:  34
Method name: attention_weighted_confidence, running accuracy: 88.67924528301887
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.67924528301887
Method name: cer_prob_product_log_last, running accuracy: 88.0503144654088
Method name: self_consistency, running accuracy: 89.30817610062893
Method name: p_true, running accuracy: 89.937106918239
Method name: normilized_likelihood, running accuracy: 87.42138364779875
Method name: normilized_entropy, running accuracy: 86.79245283018868
Method name: topk_entropy, running accuracy: 86.79245283018868
Method name: window_entropy, running accuracy: 88.67924528301887

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 158/500 [13:54:39<30:12:43, 318.02s/it, attention_weighted_confidence_acc=88.68%, cer_entropy_weighted_mean_all_acc=88.68%, cer_prob_product_log_last_acc=88.05%, self_consistency_acc=89.31%, p_true_acc=89.94%, normilized_likelihood_acc=87.42%, normilized_entropy_acc=86.79%, topk_entropy_acc=86.79%, window_entropy_acc=88.68%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 159/500 [13:54:39<29:43:27, 313.80s/it, attention_weighted_confidence_acc=88.68%, cer_entropy_weighted_mean_all_acc=88.68%, cer_prob_product_log_last_acc=88.05%, self_consistency_acc=89.31%, p_true_acc=89.94%, normilized_likelihood_acc=87.42%, normilized_entropy_acc=86.79%, topk_entropy_acc=86.79%, window_entropy_acc=88.68%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 8.075728922154967
    Answer: 19
    Ground truth:  19
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 8.075728922154967
    Answer: 19
    Ground truth:  19
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 15.9775470495224
    Answer: 19
    Ground truth:  19
Method 4: self_consistency
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 1.0
    Answer: 19
    Ground truth:  19
Method 5: p_true
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 13.07421875
    Answer: 19
    Ground truth:  19
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 0.9840988665819168
    Answer: 19
    Ground truth:  19
Method 7: normilized_entropy
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 1.773456558585167
    Answer: 19
    Ground truth:  19
Method 8: topk_entropy
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 1.7653914391994476
    Answer: 19
    Ground truth:  19
Method 9: window_entropy
  Batch 1:
    Text: To find out the total miles Sam ran this week, we need to calculate the miles he...
    Score: 5.149799227714539
    Answer: 19
    Ground truth:  19
Method name: attention_weighted_confidence, running accuracy: 88.75
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.75
Method name: cer_prob_product_log_last, running accuracy: 88.125
Method name: self_consistency, running accuracy: 89.375
Method name: p_true, running accuracy: 90.0
Method name: normilized_likelihood, running accuracy: 87.5
Method name: normilized_entropy, running accuracy: 86.875
Method name: topk_entropy, running accuracy: 86.875
Method name: window_entropy, running accuracy: 88.75

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 159/500 [13:57:56<29:43:27, 313.80s/it, attention_weighted_confidence_acc=88.75%, cer_entropy_weighted_mean_all_acc=88.75%, cer_prob_product_log_last_acc=88.12%, self_consistency_acc=89.38%, p_true_acc=90.00%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.88%, topk_entropy_acc=86.88%, window_entropy_acc=88.75%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 160/500 [13:57:56<26:20:24, 278.90s/it, attention_weighted_confidence_acc=88.75%, cer_entropy_weighted_mean_all_acc=88.75%, cer_prob_product_log_last_acc=88.12%, self_consistency_acc=89.38%, p_true_acc=90.00%, normilized_likelihood_acc=87.50%, normilized_entropy_acc=86.88%, topk_entropy_acc=86.88%, window_entropy_acc=88.75%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 3.0702699501929467
    Answer: 500
    Ground truth:  500
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 3.0702699501929467
    Answer: 500
    Ground truth:  500
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 4.641142785549164
    Answer: 560
    Ground truth:  500
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 0.375
    Answer: 500
    Ground truth:  500
Method 5: p_true
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 4.46875
    Answer: 500
    Ground truth:  500
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 1.2353794425725937
    Answer: 500
    Ground truth:  500
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 1.1715857088565826
    Answer: 500
    Ground truth:  500
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 1.0754895508289337
    Answer: 500
    Ground truth:  500
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of people Conor saw at the beach that week, we need to ...
    Score: 4.5162283182144165
    Answer: 500
    Ground truth:  500
Method name: attention_weighted_confidence, running accuracy: 88.81987577639751
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.81987577639751
Method name: cer_prob_product_log_last, running accuracy: 87.5776397515528
Method name: self_consistency, running accuracy: 89.44099378881988
Method name: p_true, running accuracy: 90.06211180124224
Method name: normilized_likelihood, running accuracy: 87.5776397515528
Method name: normilized_entropy, running accuracy: 86.95652173913044
Method name: topk_entropy, running accuracy: 86.95652173913044
Method name: window_entropy, running accuracy: 88.81987577639751

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 160/500 [14:03:43<26:20:24, 278.90s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=87.58%, self_consistency_acc=89.44%, p_true_acc=90.06%, normilized_likelihood_acc=87.58%, normilized_entropy_acc=86.96%, topk_entropy_acc=86.96%, window_entropy_acc=88.82%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 161/500 [14:03:43<28:10:10, 299.15s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=87.58%, self_consistency_acc=89.44%, p_true_acc=90.06%, normilized_likelihood_acc=87.58%, normilized_entropy_acc=86.96%, topk_entropy_acc=86.96%, window_entropy_acc=88.82%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 7.9455673272641585
    Answer: 280
    Ground truth:  280
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 7.9455673272641585
    Answer: 280
    Ground truth:  280
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 14.982678711414337
    Answer: 280
    Ground truth:  280
Method 4: self_consistency
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 0.9375
    Answer: 280
    Ground truth:  280
Method 5: p_true
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 13.83203125
    Answer: 280
    Ground truth:  280
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 1.9088108539581299
    Answer: 280
    Ground truth:  280
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 2.2061158269643784
    Answer: 280
    Ground truth:  280
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 2.1068975776433945
    Answer: 280
    Ground truth:  280
Method 9: window_entropy
  Batch 1:
    Text: To find the number of students who went out through exit C, we need to break dow...
    Score: 10.294839859008789
    Answer: 280
    Ground truth:  280
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 87.65432098765432
Method name: self_consistency, running accuracy: 89.50617283950618
Method name: p_true, running accuracy: 90.12345679012346
Method name: normilized_likelihood, running accuracy: 87.65432098765432
Method name: normilized_entropy, running accuracy: 87.03703703703704
Method name: topk_entropy, running accuracy: 87.03703703703704
Method name: window_entropy, running accuracy: 88.88888888888889

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 161/500 [14:09:18<28:10:10, 299.15s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=87.65%, self_consistency_acc=89.51%, p_true_acc=90.12%, normilized_likelihood_acc=87.65%, normilized_entropy_acc=87.04%, topk_entropy_acc=87.04%, window_entropy_acc=88.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 162/500 [14:09:18<29:06:36, 310.05s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=87.65%, self_consistency_acc=89.51%, p_true_acc=90.12%, normilized_likelihood_acc=87.65%, normilized_entropy_acc=87.04%, topk_entropy_acc=87.04%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 5.676665230275852
    Answer: 42
    Ground truth:  42
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 5.676665230275852
    Answer: 42
    Ground truth:  42
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 10.995104253292084
    Answer: 42
    Ground truth:  42
Method 4: self_consistency
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 0.6875
    Answer: 42
    Ground truth:  42
Method 5: p_true
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 9.7109375
    Answer: 42
    Ground truth:  42
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 11.26549482345581
    Answer: 42
    Ground truth:  42
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 11.185883611440659
    Answer: 42
    Ground truth:  42
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 9.890300706028938
    Answer: 42
    Ground truth:  42
Method 9: window_entropy
  Batch 1:
    Text: To find out how many fish have red or blue stripes, we need to calculate the num...
    Score: 18.52535170316696
    Answer: 42
    Ground truth:  42
Method name: attention_weighted_confidence, running accuracy: 88.95705521472392
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.95705521472392
Method name: cer_prob_product_log_last, running accuracy: 87.73006134969326
Method name: self_consistency, running accuracy: 89.57055214723927
Method name: p_true, running accuracy: 90.1840490797546
Method name: normilized_likelihood, running accuracy: 87.73006134969326
Method name: normilized_entropy, running accuracy: 87.11656441717791
Method name: topk_entropy, running accuracy: 87.11656441717791
Method name: window_entropy, running accuracy: 88.95705521472392

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 162/500 [14:17:18<29:06:36, 310.05s/it, attention_weighted_confidence_acc=88.96%, cer_entropy_weighted_mean_all_acc=88.96%, cer_prob_product_log_last_acc=87.73%, self_consistency_acc=89.57%, p_true_acc=90.18%, normilized_likelihood_acc=87.73%, normilized_entropy_acc=87.12%, topk_entropy_acc=87.12%, window_entropy_acc=88.96%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 163/500 [14:17:18<33:47:24, 360.96s/it, attention_weighted_confidence_acc=88.96%, cer_entropy_weighted_mean_all_acc=88.96%, cer_prob_product_log_last_acc=87.73%, self_consistency_acc=89.57%, p_true_acc=90.18%, normilized_likelihood_acc=87.73%, normilized_entropy_acc=87.12%, topk_entropy_acc=87.12%, window_entropy_acc=88.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 4.7162709446044655
    Answer: 12.0
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 4.7162709446044655
    Answer: 12.0
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money David has left after buying the bolts, we need to fol...
    Score: 2.999979257583618
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 0.5625
    Answer: 12.0
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 7.21875
    Answer: 12.0
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 0.7711753398180008
    Answer: 12.0
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 0.5199134647846222
    Answer: 12.0
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 0.5176946371793747
    Answer: 12.0
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money David has left, we need to subtract the total cost of...
    Score: 1.9859371185302734
    Answer: 12.0
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 89.02439024390245
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.02439024390245
Method name: cer_prob_product_log_last, running accuracy: 87.8048780487805
Method name: self_consistency, running accuracy: 89.63414634146342
Method name: p_true, running accuracy: 90.2439024390244
Method name: normilized_likelihood, running accuracy: 87.8048780487805
Method name: normilized_entropy, running accuracy: 87.1951219512195
Method name: topk_entropy, running accuracy: 87.1951219512195
Method name: window_entropy, running accuracy: 89.02439024390245

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 163/500 [14:21:09<33:47:24, 360.96s/it, attention_weighted_confidence_acc=89.02%, cer_entropy_weighted_mean_all_acc=89.02%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=89.63%, p_true_acc=90.24%, normilized_likelihood_acc=87.80%, normilized_entropy_acc=87.20%, topk_entropy_acc=87.20%, window_entropy_acc=89.02%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 164/500 [14:21:09<30:02:36, 321.89s/it, attention_weighted_confidence_acc=89.02%, cer_entropy_weighted_mean_all_acc=89.02%, cer_prob_product_log_last_acc=87.80%, self_consistency_acc=89.63%, p_true_acc=90.24%, normilized_likelihood_acc=87.80%, normilized_entropy_acc=87.20%, topk_entropy_acc=87.20%, window_entropy_acc=89.02%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 8.46225658187212
    Answer: 32
    Ground truth:  32
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 8.46225658187212
    Answer: 32
    Ground truth:  32
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 15.99984335899353
    Answer: 32
    Ground truth:  32
Method 4: self_consistency
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 1.0
    Answer: 32
    Ground truth:  32
Method 5: p_true
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 13.57421875
    Answer: 32
    Ground truth:  32
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 1.2522686868906021
    Answer: 32
    Ground truth:  32
Method 7: normilized_entropy
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 0.8741652071475983
    Answer: 32
    Ground truth:  32
Method 8: topk_entropy
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 0.8753391355276108
    Answer: 32
    Ground truth:  32
Method 9: window_entropy
  Batch 1:
    Text: To find the price of the crate of fruit, we need to calculate the cost of each i...
    Score: 6.3177648186683655
    Answer: 32
    Ground truth:  32
Method name: attention_weighted_confidence, running accuracy: 89.0909090909091
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.0909090909091
Method name: cer_prob_product_log_last, running accuracy: 87.87878787878788
Method name: self_consistency, running accuracy: 89.6969696969697
Method name: p_true, running accuracy: 90.30303030303031
Method name: normilized_likelihood, running accuracy: 87.87878787878788
Method name: normilized_entropy, running accuracy: 87.27272727272727
Method name: topk_entropy, running accuracy: 87.27272727272727
Method name: window_entropy, running accuracy: 89.0909090909091

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 164/500 [14:26:16<30:02:36, 321.89s/it, attention_weighted_confidence_acc=89.09%, cer_entropy_weighted_mean_all_acc=89.09%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=89.70%, p_true_acc=90.30%, normilized_likelihood_acc=87.88%, normilized_entropy_acc=87.27%, topk_entropy_acc=87.27%, window_entropy_acc=89.09%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 165/500 [14:26:16<29:32:22, 317.44s/it, attention_weighted_confidence_acc=89.09%, cer_entropy_weighted_mean_all_acc=89.09%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=89.70%, p_true_acc=90.30%, normilized_likelihood_acc=87.88%, normilized_entropy_acc=87.27%, topk_entropy_acc=87.27%, window_entropy_acc=89.09%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 6.446357658161149
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 6.446357658161149
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 12.994362533092499
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 0.8125
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 11.28515625
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 6.47947484254837
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 6.573156833648682
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 5.654821455478668
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Let's start by noting the information given in the problem. 
- Total num...
    Score: 16.145575761795044
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 89.1566265060241
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.1566265060241
Method name: cer_prob_product_log_last, running accuracy: 87.95180722891565
Method name: self_consistency, running accuracy: 89.7590361445783
Method name: p_true, running accuracy: 90.36144578313254
Method name: normilized_likelihood, running accuracy: 87.95180722891565
Method name: normilized_entropy, running accuracy: 87.34939759036145
Method name: topk_entropy, running accuracy: 87.34939759036145
Method name: window_entropy, running accuracy: 89.1566265060241

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 165/500 [14:33:12<29:32:22, 317.44s/it, attention_weighted_confidence_acc=89.16%, cer_entropy_weighted_mean_all_acc=89.16%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=89.76%, p_true_acc=90.36%, normilized_likelihood_acc=87.95%, normilized_entropy_acc=87.35%, topk_entropy_acc=87.35%, window_entropy_acc=89.16%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 166/500 [14:33:12<32:12:00, 347.07s/it, attention_weighted_confidence_acc=89.16%, cer_entropy_weighted_mean_all_acc=89.16%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=89.76%, p_true_acc=90.36%, normilized_likelihood_acc=87.95%, normilized_entropy_acc=87.35%, topk_entropy_acc=87.35%, window_entropy_acc=89.16%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 8.035140038440886
    Answer: 350
    Ground truth:  350
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 8.035140038440886
    Answer: 350
    Ground truth:  350
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 15.99993109703064
    Answer: 350
    Ground truth:  350
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 1.0
    Answer: 350
    Ground truth:  350
Method 5: p_true
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 14.1328125
    Answer: 350
    Ground truth:  350
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 2.4298289120197296
    Answer: 350
    Ground truth:  350
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 1.0211666077375412
    Answer: 350
    Ground truth:  350
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 1.0254114121198654
    Answer: 350
    Ground truth:  350
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, we need to first find the total width of the windows, then...
    Score: 2.7478525042533875
    Answer: 350
    Ground truth:  350
Method name: attention_weighted_confidence, running accuracy: 89.22155688622755
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.22155688622755
Method name: cer_prob_product_log_last, running accuracy: 88.02395209580838
Method name: self_consistency, running accuracy: 89.82035928143712
Method name: p_true, running accuracy: 90.41916167664671
Method name: normilized_likelihood, running accuracy: 88.02395209580838
Method name: normilized_entropy, running accuracy: 87.42514970059881
Method name: topk_entropy, running accuracy: 87.42514970059881
Method name: window_entropy, running accuracy: 89.22155688622755

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 166/500 [14:36:25<32:12:00, 347.07s/it, attention_weighted_confidence_acc=89.22%, cer_entropy_weighted_mean_all_acc=89.22%, cer_prob_product_log_last_acc=88.02%, self_consistency_acc=89.82%, p_true_acc=90.42%, normilized_likelihood_acc=88.02%, normilized_entropy_acc=87.43%, topk_entropy_acc=87.43%, window_entropy_acc=89.22%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 167/500 [14:36:25<27:49:13, 300.76s/it, attention_weighted_confidence_acc=89.22%, cer_entropy_weighted_mean_all_acc=89.22%, cer_prob_product_log_last_acc=88.02%, self_consistency_acc=89.82%, p_true_acc=90.42%, normilized_likelihood_acc=88.02%, normilized_entropy_acc=87.43%, topk_entropy_acc=87.43%, window_entropy_acc=89.22%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 8.497354348223244
    Answer: 79
    Ground truth:  79
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 8.497354348223244
    Answer: 79
    Ground truth:  79
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 15.99984586238861
    Answer: 79
    Ground truth:  79
Method 4: self_consistency
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 1.0
    Answer: 79
    Ground truth:  79
Method 5: p_true
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 14.44140625
    Answer: 79
    Ground truth:  79
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 0.8790273815393448
    Answer: 79
    Ground truth:  79
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 0.9356957525014877
    Answer: 79
    Ground truth:  79
Method 8: topk_entropy
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 0.9349130541086197
    Answer: 79
    Ground truth:  79
Method 9: window_entropy
  Batch 1:
    Text: To find the total miles biked by Alisa and Stanley, we need to calculate the tot...
    Score: 2.310315430164337
    Answer: 79
    Ground truth:  79
Method name: attention_weighted_confidence, running accuracy: 89.28571428571429
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.28571428571429
Method name: cer_prob_product_log_last, running accuracy: 88.09523809523809
Method name: self_consistency, running accuracy: 89.88095238095238
Method name: p_true, running accuracy: 90.47619047619048
Method name: normilized_likelihood, running accuracy: 88.09523809523809
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 89.28571428571429

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 167/500 [14:40:26<27:49:13, 300.76s/it, attention_weighted_confidence_acc=89.29%, cer_entropy_weighted_mean_all_acc=89.29%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=89.88%, p_true_acc=90.48%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=89.29%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▎      | 168/500 [14:40:26<26:06:18, 283.07s/it, attention_weighted_confidence_acc=89.29%, cer_entropy_weighted_mean_all_acc=89.29%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=89.88%, p_true_acc=90.48%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=89.29%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 5.0663731382033665
    Answer: 6.0
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 5.0663731382033665
    Answer: 6.0
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount the mother spent, we need to add the prices of the item...
    Score: 1.9902433156967163
    Answer: 6
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 0.625
    Answer: 6.0
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 7.89453125
    Answer: 6.0
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 1.6840456873178482
    Answer: 6.0
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 1.2008918523788452
    Answer: 6.0
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 1.1686702817678452
    Answer: 6.0
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how much change the cashier gives back, we need to subtract the tota...
    Score: 2.681034803390503
    Answer: 6.0
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 88.75739644970415
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.75739644970415
Method name: cer_prob_product_log_last, running accuracy: 87.57396449704143
Method name: self_consistency, running accuracy: 89.3491124260355
Method name: p_true, running accuracy: 89.94082840236686
Method name: normilized_likelihood, running accuracy: 87.57396449704143
Method name: normilized_entropy, running accuracy: 86.98224852071006
Method name: topk_entropy, running accuracy: 86.98224852071006
Method name: window_entropy, running accuracy: 88.75739644970415

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▎      | 168/500 [14:43:45<26:06:18, 283.07s/it, attention_weighted_confidence_acc=88.76%, cer_entropy_weighted_mean_all_acc=88.76%, cer_prob_product_log_last_acc=87.57%, self_consistency_acc=89.35%, p_true_acc=89.94%, normilized_likelihood_acc=87.57%, normilized_entropy_acc=86.98%, topk_entropy_acc=86.98%, window_entropy_acc=88.76%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 169/500 [14:43:45<23:42:22, 257.83s/it, attention_weighted_confidence_acc=88.76%, cer_entropy_weighted_mean_all_acc=88.76%, cer_prob_product_log_last_acc=87.57%, self_consistency_acc=89.35%, p_true_acc=89.94%, normilized_likelihood_acc=87.57%, normilized_entropy_acc=86.98%, topk_entropy_acc=86.98%, window_entropy_acc=88.76%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 7.863167176847173
    Answer: 21
    Ground truth:  21
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 7.863167176847173
    Answer: 21
    Ground truth:  21
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 14.99889600276947
    Answer: 21
    Ground truth:  21
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 0.9375
    Answer: 21
    Ground truth:  21
Method 5: p_true
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 12.40625
    Answer: 21
    Ground truth:  21
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 2.5431500375270844
    Answer: 21
    Ground truth:  21
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 2.982606276869774
    Answer: 21
    Ground truth:  21
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 2.81325127184391
    Answer: 21
    Ground truth:  21
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Dean has left, we first need to determine how much mo...
    Score: 13.001350909471512
    Answer: 21
    Ground truth:  21
Method name: attention_weighted_confidence, running accuracy: 88.8235294117647
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.8235294117647
Method name: cer_prob_product_log_last, running accuracy: 87.6470588235294
Method name: self_consistency, running accuracy: 89.41176470588236
Method name: p_true, running accuracy: 90.0
Method name: normilized_likelihood, running accuracy: 87.6470588235294
Method name: normilized_entropy, running accuracy: 87.05882352941177
Method name: topk_entropy, running accuracy: 87.05882352941177
Method name: window_entropy, running accuracy: 88.8235294117647

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 169/500 [14:49:00<23:42:22, 257.83s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=87.65%, self_consistency_acc=89.41%, p_true_acc=90.00%, normilized_likelihood_acc=87.65%, normilized_entropy_acc=87.06%, topk_entropy_acc=87.06%, window_entropy_acc=88.82%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 170/500 [14:49:00<25:11:06, 274.75s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=87.65%, self_consistency_acc=89.41%, p_true_acc=90.00%, normilized_likelihood_acc=87.65%, normilized_entropy_acc=87.06%, topk_entropy_acc=87.06%, window_entropy_acc=88.82%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 5.413318423129215
    Answer: 155
    Ground truth:  155
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 5.413318423129215
    Answer: 155
    Ground truth:  155
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 10.993279337882996
    Answer: 155
    Ground truth:  155
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 0.6875
    Answer: 155
    Ground truth:  155
Method 5: p_true
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 9.5625
    Answer: 155
    Ground truth:  155
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 15.582834720611572
    Answer: 155
    Ground truth:  155
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 14.992073148488998
    Answer: 155
    Ground truth:  155
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 12.902545064687729
    Answer: 155
    Ground truth:  155
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Calculate the amount of money Carrie received from twenties.
Carrie was ...
    Score: 17.572019457817078
    Answer: 155
    Ground truth:  155
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 87.71929824561403
Method name: self_consistency, running accuracy: 89.47368421052632
Method name: p_true, running accuracy: 90.05847953216374
Method name: normilized_likelihood, running accuracy: 87.71929824561403
Method name: normilized_entropy, running accuracy: 87.13450292397661
Method name: topk_entropy, running accuracy: 87.13450292397661
Method name: window_entropy, running accuracy: 88.88888888888889

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 170/500 [14:54:56<25:11:06, 274.75s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=87.72%, self_consistency_acc=89.47%, p_true_acc=90.06%, normilized_likelihood_acc=87.72%, normilized_entropy_acc=87.13%, topk_entropy_acc=87.13%, window_entropy_acc=88.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 171/500 [14:54:56<27:20:43, 299.22s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=87.72%, self_consistency_acc=89.47%, p_true_acc=90.06%, normilized_likelihood_acc=87.72%, normilized_entropy_acc=87.13%, topk_entropy_acc=87.13%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 8.575395849914662
    Answer: 23
    Ground truth:  23
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 8.575395849914662
    Answer: 23
    Ground truth:  23
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 15.999471187591553
    Answer: 23
    Ground truth:  23
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 1.0
    Answer: 23
    Ground truth:  23
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 15.41796875
    Answer: 23
    Ground truth:  23
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 1.7906282842159271
    Answer: 23
    Ground truth:  23
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 2.038786605000496
    Answer: 23
    Ground truth:  23
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 2.028697654604912
    Answer: 23
    Ground truth:  23
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. The rink has 12 red cars.

2. The...
    Score: 3.768128454685211
    Answer: 23
    Ground truth:  23
Method name: attention_weighted_confidence, running accuracy: 88.95348837209302
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.95348837209302
Method name: cer_prob_product_log_last, running accuracy: 87.79069767441861
Method name: self_consistency, running accuracy: 89.53488372093024
Method name: p_true, running accuracy: 90.11627906976744
Method name: normilized_likelihood, running accuracy: 87.79069767441861
Method name: normilized_entropy, running accuracy: 87.20930232558139
Method name: topk_entropy, running accuracy: 87.20930232558139
Method name: window_entropy, running accuracy: 88.95348837209302

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 171/500 [15:00:05<27:20:43, 299.22s/it, attention_weighted_confidence_acc=88.95%, cer_entropy_weighted_mean_all_acc=88.95%, cer_prob_product_log_last_acc=87.79%, self_consistency_acc=89.53%, p_true_acc=90.12%, normilized_likelihood_acc=87.79%, normilized_entropy_acc=87.21%, topk_entropy_acc=87.21%, window_entropy_acc=88.95%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 172/500 [15:00:05<27:31:21, 302.08s/it, attention_weighted_confidence_acc=88.95%, cer_entropy_weighted_mean_all_acc=88.95%, cer_prob_product_log_last_acc=87.79%, self_consistency_acc=89.53%, p_true_acc=90.12%, normilized_likelihood_acc=87.79%, normilized_entropy_acc=87.21%, topk_entropy_acc=87.21%, window_entropy_acc=88.95%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 7.897522310047078
    Answer: 60
    Ground truth:  60
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 7.897522310047078
    Answer: 60
    Ground truth:  60
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 13.168793082237244
    Answer: 60
    Ground truth:  60
Method 4: self_consistency
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 1.0
    Answer: 60
    Ground truth:  60
Method 5: p_true
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 11.85546875
    Answer: 60
    Ground truth:  60
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 3.0389879941940308
    Answer: 60
    Ground truth:  60
Method 7: normilized_entropy
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 2.687333643436432
    Answer: 60
    Ground truth:  60
Method 8: topk_entropy
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 2.629925638437271
    Answer: 60
    Ground truth:  60
Method 9: window_entropy
  Batch 1:
    Text: To find the percentage of the distance traveled, we need to first find out the t...
    Score: 12.944110929965973
    Answer: 60
    Ground truth:  60
Method name: attention_weighted_confidence, running accuracy: 89.01734104046243
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.01734104046243
Method name: cer_prob_product_log_last, running accuracy: 87.86127167630057
Method name: self_consistency, running accuracy: 89.59537572254335
Method name: p_true, running accuracy: 90.17341040462428
Method name: normilized_likelihood, running accuracy: 87.86127167630057
Method name: normilized_entropy, running accuracy: 87.28323699421965
Method name: topk_entropy, running accuracy: 87.28323699421965
Method name: window_entropy, running accuracy: 89.01734104046243

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 172/500 [15:04:48<27:31:21, 302.08s/it, attention_weighted_confidence_acc=89.02%, cer_entropy_weighted_mean_all_acc=89.02%, cer_prob_product_log_last_acc=87.86%, self_consistency_acc=89.60%, p_true_acc=90.17%, normilized_likelihood_acc=87.86%, normilized_entropy_acc=87.28%, topk_entropy_acc=87.28%, window_entropy_acc=89.02%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▍      | 173/500 [15:04:48<26:56:17, 296.57s/it, attention_weighted_confidence_acc=89.02%, cer_entropy_weighted_mean_all_acc=89.02%, cer_prob_product_log_last_acc=87.86%, self_consistency_acc=89.60%, p_true_acc=90.17%, normilized_likelihood_acc=87.86%, normilized_entropy_acc=87.28%, topk_entropy_acc=87.28%, window_entropy_acc=89.02%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 8.559221298416356
    Answer: 11
    Ground truth:  11
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 8.559221298416356
    Answer: 11
    Ground truth:  11
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 15.99856573343277
    Answer: 11
    Ground truth:  11
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 1.0
    Answer: 11
    Ground truth:  11
Method 5: p_true
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 14.38671875
    Answer: 11
    Ground truth:  11
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 1.2133334577083588
    Answer: 11
    Ground truth:  11
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 0.9256822466850281
    Answer: 11
    Ground truth:  11
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 0.9251627773046494
    Answer: 11
    Ground truth:  11
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Aleksandra had to pay, let's calculate the price of each it...
    Score: 3.3674222230911255
    Answer: 11
    Ground truth:  11
Method name: attention_weighted_confidence, running accuracy: 89.08045977011494
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.08045977011494
Method name: cer_prob_product_log_last, running accuracy: 87.93103448275862
Method name: self_consistency, running accuracy: 89.65517241379311
Method name: p_true, running accuracy: 90.22988505747126
Method name: normilized_likelihood, running accuracy: 87.93103448275862
Method name: normilized_entropy, running accuracy: 87.35632183908046
Method name: topk_entropy, running accuracy: 87.35632183908046
Method name: window_entropy, running accuracy: 89.08045977011494

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▍      | 173/500 [15:09:42<26:56:17, 296.57s/it, attention_weighted_confidence_acc=89.08%, cer_entropy_weighted_mean_all_acc=89.08%, cer_prob_product_log_last_acc=87.93%, self_consistency_acc=89.66%, p_true_acc=90.23%, normilized_likelihood_acc=87.93%, normilized_entropy_acc=87.36%, topk_entropy_acc=87.36%, window_entropy_acc=89.08%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▍      | 174/500 [15:09:42<26:46:54, 295.75s/it, attention_weighted_confidence_acc=89.08%, cer_entropy_weighted_mean_all_acc=89.08%, cer_prob_product_log_last_acc=87.93%, self_consistency_acc=89.66%, p_true_acc=90.23%, normilized_likelihood_acc=87.93%, normilized_entropy_acc=87.36%, topk_entropy_acc=87.36%, window_entropy_acc=89.08%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 7.902330070013425
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 7.902330070013425
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 15.604944288730621
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 1.0
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 15.23828125
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 6.485845446586609
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 6.077174663543701
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 5.464367538690567
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, we need to break it down step by step.

1. First, let's fi...
    Score: 24.58150887489319
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 89.14285714285714
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.14285714285714
Method name: cer_prob_product_log_last, running accuracy: 88.0
Method name: self_consistency, running accuracy: 89.71428571428571
Method name: p_true, running accuracy: 90.28571428571428
Method name: normilized_likelihood, running accuracy: 88.0
Method name: normilized_entropy, running accuracy: 87.42857142857143
Method name: topk_entropy, running accuracy: 87.42857142857143
Method name: window_entropy, running accuracy: 89.14285714285714

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▍      | 174/500 [15:14:35<26:46:54, 295.75s/it, attention_weighted_confidence_acc=89.14%, cer_entropy_weighted_mean_all_acc=89.14%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=89.71%, p_true_acc=90.29%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.43%, topk_entropy_acc=87.43%, window_entropy_acc=89.14%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 175/500 [15:14:35<26:37:05, 294.85s/it, attention_weighted_confidence_acc=89.14%, cer_entropy_weighted_mean_all_acc=89.14%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=89.71%, p_true_acc=90.29%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.43%, topk_entropy_acc=87.43%, window_entropy_acc=89.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 7.31617475133319
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 7.31617475133319
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 13.999350190162659
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 0.875
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 10.85546875
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 2.388536013662815
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 0.937569797039032
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 0.9295156449079514
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost and the change Adam got, we need to calculate the cost of...
    Score: 2.1548665165901184
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.20454545454545
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.20454545454545
Method name: cer_prob_product_log_last, running accuracy: 88.06818181818183
Method name: self_consistency, running accuracy: 89.77272727272727
Method name: p_true, running accuracy: 90.3409090909091
Method name: normilized_likelihood, running accuracy: 88.06818181818183
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 87.5
Method name: window_entropy, running accuracy: 89.20454545454545

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 175/500 [15:18:48<26:37:05, 294.85s/it, attention_weighted_confidence_acc=89.20%, cer_entropy_weighted_mean_all_acc=89.20%, cer_prob_product_log_last_acc=88.07%, self_consistency_acc=89.77%, p_true_acc=90.34%, normilized_likelihood_acc=88.07%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=89.20%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 176/500 [15:18:48<25:25:11, 282.44s/it, attention_weighted_confidence_acc=89.20%, cer_entropy_weighted_mean_all_acc=89.20%, cer_prob_product_log_last_acc=88.07%, self_consistency_acc=89.77%, p_true_acc=90.34%, normilized_likelihood_acc=88.07%, normilized_entropy_acc=87.50%, topk_entropy_acc=87.50%, window_entropy_acc=89.20%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 7.906492470967864
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 7.906492470967864
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 15.997281670570374
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 14.55078125
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 2.8519716560840607
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 2.0474352091550827
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 2.020309403538704
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To determine how many packs of gum Parker will need to last him 30 days, we need...
    Score: 5.555123746395111
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 89.26553672316385
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.26553672316385
Method name: cer_prob_product_log_last, running accuracy: 88.13559322033898
Method name: self_consistency, running accuracy: 89.83050847457628
Method name: p_true, running accuracy: 90.3954802259887
Method name: normilized_likelihood, running accuracy: 88.13559322033898
Method name: normilized_entropy, running accuracy: 87.57062146892656
Method name: topk_entropy, running accuracy: 87.57062146892656
Method name: window_entropy, running accuracy: 89.26553672316385

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 176/500 [15:22:31<25:25:11, 282.44s/it, attention_weighted_confidence_acc=89.27%, cer_entropy_weighted_mean_all_acc=89.27%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=89.83%, p_true_acc=90.40%, normilized_likelihood_acc=88.14%, normilized_entropy_acc=87.57%, topk_entropy_acc=87.57%, window_entropy_acc=89.27%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 177/500 [15:22:31<23:43:00, 264.34s/it, attention_weighted_confidence_acc=89.27%, cer_entropy_weighted_mean_all_acc=89.27%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=89.83%, p_true_acc=90.40%, normilized_likelihood_acc=88.14%, normilized_entropy_acc=87.57%, topk_entropy_acc=87.57%, window_entropy_acc=89.27%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 7.735873016795536
    Answer: 120
    Ground truth:  120
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 7.735873016795536
    Answer: 120
    Ground truth:  120
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 14.99484497308731
    Answer: 120
    Ground truth:  120
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 0.9375
    Answer: 120
    Ground truth:  120
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 12.4921875
    Answer: 120
    Ground truth:  120
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 5.042274832725525
    Answer: 120
    Ground truth:  120
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 4.28542760014534
    Answer: 120
    Ground truth:  120
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 3.9720351696014404
    Answer: 120
    Ground truth:  120
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The zoo has twice a...
    Score: 15.258831977844238
    Answer: 120
    Ground truth:  120
Method name: attention_weighted_confidence, running accuracy: 89.32584269662921
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.32584269662921
Method name: cer_prob_product_log_last, running accuracy: 88.20224719101124
Method name: self_consistency, running accuracy: 89.8876404494382
Method name: p_true, running accuracy: 90.4494382022472
Method name: normilized_likelihood, running accuracy: 88.20224719101124
Method name: normilized_entropy, running accuracy: 87.64044943820225
Method name: topk_entropy, running accuracy: 87.64044943820225
Method name: window_entropy, running accuracy: 89.32584269662921

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 177/500 [15:28:18<23:43:00, 264.34s/it, attention_weighted_confidence_acc=89.33%, cer_entropy_weighted_mean_all_acc=89.33%, cer_prob_product_log_last_acc=88.20%, self_consistency_acc=89.89%, p_true_acc=90.45%, normilized_likelihood_acc=88.20%, normilized_entropy_acc=87.64%, topk_entropy_acc=87.64%, window_entropy_acc=89.33%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 178/500 [15:28:18<25:51:41, 289.13s/it, attention_weighted_confidence_acc=89.33%, cer_entropy_weighted_mean_all_acc=89.33%, cer_prob_product_log_last_acc=88.20%, self_consistency_acc=89.89%, p_true_acc=90.45%, normilized_likelihood_acc=88.20%, normilized_entropy_acc=87.64%, topk_entropy_acc=87.64%, window_entropy_acc=89.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 6.496025269172584
    Answer: 18
    Ground truth:  18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 6.496025269172584
    Answer: 18
    Ground truth:  18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 12.997484982013702
    Answer: 18
    Ground truth:  18
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 0.8125
    Answer: 18
    Ground truth:  18
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 12.31640625
    Answer: 18
    Ground truth:  18
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 3.896006241440773
    Answer: 18
    Ground truth:  18
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 4.176960542798042
    Answer: 18
    Ground truth:  18
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 3.9634600430727005
    Answer: 18
    Ground truth:  18
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1:  We are given ...
    Score: 14.51525193452835
    Answer: 18
    Ground truth:  18
Method name: attention_weighted_confidence, running accuracy: 89.3854748603352
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.3854748603352
Method name: cer_prob_product_log_last, running accuracy: 88.26815642458101
Method name: self_consistency, running accuracy: 89.94413407821229
Method name: p_true, running accuracy: 90.5027932960894
Method name: normilized_likelihood, running accuracy: 88.26815642458101
Method name: normilized_entropy, running accuracy: 87.70949720670392
Method name: topk_entropy, running accuracy: 87.70949720670392
Method name: window_entropy, running accuracy: 89.3854748603352

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 178/500 [15:36:49<25:51:41, 289.13s/it, attention_weighted_confidence_acc=89.39%, cer_entropy_weighted_mean_all_acc=89.39%, cer_prob_product_log_last_acc=88.27%, self_consistency_acc=89.94%, p_true_acc=90.50%, normilized_likelihood_acc=88.27%, normilized_entropy_acc=87.71%, topk_entropy_acc=87.71%, window_entropy_acc=89.39%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 179/500 [15:36:49<31:42:50, 355.67s/it, attention_weighted_confidence_acc=89.39%, cer_entropy_weighted_mean_all_acc=89.39%, cer_prob_product_log_last_acc=88.27%, self_consistency_acc=89.94%, p_true_acc=90.50%, normilized_likelihood_acc=88.27%, normilized_entropy_acc=87.71%, topk_entropy_acc=87.71%, window_entropy_acc=89.39%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 8.029630776968197
    Answer: 48
    Ground truth:  48
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 8.029630776968197
    Answer: 48
    Ground truth:  48
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 15.999934673309326
    Answer: 48
    Ground truth:  48
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 1.0
    Answer: 48
    Ground truth:  48
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 13.015625
    Answer: 48
    Ground truth:  48
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 1.9456519484519958
    Answer: 48
    Ground truth:  48
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 1.2972985059022903
    Answer: 48
    Ground truth:  48
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 1.290154680609703
    Answer: 48
    Ground truth:  48
Method 9: window_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Each school sends 2 basketball teams (gi...
    Score: 4.193487107753754
    Answer: 48
    Ground truth:  48
Method name: attention_weighted_confidence, running accuracy: 89.44444444444444
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.44444444444444
Method name: cer_prob_product_log_last, running accuracy: 88.33333333333333
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.55555555555556
Method name: normilized_likelihood, running accuracy: 88.33333333333333
Method name: normilized_entropy, running accuracy: 87.77777777777777
Method name: topk_entropy, running accuracy: 87.77777777777777
Method name: window_entropy, running accuracy: 89.44444444444444

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 179/500 [15:41:18<31:42:50, 355.67s/it, attention_weighted_confidence_acc=89.44%, cer_entropy_weighted_mean_all_acc=89.44%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=90.00%, p_true_acc=90.56%, normilized_likelihood_acc=88.33%, normilized_entropy_acc=87.78%, topk_entropy_acc=87.78%, window_entropy_acc=89.44%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 180/500 [15:41:18<29:18:20, 329.69s/it, attention_weighted_confidence_acc=89.44%, cer_entropy_weighted_mean_all_acc=89.44%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=90.00%, p_true_acc=90.56%, normilized_likelihood_acc=88.33%, normilized_entropy_acc=87.78%, topk_entropy_acc=87.78%, window_entropy_acc=89.44%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 8.34540016262481
    Answer: 80
    Ground truth:  80
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 8.34540016262481
    Answer: 80
    Ground truth:  80
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 15.99949312210083
    Answer: 80
    Ground truth:  80
Method 4: self_consistency
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 1.0
    Answer: 80
    Ground truth:  80
Method 5: p_true
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 10.57421875
    Answer: 80
    Ground truth:  80
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 2.0473500341176987
    Answer: 80
    Ground truth:  80
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 1.5184468924999237
    Answer: 80
    Ground truth:  80
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 1.5015485286712646
    Answer: 80
    Ground truth:  80
Method 9: window_entropy
  Batch 1:
    Text: To calculate the total number of leftover burritos, we'll first find out the tot...
    Score: 3.360214650630951
    Answer: 80
    Ground truth:  80
Method name: attention_weighted_confidence, running accuracy: 89.50276243093923
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.50276243093923
Method name: cer_prob_product_log_last, running accuracy: 88.39779005524862
Method name: self_consistency, running accuracy: 90.05524861878453
Method name: p_true, running accuracy: 90.60773480662984
Method name: normilized_likelihood, running accuracy: 88.39779005524862
Method name: normilized_entropy, running accuracy: 87.84530386740332
Method name: topk_entropy, running accuracy: 87.84530386740332
Method name: window_entropy, running accuracy: 89.50276243093923

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 180/500 [15:45:59<29:18:20, 329.69s/it, attention_weighted_confidence_acc=89.50%, cer_entropy_weighted_mean_all_acc=89.50%, cer_prob_product_log_last_acc=88.40%, self_consistency_acc=90.06%, p_true_acc=90.61%, normilized_likelihood_acc=88.40%, normilized_entropy_acc=87.85%, topk_entropy_acc=87.85%, window_entropy_acc=89.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 181/500 [15:45:59<27:56:23, 315.31s/it, attention_weighted_confidence_acc=89.50%, cer_entropy_weighted_mean_all_acc=89.50%, cer_prob_product_log_last_acc=88.40%, self_consistency_acc=90.06%, p_true_acc=90.61%, normilized_likelihood_acc=88.40%, normilized_entropy_acc=87.85%, topk_entropy_acc=87.85%, window_entropy_acc=89.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 2.2677867637112588
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 2.2677867637112588
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 4.966152250766754
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 0.3125
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 2.951171875
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 3.1016906797885895
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 2.9835771918296814
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1.  Octavia drinks half the daily re...
    Score: 2.5406665802001953
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's start by finding out how many cups of coffee Octavi...
    Score: 4.769188225269318
    Answer: 36
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 89.56043956043956
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.56043956043956
Method name: cer_prob_product_log_last, running accuracy: 88.46153846153845
Method name: self_consistency, running accuracy: 90.10989010989012
Method name: p_true, running accuracy: 90.65934065934066
Method name: normilized_likelihood, running accuracy: 88.46153846153845
Method name: normilized_entropy, running accuracy: 87.91208791208791
Method name: topk_entropy, running accuracy: 87.91208791208791
Method name: window_entropy, running accuracy: 89.01098901098901

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 181/500 [15:54:18<27:56:23, 315.31s/it, attention_weighted_confidence_acc=89.56%, cer_entropy_weighted_mean_all_acc=89.56%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=90.11%, p_true_acc=90.66%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=87.91%, topk_entropy_acc=87.91%, window_entropy_acc=89.01%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▋      | 182/500 [15:54:18<32:43:09, 370.41s/it, attention_weighted_confidence_acc=89.56%, cer_entropy_weighted_mean_all_acc=89.56%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=90.11%, p_true_acc=90.66%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=87.91%, topk_entropy_acc=87.91%, window_entropy_acc=89.01%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 8.582874308205776
    Answer: 168
    Ground truth:  168
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 8.582874308205776
    Answer: 168
    Ground truth:  168
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 15.981436312198639
    Answer: 168
    Ground truth:  168
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 1.0
    Answer: 168
    Ground truth:  168
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 15.28125
    Answer: 168
    Ground truth:  168
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 2.0170398503541946
    Answer: 168
    Ground truth:  168
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 1.0664971321821213
    Answer: 168
    Ground truth:  168
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 1.0640672445297241
    Answer: 168
    Ground truth:  168
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. The number of spider...
    Score: 3.2437063455581665
    Answer: 168
    Ground truth:  168
Method name: attention_weighted_confidence, running accuracy: 89.61748633879782
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.61748633879782
Method name: cer_prob_product_log_last, running accuracy: 88.52459016393442
Method name: self_consistency, running accuracy: 90.1639344262295
Method name: p_true, running accuracy: 90.7103825136612
Method name: normilized_likelihood, running accuracy: 88.52459016393442
Method name: normilized_entropy, running accuracy: 87.97814207650273
Method name: topk_entropy, running accuracy: 87.97814207650273
Method name: window_entropy, running accuracy: 89.07103825136612

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▋      | 182/500 [15:58:56<32:43:09, 370.41s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=88.52%, self_consistency_acc=90.16%, p_true_acc=90.71%, normilized_likelihood_acc=88.52%, normilized_entropy_acc=87.98%, topk_entropy_acc=87.98%, window_entropy_acc=89.07%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 183/500 [15:58:56<30:09:35, 342.51s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=88.52%, self_consistency_acc=90.16%, p_true_acc=90.71%, normilized_likelihood_acc=88.52%, normilized_entropy_acc=87.98%, topk_entropy_acc=87.98%, window_entropy_acc=89.07%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 5.2384184227691835
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 5.2384184227691835
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 10.625267386436462
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 0.6875
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 8.771484375
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 4.101001054048538
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 4.158921390771866
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 3.654055953025818
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find out how many bookcases Elly needs, we can calculate the total capacity o...
    Score: 10.63279414176941
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 89.67391304347827
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.67391304347827
Method name: cer_prob_product_log_last, running accuracy: 88.58695652173914
Method name: self_consistency, running accuracy: 90.21739130434783
Method name: p_true, running accuracy: 90.76086956521739
Method name: normilized_likelihood, running accuracy: 88.58695652173914
Method name: normilized_entropy, running accuracy: 88.04347826086956
Method name: topk_entropy, running accuracy: 88.04347826086956
Method name: window_entropy, running accuracy: 89.13043478260869

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 183/500 [16:07:16<30:09:35, 342.51s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.59%, self_consistency_acc=90.22%, p_true_acc=90.76%, normilized_likelihood_acc=88.59%, normilized_entropy_acc=88.04%, topk_entropy_acc=88.04%, window_entropy_acc=89.13%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 184/500 [16:07:16<34:12:53, 389.79s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.59%, self_consistency_acc=90.22%, p_true_acc=90.76%, normilized_likelihood_acc=88.59%, normilized_entropy_acc=88.04%, topk_entropy_acc=88.04%, window_entropy_acc=89.13%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 7.718888918092301
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 7.718888918092301
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 14.998468697071075
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 0.9375
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 14.703125
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 6.866902604699135
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 6.340336382389069
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 5.62633104622364
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the number of badges each of them ea...
    Score: 18.904463529586792
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 89.72972972972974
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.72972972972974
Method name: cer_prob_product_log_last, running accuracy: 88.64864864864866
Method name: self_consistency, running accuracy: 90.27027027027027
Method name: p_true, running accuracy: 90.81081081081082
Method name: normilized_likelihood, running accuracy: 88.64864864864866
Method name: normilized_entropy, running accuracy: 88.10810810810811
Method name: topk_entropy, running accuracy: 88.10810810810811
Method name: window_entropy, running accuracy: 89.1891891891892

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 184/500 [16:12:38<34:12:53, 389.79s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=88.65%, self_consistency_acc=90.27%, p_true_acc=90.81%, normilized_likelihood_acc=88.65%, normilized_entropy_acc=88.11%, topk_entropy_acc=88.11%, window_entropy_acc=89.19%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 185/500 [16:12:38<32:20:04, 369.54s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=88.65%, self_consistency_acc=90.27%, p_true_acc=90.81%, normilized_likelihood_acc=88.65%, normilized_entropy_acc=88.11%, topk_entropy_acc=88.11%, window_entropy_acc=89.19%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 8.241563411198848
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 8.241563411198848
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 14.13938895240426
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 1.0
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 14.921875
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 3.469449058175087
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 1.4595514684915543
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 1.3901680707931519
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find the number of oranges left unwashed, we need to determine how many orang...
    Score: 11.30516529083252
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 89.78494623655914
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.78494623655914
Method name: cer_prob_product_log_last, running accuracy: 88.70967741935483
Method name: self_consistency, running accuracy: 90.32258064516128
Method name: p_true, running accuracy: 90.86021505376344
Method name: normilized_likelihood, running accuracy: 88.70967741935483
Method name: normilized_entropy, running accuracy: 88.17204301075269
Method name: topk_entropy, running accuracy: 88.17204301075269
Method name: window_entropy, running accuracy: 89.24731182795699

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 185/500 [16:17:01<32:20:04, 369.54s/it, attention_weighted_confidence_acc=89.78%, cer_entropy_weighted_mean_all_acc=89.78%, cer_prob_product_log_last_acc=88.71%, self_consistency_acc=90.32%, p_true_acc=90.86%, normilized_likelihood_acc=88.71%, normilized_entropy_acc=88.17%, topk_entropy_acc=88.17%, window_entropy_acc=89.25%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 186/500 [16:17:01<29:26:44, 337.59s/it, attention_weighted_confidence_acc=89.78%, cer_entropy_weighted_mean_all_acc=89.78%, cer_prob_product_log_last_acc=88.71%, self_consistency_acc=90.32%, p_true_acc=90.86%, normilized_likelihood_acc=88.71%, normilized_entropy_acc=88.17%, topk_entropy_acc=88.17%, window_entropy_acc=89.25%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 6.9361410443930955
    Answer: 374
    Ground truth:  374
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 6.9361410443930955
    Answer: 374
    Ground truth:  374
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 13.983867347240448
    Answer: 374
    Ground truth:  374
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 0.875
    Answer: 374
    Ground truth:  374
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 11.8046875
    Answer: 374
    Ground truth:  374
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 10.005384787917137
    Answer: 374
    Ground truth:  374
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 9.228538632392883
    Answer: 374
    Ground truth:  374
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 7.72261255979538
    Answer: 374
    Ground truth:  374
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the total distance the bird traveled...
    Score: 16.45490050315857
    Answer: 374
    Ground truth:  374
Method name: attention_weighted_confidence, running accuracy: 89.83957219251337
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.83957219251337
Method name: cer_prob_product_log_last, running accuracy: 88.77005347593582
Method name: self_consistency, running accuracy: 90.37433155080214
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 88.77005347593582
Method name: normilized_entropy, running accuracy: 88.23529411764706
Method name: topk_entropy, running accuracy: 88.23529411764706
Method name: window_entropy, running accuracy: 89.3048128342246

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 186/500 [16:24:37<29:26:44, 337.59s/it, attention_weighted_confidence_acc=89.84%, cer_entropy_weighted_mean_all_acc=89.84%, cer_prob_product_log_last_acc=88.77%, self_consistency_acc=90.37%, p_true_acc=90.91%, normilized_likelihood_acc=88.77%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.30%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 187/500 [16:24:37<32:25:51, 373.01s/it, attention_weighted_confidence_acc=89.84%, cer_entropy_weighted_mean_all_acc=89.84%, cer_prob_product_log_last_acc=88.77%, self_consistency_acc=90.37%, p_true_acc=90.91%, normilized_likelihood_acc=88.77%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.30%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 7.730891066400844
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 7.730891066400844
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 14.999954342842102
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 0.9375
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 14.14453125
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 1.5429288148880005
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 0.7274074405431747
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 0.7181309461593628
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: To find the number of people left on the train, we'll go through the process ste...
    Score: 8.150717616081238
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 89.8936170212766
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.8936170212766
Method name: cer_prob_product_log_last, running accuracy: 88.82978723404256
Method name: self_consistency, running accuracy: 90.42553191489363
Method name: p_true, running accuracy: 90.95744680851064
Method name: normilized_likelihood, running accuracy: 88.82978723404256
Method name: normilized_entropy, running accuracy: 88.29787234042553
Method name: topk_entropy, running accuracy: 88.29787234042553
Method name: window_entropy, running accuracy: 89.36170212765957

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 187/500 [16:27:45<32:25:51, 373.01s/it, attention_weighted_confidence_acc=89.89%, cer_entropy_weighted_mean_all_acc=89.89%, cer_prob_product_log_last_acc=88.83%, self_consistency_acc=90.43%, p_true_acc=90.96%, normilized_likelihood_acc=88.83%, normilized_entropy_acc=88.30%, topk_entropy_acc=88.30%, window_entropy_acc=89.36%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 188/500 [16:27:45<27:30:54, 317.48s/it, attention_weighted_confidence_acc=89.89%, cer_entropy_weighted_mean_all_acc=89.89%, cer_prob_product_log_last_acc=88.83%, self_consistency_acc=90.43%, p_true_acc=90.96%, normilized_likelihood_acc=88.83%, normilized_entropy_acc=88.30%, topk_entropy_acc=88.30%, window_entropy_acc=89.36%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 4.094900249634658
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 4.094900249634658
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 7.974885702133179
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 0.5
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 6.6796875
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 6.636169940233231
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 6.214938715100288
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 5.566785037517548
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

Let Jame's current age...
    Score: 10.475392818450928
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 89.94708994708994
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.94708994708994
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 90.47619047619048
Method name: p_true, running accuracy: 91.005291005291
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.35978835978835
Method name: topk_entropy, running accuracy: 88.35978835978835
Method name: window_entropy, running accuracy: 89.41798941798942

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 188/500 [16:34:34<27:30:54, 317.48s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.48%, p_true_acc=91.01%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.42%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 189/500 [16:34:34<29:48:33, 345.06s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.48%, p_true_acc=91.01%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.42%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 7.319069602025788
    Answer: 90
    Ground truth:  90
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 7.319069602025788
    Answer: 90
    Ground truth:  90
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 15.998755633831024
    Answer: 90
    Ground truth:  90
Method 4: self_consistency
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 1.0
    Answer: 90
    Ground truth:  90
Method 5: p_true
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 11.83984375
    Answer: 90
    Ground truth:  90
Method 6: normilized_likelihood
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 5.546966969966888
    Answer: 90
    Ground truth:  90
Method 7: normilized_entropy
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 2.267276406288147
    Answer: 90
    Ground truth:  90
Method 8: topk_entropy
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 2.2458470165729523
    Answer: 90
    Ground truth:  90
Method 9: window_entropy
  Batch 1:
    Text: First, Johnny added his allowance of $20 and the extra $10 he had, which gives u...
    Score: 3.6349628567695618
    Answer: 90
    Ground truth:  90
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 88.94736842105263
Method name: self_consistency, running accuracy: 90.52631578947368
Method name: p_true, running accuracy: 91.05263157894737
Method name: normilized_likelihood, running accuracy: 88.94736842105263
Method name: normilized_entropy, running accuracy: 88.42105263157895
Method name: topk_entropy, running accuracy: 88.42105263157895
Method name: window_entropy, running accuracy: 89.47368421052632

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 189/500 [16:37:19<29:48:33, 345.06s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.95%, self_consistency_acc=90.53%, p_true_acc=91.05%, normilized_likelihood_acc=88.95%, normilized_entropy_acc=88.42%, topk_entropy_acc=88.42%, window_entropy_acc=89.47%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 190/500 [16:37:19<25:03:17, 290.96s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.95%, self_consistency_acc=90.53%, p_true_acc=91.05%, normilized_likelihood_acc=88.95%, normilized_entropy_acc=88.42%, topk_entropy_acc=88.42%, window_entropy_acc=89.47%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 7.5993224820757
    Answer: 8400
    Ground truth:  8400
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 7.5993224820757
    Answer: 8400
    Ground truth:  8400
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much more money Nick will make at the job with a higher net pay,...
    Score: 0.979505181312561
    Answer: 21
    Ground truth:  8400
Method 4: self_consistency
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 0.9375
    Answer: 8400
    Ground truth:  8400
Method 5: p_true
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 11.12890625
    Answer: 8400
    Ground truth:  8400
Method 6: normilized_likelihood
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 7.882302239537239
    Answer: 8400
    Ground truth:  8400
Method 7: normilized_entropy
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 8.497475937008858
    Answer: 8400
    Ground truth:  8400
Method 8: topk_entropy
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 7.519409313797951
    Answer: 8400
    Ground truth:  8400
Method 9: window_entropy
  Batch 1:
    Text: To find which job has the higher net pay, we need to calculate the take-home pay...
    Score: 19.48578816652298
    Answer: 8400
    Ground truth:  8400
Method name: attention_weighted_confidence, running accuracy: 90.0523560209424
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0523560209424
Method name: cer_prob_product_log_last, running accuracy: 88.48167539267016
Method name: self_consistency, running accuracy: 90.57591623036649
Method name: p_true, running accuracy: 91.09947643979058
Method name: normilized_likelihood, running accuracy: 89.00523560209425
Method name: normilized_entropy, running accuracy: 88.48167539267016
Method name: topk_entropy, running accuracy: 88.48167539267016
Method name: window_entropy, running accuracy: 89.52879581151832

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 190/500 [16:45:21<25:03:17, 290.96s/it, attention_weighted_confidence_acc=90.05%, cer_entropy_weighted_mean_all_acc=90.05%, cer_prob_product_log_last_acc=88.48%, self_consistency_acc=90.58%, p_true_acc=91.10%, normilized_likelihood_acc=89.01%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.53%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 191/500 [16:45:21<29:54:05, 348.37s/it, attention_weighted_confidence_acc=90.05%, cer_entropy_weighted_mean_all_acc=90.05%, cer_prob_product_log_last_acc=88.48%, self_consistency_acc=90.58%, p_true_acc=91.10%, normilized_likelihood_acc=89.01%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.53%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 8.401223501207959
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 8.401223501207959
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 15.999963402748108
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 1.0
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 15.04296875
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 1.1009803786873817
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 0.890556737780571
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 0.8850985616445541
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Let Sam's current age be x.

Since R...
    Score: 2.8382551670074463
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 90.10416666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.10416666666666
Method name: cer_prob_product_log_last, running accuracy: 88.54166666666666
Method name: self_consistency, running accuracy: 90.625
Method name: p_true, running accuracy: 91.14583333333334
Method name: normilized_likelihood, running accuracy: 89.0625
Method name: normilized_entropy, running accuracy: 88.54166666666666
Method name: topk_entropy, running accuracy: 88.54166666666666
Method name: window_entropy, running accuracy: 89.58333333333334

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 191/500 [16:50:49<29:54:05, 348.37s/it, attention_weighted_confidence_acc=90.10%, cer_entropy_weighted_mean_all_acc=90.10%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=90.62%, p_true_acc=91.15%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.54%, topk_entropy_acc=88.54%, window_entropy_acc=89.58%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 192/500 [16:50:49<29:16:55, 342.26s/it, attention_weighted_confidence_acc=90.10%, cer_entropy_weighted_mean_all_acc=90.10%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=90.62%, p_true_acc=91.15%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.54%, topk_entropy_acc=88.54%, window_entropy_acc=89.58%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 7.212993219006441
    Answer: 280
    Ground truth:  280
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 7.212993219006441
    Answer: 280
    Ground truth:  280
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 13.697437822818756
    Answer: 280
    Ground truth:  280
Method 4: self_consistency
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 0.875
    Answer: 280
    Ground truth:  280
Method 5: p_true
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 11.2431640625
    Answer: 280
    Ground truth:  280
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 10.45267903804779
    Answer: 280
    Ground truth:  280
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 10.227365583181381
    Answer: 280
    Ground truth:  280
Method 8: topk_entropy
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 8.842549592256546
    Answer: 280
    Ground truth:  280
Method 9: window_entropy
  Batch 1:
    Text: To find the total length of the baked goods, we need to convert all measurements...
    Score: 21.07259964942932
    Answer: 280
    Ground truth:  280
Method name: attention_weighted_confidence, running accuracy: 90.15544041450777
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.15544041450777
Method name: cer_prob_product_log_last, running accuracy: 88.60103626943005
Method name: self_consistency, running accuracy: 90.67357512953367
Method name: p_true, running accuracy: 91.19170984455958
Method name: normilized_likelihood, running accuracy: 89.11917098445595
Method name: normilized_entropy, running accuracy: 88.60103626943005
Method name: topk_entropy, running accuracy: 88.60103626943005
Method name: window_entropy, running accuracy: 89.63730569948186

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 192/500 [16:57:57<29:16:55, 342.26s/it, attention_weighted_confidence_acc=90.16%, cer_entropy_weighted_mean_all_acc=90.16%, cer_prob_product_log_last_acc=88.60%, self_consistency_acc=90.67%, p_true_acc=91.19%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.60%, topk_entropy_acc=88.60%, window_entropy_acc=89.64%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▊      | 193/500 [16:57:57<31:21:54, 367.80s/it, attention_weighted_confidence_acc=90.16%, cer_entropy_weighted_mean_all_acc=90.16%, cer_prob_product_log_last_acc=88.60%, self_consistency_acc=90.67%, p_true_acc=91.19%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.60%, topk_entropy_acc=88.60%, window_entropy_acc=89.64%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 7.60650486422127
    Answer: 180
    Ground truth:  180
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 7.60650486422127
    Answer: 180
    Ground truth:  180
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 13.999903559684753
    Answer: 180
    Ground truth:  180
Method 4: self_consistency
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 0.875
    Answer: 180
    Ground truth:  180
Method 5: p_true
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 11.70703125
    Answer: 180
    Ground truth:  180
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 1.8660838454961777
    Answer: 180
    Ground truth:  180
Method 7: normilized_entropy
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 1.3365328162908554
    Answer: 180
    Ground truth:  180
Method 8: topk_entropy
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 1.3323862105607986
    Answer: 180
    Ground truth:  180
Method 9: window_entropy
  Batch 1:
    Text: To find the average weight of the three friends, we first need to find their ind...
    Score: 4.849778592586517
    Answer: 180
    Ground truth:  180
Method name: attention_weighted_confidence, running accuracy: 90.20618556701031
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.20618556701031
Method name: cer_prob_product_log_last, running accuracy: 88.65979381443299
Method name: self_consistency, running accuracy: 90.72164948453609
Method name: p_true, running accuracy: 91.23711340206185
Method name: normilized_likelihood, running accuracy: 89.17525773195877
Method name: normilized_entropy, running accuracy: 88.65979381443299
Method name: topk_entropy, running accuracy: 88.65979381443299
Method name: window_entropy, running accuracy: 89.69072164948454

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▊      | 193/500 [17:02:45<31:21:54, 367.80s/it, attention_weighted_confidence_acc=90.21%, cer_entropy_weighted_mean_all_acc=90.21%, cer_prob_product_log_last_acc=88.66%, self_consistency_acc=90.72%, p_true_acc=91.24%, normilized_likelihood_acc=89.18%, normilized_entropy_acc=88.66%, topk_entropy_acc=88.66%, window_entropy_acc=89.69%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 194/500 [17:02:45<29:14:23, 344.00s/it, attention_weighted_confidence_acc=90.21%, cer_entropy_weighted_mean_all_acc=90.21%, cer_prob_product_log_last_acc=88.66%, self_consistency_acc=90.72%, p_true_acc=91.24%, normilized_likelihood_acc=89.18%, normilized_entropy_acc=88.66%, topk_entropy_acc=88.66%, window_entropy_acc=89.69%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 8.7077526417102
    Answer: 40
    Ground truth:  40
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 8.7077526417102
    Answer: 40
    Ground truth:  40
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 15.999833226203918
    Answer: 40
    Ground truth:  40
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 1.0
    Answer: 40
    Ground truth:  40
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 14.6484375
    Answer: 40
    Ground truth:  40
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 3.4823170006275177
    Answer: 40
    Ground truth:  40
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 2.378582939505577
    Answer: 40
    Ground truth:  40
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 2.3731126338243484
    Answer: 40
    Ground truth:  40
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Calculate the numbe...
    Score: 6.971215158700943
    Answer: 40
    Ground truth:  40
Method name: attention_weighted_confidence, running accuracy: 90.25641025641026
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.25641025641026
Method name: cer_prob_product_log_last, running accuracy: 88.71794871794872
Method name: self_consistency, running accuracy: 90.76923076923077
Method name: p_true, running accuracy: 91.28205128205128
Method name: normilized_likelihood, running accuracy: 89.23076923076924
Method name: normilized_entropy, running accuracy: 88.71794871794872
Method name: topk_entropy, running accuracy: 88.71794871794872
Method name: window_entropy, running accuracy: 89.74358974358975

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 194/500 [17:07:52<29:14:23, 344.00s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=90.77%, p_true_acc=91.28%, normilized_likelihood_acc=89.23%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.74%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 195/500 [17:07:52<28:12:38, 332.98s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=90.77%, p_true_acc=91.28%, normilized_likelihood_acc=89.23%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 8.016820782854523
    Answer: 83
    Ground truth:  83
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 8.016820782854523
    Answer: 83
    Ground truth:  83
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 15.998620331287384
    Answer: 83
    Ground truth:  83
Method 4: self_consistency
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 1.0
    Answer: 83
    Ground truth:  83
Method 5: p_true
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 13.01171875
    Answer: 83
    Ground truth:  83
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 3.0397550761699677
    Answer: 83
    Ground truth:  83
Method 7: normilized_entropy
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 3.0420429408550262
    Answer: 83
    Ground truth:  83
Method 8: topk_entropy
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 2.9452492743730545
    Answer: 83
    Ground truth:  83
Method 9: window_entropy
  Batch 1:
    Text: To find the maximum number of boxes that can be loaded onto the truck while not ...
    Score: 9.3226797580719
    Answer: 83
    Ground truth:  83
Method name: attention_weighted_confidence, running accuracy: 90.3061224489796
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.3061224489796
Method name: cer_prob_product_log_last, running accuracy: 88.77551020408163
Method name: self_consistency, running accuracy: 90.81632653061224
Method name: p_true, running accuracy: 91.3265306122449
Method name: normilized_likelihood, running accuracy: 89.28571428571429
Method name: normilized_entropy, running accuracy: 88.77551020408163
Method name: topk_entropy, running accuracy: 88.77551020408163
Method name: window_entropy, running accuracy: 89.79591836734694

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 195/500 [17:12:11<28:12:38, 332.98s/it, attention_weighted_confidence_acc=90.31%, cer_entropy_weighted_mean_all_acc=90.31%, cer_prob_product_log_last_acc=88.78%, self_consistency_acc=90.82%, p_true_acc=91.33%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=88.78%, topk_entropy_acc=88.78%, window_entropy_acc=89.80%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 196/500 [17:12:11<26:14:20, 310.73s/it, attention_weighted_confidence_acc=90.31%, cer_entropy_weighted_mean_all_acc=90.31%, cer_prob_product_log_last_acc=88.78%, self_consistency_acc=90.82%, p_true_acc=91.33%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=88.78%, topk_entropy_acc=88.78%, window_entropy_acc=89.80%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 3.326202130780774
    Answer: 40
    Ground truth:  36
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 3.326202130780774
    Answer: 40
    Ground truth:  36
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 6.935487687587738
    Answer: 40
    Ground truth:  36
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 0.4375
    Answer: 40
    Ground truth:  36
Method 5: p_true
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 5.134765625
    Answer: 40
    Ground truth:  36
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 2.368825227022171
    Answer: 40
    Ground truth:  36
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 2.62646946310997
    Answer: 40
    Ground truth:  36
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 2.3488307893276215
    Answer: 40
    Ground truth:  36
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of hours Josh spent working out across the 8 weeks, let...
    Score: 5.405698120594025
    Answer: 40
    Ground truth:  36
Method name: attention_weighted_confidence, running accuracy: 89.84771573604061
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.84771573604061
Method name: cer_prob_product_log_last, running accuracy: 88.3248730964467
Method name: self_consistency, running accuracy: 90.35532994923858
Method name: p_true, running accuracy: 90.86294416243655
Method name: normilized_likelihood, running accuracy: 88.83248730964468
Method name: normilized_entropy, running accuracy: 88.3248730964467
Method name: topk_entropy, running accuracy: 88.3248730964467
Method name: window_entropy, running accuracy: 89.34010152284264

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 196/500 [17:17:31<26:14:20, 310.73s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=88.32%, self_consistency_acc=90.36%, p_true_acc=90.86%, normilized_likelihood_acc=88.83%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.34%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 197/500 [17:17:31<26:23:19, 313.53s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=88.32%, self_consistency_acc=90.36%, p_true_acc=90.86%, normilized_likelihood_acc=88.83%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.34%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 4.002933299995797
    Answer: 150
    Ground truth:  150
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 4.002933299995797
    Answer: 150
    Ground truth:  150
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 7.998762667179108
    Answer: 150
    Ground truth:  150
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 0.5
    Answer: 150
    Ground truth:  150
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 7.4296875
    Answer: 150
    Ground truth:  150
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 6.2589662075042725
    Answer: 150
    Ground truth:  150
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 5.984882652759552
    Answer: 150
    Ground truth:  150
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 5.160399377346039
    Answer: 150
    Ground truth:  150
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Lillian builds 3 bird feeders and...
    Score: 12.097019970417023
    Answer: 150
    Ground truth:  150
Method name: attention_weighted_confidence, running accuracy: 89.8989898989899
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.8989898989899
Method name: cer_prob_product_log_last, running accuracy: 88.38383838383838
Method name: self_consistency, running accuracy: 90.40404040404042
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.38383838383838
Method name: topk_entropy, running accuracy: 88.38383838383838
Method name: window_entropy, running accuracy: 89.39393939393939

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 197/500 [17:27:20<26:23:19, 313.53s/it, attention_weighted_confidence_acc=89.90%, cer_entropy_weighted_mean_all_acc=89.90%, cer_prob_product_log_last_acc=88.38%, self_consistency_acc=90.40%, p_true_acc=90.91%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.38%, topk_entropy_acc=88.38%, window_entropy_acc=89.39%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|███▉      | 198/500 [17:27:20<33:13:48, 396.12s/it, attention_weighted_confidence_acc=89.90%, cer_entropy_weighted_mean_all_acc=89.90%, cer_prob_product_log_last_acc=88.38%, self_consistency_acc=90.40%, p_true_acc=90.91%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.38%, topk_entropy_acc=88.38%, window_entropy_acc=89.39%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 4.23753752404618
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 4.23753752404618
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 8.772547841072083
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 0.5625
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 8.0546875
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 9.566297501325607
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 8.470088541507721
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 7.2622517347335815
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find the amount of money left with Ronnie, let's break down the problem step ...
    Score: 12.351930439472198
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 89.9497487437186
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.9497487437186
Method name: cer_prob_product_log_last, running accuracy: 88.44221105527639
Method name: self_consistency, running accuracy: 90.45226130653266
Method name: p_true, running accuracy: 90.95477386934674
Method name: normilized_likelihood, running accuracy: 88.94472361809045
Method name: normilized_entropy, running accuracy: 88.44221105527639
Method name: topk_entropy, running accuracy: 88.44221105527639
Method name: window_entropy, running accuracy: 89.44723618090453

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|███▉      | 198/500 [17:35:08<33:13:48, 396.12s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.44%, self_consistency_acc=90.45%, p_true_acc=90.95%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.44%, topk_entropy_acc=88.44%, window_entropy_acc=89.45%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|███▉      | 199/500 [17:35:08<34:55:47, 417.77s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.44%, self_consistency_acc=90.45%, p_true_acc=90.95%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.44%, topk_entropy_acc=88.44%, window_entropy_acc=89.45%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 7.840834522483316
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 7.840834522483316
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 15.530738651752472
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 15.796875
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 1.5713281333446503
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 0.7450070381164551
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 0.7437211275100708
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find out how many cookies were in each jar, we first need to determine how ma...
    Score: 3.156757354736328
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 88.5
Method name: self_consistency, running accuracy: 90.5
Method name: p_true, running accuracy: 91.0
Method name: normilized_likelihood, running accuracy: 89.0
Method name: normilized_entropy, running accuracy: 88.5
Method name: topk_entropy, running accuracy: 88.5
Method name: window_entropy, running accuracy: 89.5

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|███▉      | 199/500 [17:37:58<34:55:47, 417.77s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.50%, self_consistency_acc=90.50%, p_true_acc=91.00%, normilized_likelihood_acc=89.00%, normilized_entropy_acc=88.50%, topk_entropy_acc=88.50%, window_entropy_acc=89.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 200/500 [17:37:58<28:36:14, 343.25s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.50%, self_consistency_acc=90.50%, p_true_acc=91.00%, normilized_likelihood_acc=89.00%, normilized_entropy_acc=88.50%, topk_entropy_acc=88.50%, window_entropy_acc=89.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 2.3309431949814745
    Answer: 45
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 2.3309431949814745
    Answer: 45
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 4.999593377113342
    Answer: 45
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 0.3125
    Answer: 45
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 4.2109375
    Answer: 45
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 3.467225044965744
    Answer: 45
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 3.40462264418602
    Answer: 45
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 2.9329183995723724
    Answer: 45
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money James lost, we need to calculate the total cost of th...
    Score: 5.759864270687103
    Answer: 45
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 89.55223880597015
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.55223880597015
Method name: cer_prob_product_log_last, running accuracy: 88.05970149253731
Method name: self_consistency, running accuracy: 90.04975124378109
Method name: p_true, running accuracy: 90.54726368159204
Method name: normilized_likelihood, running accuracy: 88.55721393034825
Method name: normilized_entropy, running accuracy: 88.05970149253731
Method name: topk_entropy, running accuracy: 88.05970149253731
Method name: window_entropy, running accuracy: 89.05472636815921

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 200/500 [17:46:36<28:36:14, 343.25s/it, attention_weighted_confidence_acc=89.55%, cer_entropy_weighted_mean_all_acc=89.55%, cer_prob_product_log_last_acc=88.06%, self_consistency_acc=90.05%, p_true_acc=90.55%, normilized_likelihood_acc=88.56%, normilized_entropy_acc=88.06%, topk_entropy_acc=88.06%, window_entropy_acc=89.05%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 201/500 [17:46:36<32:52:28, 395.81s/it, attention_weighted_confidence_acc=89.55%, cer_entropy_weighted_mean_all_acc=89.55%, cer_prob_product_log_last_acc=88.06%, self_consistency_acc=90.05%, p_true_acc=90.55%, normilized_likelihood_acc=88.56%, normilized_entropy_acc=88.06%, topk_entropy_acc=88.06%, window_entropy_acc=89.05%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 7.174713776830058
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 7.174713776830058
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 13.999524235725403
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 0.875
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 12.41796875
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 1.9994225651025772
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 1.3887383937835693
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 1.3906441926956177
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of windows, let's break it down step by step:

1. Each ...
    Score: 2.316906154155731
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 89.60396039603961
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.60396039603961
Method name: cer_prob_product_log_last, running accuracy: 88.11881188118812
Method name: self_consistency, running accuracy: 90.0990099009901
Method name: p_true, running accuracy: 90.5940594059406
Method name: normilized_likelihood, running accuracy: 88.61386138613861
Method name: normilized_entropy, running accuracy: 88.11881188118812
Method name: topk_entropy, running accuracy: 88.11881188118812
Method name: window_entropy, running accuracy: 89.10891089108911

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 201/500 [17:50:57<32:52:28, 395.81s/it, attention_weighted_confidence_acc=89.60%, cer_entropy_weighted_mean_all_acc=89.60%, cer_prob_product_log_last_acc=88.12%, self_consistency_acc=90.10%, p_true_acc=90.59%, normilized_likelihood_acc=88.61%, normilized_entropy_acc=88.12%, topk_entropy_acc=88.12%, window_entropy_acc=89.11%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 202/500 [17:50:57<29:25:23, 355.45s/it, attention_weighted_confidence_acc=89.60%, cer_entropy_weighted_mean_all_acc=89.60%, cer_prob_product_log_last_acc=88.12%, self_consistency_acc=90.10%, p_true_acc=90.59%, normilized_likelihood_acc=88.61%, normilized_entropy_acc=88.12%, topk_entropy_acc=88.12%, window_entropy_acc=89.11%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 6.945289977919173
    Answer: 34
    Ground truth:  34
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 6.945289977919173
    Answer: 34
    Ground truth:  34
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 13.991106033325195
    Answer: 34
    Ground truth:  34
Method 4: self_consistency
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 0.875
    Answer: 34
    Ground truth:  34
Method 5: p_true
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 10.109375
    Answer: 34
    Ground truth:  34
Method 6: normilized_likelihood
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 7.123586028814316
    Answer: 34
    Ground truth:  34
Method 7: normilized_entropy
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 5.729295581579208
    Answer: 34
    Ground truth:  34
Method 8: topk_entropy
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 5.0822848081588745
    Answer: 34
    Ground truth:  34
Method 9: window_entropy
  Batch 1:
    Text: First, we need to find out how many measuring spoons Jonathan has initially. He ...
    Score: 19.335443079471588
    Answer: 34
    Ground truth:  34
Method name: attention_weighted_confidence, running accuracy: 89.65517241379311
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.65517241379311
Method name: cer_prob_product_log_last, running accuracy: 88.17733990147784
Method name: self_consistency, running accuracy: 90.14778325123153
Method name: p_true, running accuracy: 90.64039408866995
Method name: normilized_likelihood, running accuracy: 88.66995073891626
Method name: normilized_entropy, running accuracy: 88.17733990147784
Method name: topk_entropy, running accuracy: 88.17733990147784
Method name: window_entropy, running accuracy: 89.16256157635468

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 202/500 [17:56:25<29:25:23, 355.45s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=90.15%, p_true_acc=90.64%, normilized_likelihood_acc=88.67%, normilized_entropy_acc=88.18%, topk_entropy_acc=88.18%, window_entropy_acc=89.16%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 203/500 [17:56:25<28:38:16, 347.13s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=90.15%, p_true_acc=90.64%, normilized_likelihood_acc=88.67%, normilized_entropy_acc=88.18%, topk_entropy_acc=88.18%, window_entropy_acc=89.16%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 8.342484065906703
    Answer: 312
    Ground truth:  312
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 8.342484065906703
    Answer: 312
    Ground truth:  312
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 15.997157275676727
    Answer: 312
    Ground truth:  312
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 1.0
    Answer: 312
    Ground truth:  312
Method 5: p_true
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 14.15625
    Answer: 312
    Ground truth:  312
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 2.01933054625988
    Answer: 312
    Ground truth:  312
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 1.225616306066513
    Answer: 312
    Ground truth:  312
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 1.2151914089918137
    Answer: 312
    Ground truth:  312
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Gary spends on water for laundry in a year, we need to calc...
    Score: 4.354910165071487
    Answer: 312
    Ground truth:  312
Method name: attention_weighted_confidence, running accuracy: 89.70588235294117
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.70588235294117
Method name: cer_prob_product_log_last, running accuracy: 88.23529411764706
Method name: self_consistency, running accuracy: 90.19607843137256
Method name: p_true, running accuracy: 90.68627450980392
Method name: normilized_likelihood, running accuracy: 88.72549019607843
Method name: normilized_entropy, running accuracy: 88.23529411764706
Method name: topk_entropy, running accuracy: 88.23529411764706
Method name: window_entropy, running accuracy: 89.2156862745098

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 203/500 [18:01:12<28:38:16, 347.13s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=90.20%, p_true_acc=90.69%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.22%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 204/500 [18:01:12<27:03:18, 329.05s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=90.20%, p_true_acc=90.69%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.22%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 5.826118449657091
    Answer: 200
    Ground truth:  200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 5.826118449657091
    Answer: 200
    Ground truth:  200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 11.997057914733887
    Answer: 200
    Ground truth:  200
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 0.75
    Answer: 200
    Ground truth:  200
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 10.625
    Answer: 200
    Ground truth:  200
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 3.006797343492508
    Answer: 200
    Ground truth:  200
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 2.599678486585617
    Answer: 200
    Ground truth:  200
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 2.5122242271900177
    Answer: 200
    Ground truth:  200
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Determine the total...
    Score: 6.147467911243439
    Answer: 200
    Ground truth:  200
Method name: attention_weighted_confidence, running accuracy: 89.75609756097562
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.75609756097562
Method name: cer_prob_product_log_last, running accuracy: 88.29268292682927
Method name: self_consistency, running accuracy: 90.2439024390244
Method name: p_true, running accuracy: 90.73170731707317
Method name: normilized_likelihood, running accuracy: 88.78048780487805
Method name: normilized_entropy, running accuracy: 88.29268292682927
Method name: topk_entropy, running accuracy: 88.29268292682927
Method name: window_entropy, running accuracy: 89.26829268292683

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 204/500 [18:07:00<27:03:18, 329.05s/it, attention_weighted_confidence_acc=89.76%, cer_entropy_weighted_mean_all_acc=89.76%, cer_prob_product_log_last_acc=88.29%, self_consistency_acc=90.24%, p_true_acc=90.73%, normilized_likelihood_acc=88.78%, normilized_entropy_acc=88.29%, topk_entropy_acc=88.29%, window_entropy_acc=89.27%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 205/500 [18:07:00<27:25:58, 334.78s/it, attention_weighted_confidence_acc=89.76%, cer_entropy_weighted_mean_all_acc=89.76%, cer_prob_product_log_last_acc=88.29%, self_consistency_acc=90.24%, p_true_acc=90.73%, normilized_likelihood_acc=88.78%, normilized_entropy_acc=88.29%, topk_entropy_acc=88.29%, window_entropy_acc=89.27%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 8.382837412399816
    Answer: 144
    Ground truth:  144
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 8.382837412399816
    Answer: 144
    Ground truth:  144
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 15.988142371177673
    Answer: 144
    Ground truth:  144
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 1.0
    Answer: 144
    Ground truth:  144
Method 5: p_true
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 13.83984375
    Answer: 144
    Ground truth:  144
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 3.617895483970642
    Answer: 144
    Ground truth:  144
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 3.7285947799682617
    Answer: 144
    Ground truth:  144
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 3.462748169898987
    Answer: 144
    Ground truth:  144
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost to paint the room, we first need to find the total square...
    Score: 15.454277098178864
    Answer: 144
    Ground truth:  144
Method name: attention_weighted_confidence, running accuracy: 89.80582524271846
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.80582524271846
Method name: cer_prob_product_log_last, running accuracy: 88.3495145631068
Method name: self_consistency, running accuracy: 90.29126213592234
Method name: p_true, running accuracy: 90.77669902912622
Method name: normilized_likelihood, running accuracy: 88.83495145631069
Method name: normilized_entropy, running accuracy: 88.3495145631068
Method name: topk_entropy, running accuracy: 88.3495145631068
Method name: window_entropy, running accuracy: 89.32038834951457

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 205/500 [18:13:22<27:25:58, 334.78s/it, attention_weighted_confidence_acc=89.81%, cer_entropy_weighted_mean_all_acc=89.81%, cer_prob_product_log_last_acc=88.35%, self_consistency_acc=90.29%, p_true_acc=90.78%, normilized_likelihood_acc=88.83%, normilized_entropy_acc=88.35%, topk_entropy_acc=88.35%, window_entropy_acc=89.32%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 206/500 [18:13:22<28:29:32, 348.89s/it, attention_weighted_confidence_acc=89.81%, cer_entropy_weighted_mean_all_acc=89.81%, cer_prob_product_log_last_acc=88.35%, self_consistency_acc=90.29%, p_true_acc=90.78%, normilized_likelihood_acc=88.83%, normilized_entropy_acc=88.35%, topk_entropy_acc=88.35%, window_entropy_acc=89.32%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 8.391913583193814
    Answer: 45
    Ground truth:  45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 8.391913583193814
    Answer: 45
    Ground truth:  45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 15.999943614006042
    Answer: 45
    Ground truth:  45
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 1.0
    Answer: 45
    Ground truth:  45
Method 5: p_true
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 13.06640625
    Answer: 45
    Ground truth:  45
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 0.9621427357196808
    Answer: 45
    Ground truth:  45
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 0.5759365856647491
    Answer: 45
    Ground truth:  45
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 0.5775279402732849
    Answer: 45
    Ground truth:  45
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Annabelle still needs to save, we need to calculate h...
    Score: 3.341657340526581
    Answer: 45
    Ground truth:  45
Method name: attention_weighted_confidence, running accuracy: 89.85507246376811
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.85507246376811
Method name: cer_prob_product_log_last, running accuracy: 88.40579710144928
Method name: self_consistency, running accuracy: 90.33816425120773
Method name: p_true, running accuracy: 90.82125603864735
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.40579710144928
Method name: topk_entropy, running accuracy: 88.40579710144928
Method name: window_entropy, running accuracy: 89.3719806763285

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 206/500 [18:18:53<28:29:32, 348.89s/it, attention_weighted_confidence_acc=89.86%, cer_entropy_weighted_mean_all_acc=89.86%, cer_prob_product_log_last_acc=88.41%, self_consistency_acc=90.34%, p_true_acc=90.82%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.37%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████▏     | 207/500 [18:18:53<27:58:13, 343.66s/it, attention_weighted_confidence_acc=89.86%, cer_entropy_weighted_mean_all_acc=89.86%, cer_prob_product_log_last_acc=88.41%, self_consistency_acc=90.34%, p_true_acc=90.82%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.37%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 7.712646362137588
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 7.712646362137588
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 15.999515116214752
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 1.0
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 12.71875
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 6.461591675877571
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 6.569643467664719
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 5.802297383546829
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To find out how many socks Lindsay missed, we first need to calculate how many s...
    Score: 15.360645055770874
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 89.90384615384616
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.90384615384616
Method name: cer_prob_product_log_last, running accuracy: 88.46153846153845
Method name: self_consistency, running accuracy: 90.38461538461539
Method name: p_true, running accuracy: 90.86538461538461
Method name: normilized_likelihood, running accuracy: 88.9423076923077
Method name: normilized_entropy, running accuracy: 88.46153846153845
Method name: topk_entropy, running accuracy: 88.46153846153845
Method name: window_entropy, running accuracy: 89.42307692307693

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████▏     | 207/500 [18:22:46<27:58:13, 343.66s/it, attention_weighted_confidence_acc=89.90%, cer_entropy_weighted_mean_all_acc=89.90%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=90.38%, p_true_acc=90.87%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.42%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 208/500 [18:22:46<25:10:58, 310.48s/it, attention_weighted_confidence_acc=89.90%, cer_entropy_weighted_mean_all_acc=89.90%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=90.38%, p_true_acc=90.87%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.42%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 8.205965536327751
    Answer: 600
    Ground truth:  600
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 8.205965536327751
    Answer: 600
    Ground truth:  600
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 15.999895930290222
    Answer: 600
    Ground truth:  600
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 1.0
    Answer: 600
    Ground truth:  600
Method 5: p_true
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 13.55859375
    Answer: 600
    Ground truth:  600
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 2.0904850512742996
    Answer: 600
    Ground truth:  600
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 0.9991282671689987
    Answer: 600
    Ground truth:  600
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 0.9986916482448578
    Answer: 600
    Ground truth:  600
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of thorns, we need to follow the steps below:

1. First...
    Score: 2.059227019548416
    Answer: 600
    Ground truth:  600
Method name: attention_weighted_confidence, running accuracy: 89.95215311004785
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.95215311004785
Method name: cer_prob_product_log_last, running accuracy: 88.51674641148325
Method name: self_consistency, running accuracy: 90.43062200956938
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 88.99521531100478
Method name: normilized_entropy, running accuracy: 88.51674641148325
Method name: topk_entropy, running accuracy: 88.51674641148325
Method name: window_entropy, running accuracy: 89.47368421052632

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 208/500 [18:25:53<25:10:58, 310.48s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.52%, self_consistency_acc=90.43%, p_true_acc=90.91%, normilized_likelihood_acc=89.00%, normilized_entropy_acc=88.52%, topk_entropy_acc=88.52%, window_entropy_acc=89.47%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 209/500 [18:25:53<22:05:33, 273.31s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.52%, self_consistency_acc=90.43%, p_true_acc=90.91%, normilized_likelihood_acc=89.00%, normilized_entropy_acc=88.52%, topk_entropy_acc=88.52%, window_entropy_acc=89.47%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 7.946038485837904
    Answer: 105
    Ground truth:  105
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 7.946038485837904
    Answer: 105
    Ground truth:  105
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 15.418318510055542
    Answer: 105
    Ground truth:  105
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 1.0
    Answer: 105
    Ground truth:  105
Method 5: p_true
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 14.91015625
    Answer: 105
    Ground truth:  105
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 2.3478138148784637
    Answer: 105
    Ground truth:  105
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 1.1288845092058182
    Answer: 105
    Ground truth:  105
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 1.1264726519584656
    Answer: 105
    Ground truth:  105
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount of money James has, we need to convert the coins to cen...
    Score: 4.505447685718536
    Answer: 105
    Ground truth:  105
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 88.57142857142857
Method name: self_consistency, running accuracy: 90.47619047619048
Method name: p_true, running accuracy: 90.95238095238095
Method name: normilized_likelihood, running accuracy: 89.04761904761904
Method name: normilized_entropy, running accuracy: 88.57142857142857
Method name: topk_entropy, running accuracy: 88.57142857142857
Method name: window_entropy, running accuracy: 89.52380952380953

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 209/500 [18:28:52<22:05:33, 273.31s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.57%, self_consistency_acc=90.48%, p_true_acc=90.95%, normilized_likelihood_acc=89.05%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=89.52%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 210/500 [18:28:52<19:44:24, 245.05s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.57%, self_consistency_acc=90.48%, p_true_acc=90.95%, normilized_likelihood_acc=89.05%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=89.52%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 1.502111045836265
    Answer: 7.5
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 1.502111045836265
    Answer: 7.5
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many bottles of water John drinks, we need to determine the tota...
    Score: 2.998833656311035
    Answer: 8
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 0.1875
    Answer: 7.5
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 2.625
    Answer: 7.5
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 4.122166782617569
    Answer: 7.5
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 3.881808042526245
    Answer: 7.5
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 3.2713300585746765
    Answer: 7.5
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: To find out how many bottles John drinks in total, we need to determine the time...
    Score: 5.194162666797638
    Answer: 7.5
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 89.57345971563981
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.57345971563981
Method name: cer_prob_product_log_last, running accuracy: 88.15165876777252
Method name: self_consistency, running accuracy: 90.04739336492891
Method name: p_true, running accuracy: 90.52132701421802
Method name: normilized_likelihood, running accuracy: 88.62559241706161
Method name: normilized_entropy, running accuracy: 88.15165876777252
Method name: topk_entropy, running accuracy: 88.15165876777252
Method name: window_entropy, running accuracy: 89.0995260663507

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 210/500 [18:36:02<19:44:24, 245.05s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.15%, self_consistency_acc=90.05%, p_true_acc=90.52%, normilized_likelihood_acc=88.63%, normilized_entropy_acc=88.15%, topk_entropy_acc=88.15%, window_entropy_acc=89.10%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 211/500 [18:36:02<24:07:38, 300.55s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.15%, self_consistency_acc=90.05%, p_true_acc=90.52%, normilized_likelihood_acc=88.63%, normilized_entropy_acc=88.15%, topk_entropy_acc=88.15%, window_entropy_acc=89.10%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 3.9618893106653545
    Answer: 70000
    Ground truth:  70000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 3.9618893106653545
    Answer: 70000
    Ground truth:  70000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the increase in house value, we need to calculate 150% of the original $...
    Score: 0.0
    Answer: 25000
    Ground truth:  70000
Method 4: self_consistency
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 0.5
    Answer: 70000
    Ground truth:  70000
Method 5: p_true
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 6.265625
    Answer: 70000
    Ground truth:  70000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 9.537874191999435
    Answer: 70000
    Ground truth:  70000
Method 7: normilized_entropy
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 9.649473279714584
    Answer: 70000
    Ground truth:  70000
Method 8: topk_entropy
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 8.093409776687622
    Answer: 70000
    Ground truth:  70000
Method 9: window_entropy
  Batch 1:
    Text: To find Josh's profit, we need to calculate the final value of the house after r...
    Score: 11.008545875549316
    Answer: 70000
    Ground truth:  70000
Method name: attention_weighted_confidence, running accuracy: 89.62264150943396
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.62264150943396
Method name: cer_prob_product_log_last, running accuracy: 87.73584905660378
Method name: self_consistency, running accuracy: 90.09433962264151
Method name: p_true, running accuracy: 90.56603773584906
Method name: normilized_likelihood, running accuracy: 88.67924528301887
Method name: normilized_entropy, running accuracy: 88.20754716981132
Method name: topk_entropy, running accuracy: 88.20754716981132
Method name: window_entropy, running accuracy: 89.15094339622641

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 211/500 [18:43:16<24:07:38, 300.55s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=87.74%, self_consistency_acc=90.09%, p_true_acc=90.57%, normilized_likelihood_acc=88.68%, normilized_entropy_acc=88.21%, topk_entropy_acc=88.21%, window_entropy_acc=89.15%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 212/500 [18:43:16<27:15:07, 340.65s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=87.74%, self_consistency_acc=90.09%, p_true_acc=90.57%, normilized_likelihood_acc=88.68%, normilized_entropy_acc=88.21%, topk_entropy_acc=88.21%, window_entropy_acc=89.15%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 3.671189799417014
    Answer: 200
    Ground truth:  200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 3.671189799417014
    Answer: 200
    Ground truth:  200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 7.284635096788406
    Answer: 200
    Ground truth:  200
Method 4: self_consistency
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 0.5
    Answer: 200
    Ground truth:  200
Method 5: p_true
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 6.20703125
    Answer: 200
    Ground truth:  200
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 6.016910672187805
    Answer: 200
    Ground truth:  200
Method 7: normilized_entropy
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 5.872034996747971
    Answer: 200
    Ground truth:  200
Method 8: topk_entropy
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 5.147714763879776
    Answer: 200
    Ground truth:  200
Method 9: window_entropy
  Batch 1:
    Text: To find the distance outside of the dragon's flames where Polly can still hit th...
    Score: 10.656287908554077
    Answer: 200
    Ground truth:  200
Method name: attention_weighted_confidence, running accuracy: 89.67136150234741
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.67136150234741
Method name: cer_prob_product_log_last, running accuracy: 87.79342723004694
Method name: self_consistency, running accuracy: 90.14084507042254
Method name: p_true, running accuracy: 90.61032863849765
Method name: normilized_likelihood, running accuracy: 88.73239436619718
Method name: normilized_entropy, running accuracy: 88.26291079812206
Method name: topk_entropy, running accuracy: 88.26291079812206
Method name: window_entropy, running accuracy: 89.2018779342723

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 212/500 [18:49:51<27:15:07, 340.65s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=87.79%, self_consistency_acc=90.14%, p_true_acc=90.61%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.26%, topk_entropy_acc=88.26%, window_entropy_acc=89.20%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 213/500 [18:49:51<28:27:27, 356.96s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=87.79%, self_consistency_acc=90.14%, p_true_acc=90.61%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.26%, topk_entropy_acc=88.26%, window_entropy_acc=89.20%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 8.666300652735309
    Answer: 45
    Ground truth:  45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 8.666300652735309
    Answer: 45
    Ground truth:  45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 15.999741792678833
    Answer: 45
    Ground truth:  45
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 1.0
    Answer: 45
    Ground truth:  45
Method 5: p_true
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 14.734375
    Answer: 45
    Ground truth:  45
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 0.9666668698191643
    Answer: 45
    Ground truth:  45
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 0.489443764090538
    Answer: 45
    Ground truth:  45
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 0.48848526924848557
    Answer: 45
    Ground truth:  45
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of rodents in the pet shop, we need to add the number o...
    Score: 4.904383689165115
    Answer: 45
    Ground truth:  45
Method name: attention_weighted_confidence, running accuracy: 89.7196261682243
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.7196261682243
Method name: cer_prob_product_log_last, running accuracy: 87.85046728971963
Method name: self_consistency, running accuracy: 90.18691588785047
Method name: p_true, running accuracy: 90.65420560747664
Method name: normilized_likelihood, running accuracy: 88.78504672897196
Method name: normilized_entropy, running accuracy: 88.3177570093458
Method name: topk_entropy, running accuracy: 88.3177570093458
Method name: window_entropy, running accuracy: 89.25233644859813

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 213/500 [18:53:53<28:27:27, 356.96s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=87.85%, self_consistency_acc=90.19%, p_true_acc=90.65%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.25%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 214/500 [18:53:53<25:36:17, 322.30s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=87.85%, self_consistency_acc=90.19%, p_true_acc=90.65%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.25%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 7.640761515664473
    Answer: 22
    Ground truth:  22
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 7.640761515664473
    Answer: 22
    Ground truth:  22
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 14.986184298992157
    Answer: 22
    Ground truth:  22
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 0.9375
    Answer: 22
    Ground truth:  22
Method 5: p_true
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 13.33203125
    Answer: 22
    Ground truth:  22
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 15.842135205864906
    Answer: 22
    Ground truth:  22
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 15.31644544005394
    Answer: 22
    Ground truth:  22
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 13.626363918185234
    Answer: 22
    Ground truth:  22
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of times the alarm rang, we need to determine how many ...
    Score: 22.3500414788723
    Answer: 22
    Ground truth:  22
Method name: attention_weighted_confidence, running accuracy: 89.76744186046511
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.76744186046511
Method name: cer_prob_product_log_last, running accuracy: 87.90697674418605
Method name: self_consistency, running accuracy: 90.23255813953487
Method name: p_true, running accuracy: 90.69767441860465
Method name: normilized_likelihood, running accuracy: 88.83720930232558
Method name: normilized_entropy, running accuracy: 88.37209302325581
Method name: topk_entropy, running accuracy: 88.37209302325581
Method name: window_entropy, running accuracy: 89.30232558139535

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 214/500 [18:58:19<25:36:17, 322.30s/it, attention_weighted_confidence_acc=89.77%, cer_entropy_weighted_mean_all_acc=89.77%, cer_prob_product_log_last_acc=87.91%, self_consistency_acc=90.23%, p_true_acc=90.70%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.37%, topk_entropy_acc=88.37%, window_entropy_acc=89.30%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 215/500 [18:58:19<24:10:14, 305.31s/it, attention_weighted_confidence_acc=89.77%, cer_entropy_weighted_mean_all_acc=89.77%, cer_prob_product_log_last_acc=87.91%, self_consistency_acc=90.23%, p_true_acc=90.70%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.37%, topk_entropy_acc=88.37%, window_entropy_acc=89.30%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 8.12754010431808
    Answer: 132
    Ground truth:  132
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 8.12754010431808
    Answer: 132
    Ground truth:  132
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 15.62449860572815
    Answer: 132
    Ground truth:  132
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 1.0
    Answer: 132
    Ground truth:  132
Method 5: p_true
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 14.54296875
    Answer: 132
    Ground truth:  132
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 1.835159108042717
    Answer: 132
    Ground truth:  132
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 1.3819715082645416
    Answer: 132
    Ground truth:  132
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 1.3712271749973297
    Answer: 132
    Ground truth:  132
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Ella would earn after 12 hours of walking dogs at the same ...
    Score: 2.9963423013687134
    Answer: 132
    Ground truth:  132
Method name: attention_weighted_confidence, running accuracy: 89.81481481481481
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.81481481481481
Method name: cer_prob_product_log_last, running accuracy: 87.96296296296296
Method name: self_consistency, running accuracy: 90.27777777777779
Method name: p_true, running accuracy: 90.74074074074075
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.42592592592592
Method name: topk_entropy, running accuracy: 88.42592592592592
Method name: window_entropy, running accuracy: 89.35185185185185

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 215/500 [19:01:55<24:10:14, 305.31s/it, attention_weighted_confidence_acc=89.81%, cer_entropy_weighted_mean_all_acc=89.81%, cer_prob_product_log_last_acc=87.96%, self_consistency_acc=90.28%, p_true_acc=90.74%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.43%, topk_entropy_acc=88.43%, window_entropy_acc=89.35%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 216/500 [19:01:55<21:59:12, 278.71s/it, attention_weighted_confidence_acc=89.81%, cer_entropy_weighted_mean_all_acc=89.81%, cer_prob_product_log_last_acc=87.96%, self_consistency_acc=90.28%, p_true_acc=90.74%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.43%, topk_entropy_acc=88.43%, window_entropy_acc=89.35%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 8.130833051046597
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 8.130833051046597
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 15.996931850910187
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 1.0
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 14.5078125
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 3.558911547064781
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 3.0072005838155746
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 2.903013586997986
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find out how many trucks will be needed, we first need to calculate the total...
    Score: 10.00209891796112
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 89.86175115207374
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.86175115207374
Method name: cer_prob_product_log_last, running accuracy: 88.0184331797235
Method name: self_consistency, running accuracy: 90.32258064516128
Method name: p_true, running accuracy: 90.78341013824884
Method name: normilized_likelihood, running accuracy: 88.94009216589862
Method name: normilized_entropy, running accuracy: 88.47926267281106
Method name: topk_entropy, running accuracy: 88.47926267281106
Method name: window_entropy, running accuracy: 89.40092165898618

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 216/500 [19:05:27<21:59:12, 278.71s/it, attention_weighted_confidence_acc=89.86%, cer_entropy_weighted_mean_all_acc=89.86%, cer_prob_product_log_last_acc=88.02%, self_consistency_acc=90.32%, p_true_acc=90.78%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.40%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 217/500 [19:05:27<20:19:29, 258.55s/it, attention_weighted_confidence_acc=89.86%, cer_entropy_weighted_mean_all_acc=89.86%, cer_prob_product_log_last_acc=88.02%, self_consistency_acc=90.32%, p_true_acc=90.78%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.40%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 8.091256774050002
    Answer: 23
    Ground truth:  23
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 8.091256774050002
    Answer: 23
    Ground truth:  23
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 15.996587812900543
    Answer: 23
    Ground truth:  23
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 1.0
    Answer: 23
    Ground truth:  23
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 15.625
    Answer: 23
    Ground truth:  23
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 1.8160746097564697
    Answer: 23
    Ground truth:  23
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 0.4058845639228821
    Answer: 23
    Ground truth:  23
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 0.40738318860530853
    Answer: 23
    Ground truth:  23
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Bruce works 5 hours...
    Score: 4.703662842512131
    Answer: 23
    Ground truth:  23
Method name: attention_weighted_confidence, running accuracy: 89.90825688073394
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.90825688073394
Method name: cer_prob_product_log_last, running accuracy: 88.07339449541286
Method name: self_consistency, running accuracy: 90.36697247706422
Method name: p_true, running accuracy: 90.82568807339449
Method name: normilized_likelihood, running accuracy: 88.9908256880734
Method name: normilized_entropy, running accuracy: 88.53211009174312
Method name: topk_entropy, running accuracy: 88.53211009174312
Method name: window_entropy, running accuracy: 89.44954128440367

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 217/500 [19:09:01<20:19:29, 258.55s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=88.07%, self_consistency_acc=90.37%, p_true_acc=90.83%, normilized_likelihood_acc=88.99%, normilized_entropy_acc=88.53%, topk_entropy_acc=88.53%, window_entropy_acc=89.45%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▎     | 218/500 [19:09:01<19:12:31, 245.22s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=88.07%, self_consistency_acc=90.37%, p_true_acc=90.83%, normilized_likelihood_acc=88.99%, normilized_entropy_acc=88.53%, topk_entropy_acc=88.53%, window_entropy_acc=89.45%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 7.259882169771537
    Answer: 500
    Ground truth:  500
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 7.259882169771537
    Answer: 500
    Ground truth:  500
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 13.999668836593628
    Answer: 500
    Ground truth:  500
Method 4: self_consistency
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 0.875
    Answer: 500
    Ground truth:  500
Method 5: p_true
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 13.078125
    Answer: 500
    Ground truth:  500
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 3.6442603766918182
    Answer: 500
    Ground truth:  500
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 2.82802814245224
    Answer: 500
    Ground truth:  500
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 2.74914987385273
    Answer: 500
    Ground truth:  500
Method 9: window_entropy
  Batch 1:
    Text: To find the number of pieces left, let's break down the information given.

1. P...
    Score: 9.73997014760971
    Answer: 500
    Ground truth:  500
Method name: attention_weighted_confidence, running accuracy: 89.95433789954338
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.95433789954338
Method name: cer_prob_product_log_last, running accuracy: 88.12785388127854
Method name: self_consistency, running accuracy: 90.41095890410958
Method name: p_true, running accuracy: 90.8675799086758
Method name: normilized_likelihood, running accuracy: 89.04109589041096
Method name: normilized_entropy, running accuracy: 88.58447488584474
Method name: topk_entropy, running accuracy: 88.58447488584474
Method name: window_entropy, running accuracy: 89.49771689497716

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▎     | 218/500 [19:13:56<19:12:31, 245.22s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.13%, self_consistency_acc=90.41%, p_true_acc=90.87%, normilized_likelihood_acc=89.04%, normilized_entropy_acc=88.58%, topk_entropy_acc=88.58%, window_entropy_acc=89.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 219/500 [19:13:56<20:18:37, 260.20s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=88.13%, self_consistency_acc=90.41%, p_true_acc=90.87%, normilized_likelihood_acc=89.04%, normilized_entropy_acc=88.58%, topk_entropy_acc=88.58%, window_entropy_acc=89.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 7.84463908369827
    Answer: 360
    Ground truth:  360
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 7.84463908369827
    Answer: 360
    Ground truth:  360
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 15.99882572889328
    Answer: 360
    Ground truth:  360
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 1.0
    Answer: 360
    Ground truth:  360
Method 5: p_true
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 14.0
    Answer: 360
    Ground truth:  360
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 2.558065801858902
    Answer: 360
    Ground truth:  360
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 2.7627726644277573
    Answer: 360
    Ground truth:  360
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 2.618708610534668
    Answer: 360
    Ground truth:  360
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of math teachers on the court, first, we need to find o...
    Score: 17.073578715324402
    Answer: 360
    Ground truth:  360
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 88.18181818181819
Method name: self_consistency, running accuracy: 90.45454545454545
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 89.0909090909091
Method name: normilized_entropy, running accuracy: 88.63636363636364
Method name: topk_entropy, running accuracy: 88.63636363636364
Method name: window_entropy, running accuracy: 89.54545454545455

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 219/500 [19:18:37<20:18:37, 260.20s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=90.45%, p_true_acc=90.91%, normilized_likelihood_acc=89.09%, normilized_entropy_acc=88.64%, topk_entropy_acc=88.64%, window_entropy_acc=89.55%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 220/500 [19:18:37<20:44:01, 266.58s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=90.45%, p_true_acc=90.91%, normilized_likelihood_acc=89.09%, normilized_entropy_acc=88.64%, topk_entropy_acc=88.64%, window_entropy_acc=89.55%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 5.052444103604384
    Answer: 100
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 5.052444103604384
    Answer: 100
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 9.958800435066223
    Answer: 100
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 0.625
    Answer: 100
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 8.66796875
    Answer: 100
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 3.812987595796585
    Answer: 100
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 3.9387206435203552
    Answer: 100
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 3.4642063677310944
    Answer: 100
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of tennis shoes sold by the end of the sale, we'll brea...
    Score: 11.777625441551208
    Answer: 100
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 89.59276018099548
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.59276018099548
Method name: cer_prob_product_log_last, running accuracy: 87.78280542986425
Method name: self_consistency, running accuracy: 90.04524886877829
Method name: p_true, running accuracy: 90.49773755656109
Method name: normilized_likelihood, running accuracy: 88.68778280542986
Method name: normilized_entropy, running accuracy: 88.23529411764706
Method name: topk_entropy, running accuracy: 88.23529411764706
Method name: window_entropy, running accuracy: 89.14027149321268

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 220/500 [19:24:35<20:44:01, 266.58s/it, attention_weighted_confidence_acc=89.59%, cer_entropy_weighted_mean_all_acc=89.59%, cer_prob_product_log_last_acc=87.78%, self_consistency_acc=90.05%, p_true_acc=90.50%, normilized_likelihood_acc=88.69%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.14%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 221/500 [19:24:35<22:46:20, 293.84s/it, attention_weighted_confidence_acc=89.59%, cer_entropy_weighted_mean_all_acc=89.59%, cer_prob_product_log_last_acc=87.78%, self_consistency_acc=90.05%, p_true_acc=90.50%, normilized_likelihood_acc=88.69%, normilized_entropy_acc=88.24%, topk_entropy_acc=88.24%, window_entropy_acc=89.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 6.6604352192389005
    Answer: 7
    Ground truth:  7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 6.6604352192389005
    Answer: 7
    Ground truth:  7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 12.039008252322674
    Answer: 7
    Ground truth:  7
Method 4: self_consistency
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 0.8125
    Answer: 7
    Ground truth:  7
Method 5: p_true
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 10.0
    Answer: 7
    Ground truth:  7
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 1.0012720078229904
    Answer: 7
    Ground truth:  7
Method 7: normilized_entropy
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 1.0978121757507324
    Answer: 7
    Ground truth:  7
Method 8: topk_entropy
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 1.0566422492265701
    Answer: 7
    Ground truth:  7
Method 9: window_entropy
  Batch 1:
    Text: To determine how many oranges each person will get, we first need to find the to...
    Score: 3.5364668369293213
    Answer: 7
    Ground truth:  7
Method name: attention_weighted_confidence, running accuracy: 89.63963963963964
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.63963963963964
Method name: cer_prob_product_log_last, running accuracy: 87.83783783783784
Method name: self_consistency, running accuracy: 90.09009009009009
Method name: p_true, running accuracy: 90.54054054054053
Method name: normilized_likelihood, running accuracy: 88.73873873873875
Method name: normilized_entropy, running accuracy: 88.28828828828829
Method name: topk_entropy, running accuracy: 88.28828828828829
Method name: window_entropy, running accuracy: 89.1891891891892

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 221/500 [19:28:05<22:46:20, 293.84s/it, attention_weighted_confidence_acc=89.64%, cer_entropy_weighted_mean_all_acc=89.64%, cer_prob_product_log_last_acc=87.84%, self_consistency_acc=90.09%, p_true_acc=90.54%, normilized_likelihood_acc=88.74%, normilized_entropy_acc=88.29%, topk_entropy_acc=88.29%, window_entropy_acc=89.19%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 222/500 [19:28:05<20:45:32, 268.82s/it, attention_weighted_confidence_acc=89.64%, cer_entropy_weighted_mean_all_acc=89.64%, cer_prob_product_log_last_acc=87.84%, self_consistency_acc=90.09%, p_true_acc=90.54%, normilized_likelihood_acc=88.74%, normilized_entropy_acc=88.29%, topk_entropy_acc=88.29%, window_entropy_acc=89.19%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 7.137419497227947
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 7.137419497227947
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 14.997419595718384
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 0.9375
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 11.14453125
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 4.934825748205185
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 3.666231796145439
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 3.489746555685997
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To find the total weight of the adults, we first need to multiply the weight of ...
    Score: 11.50758296251297
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 89.68609865470853
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.68609865470853
Method name: cer_prob_product_log_last, running accuracy: 87.89237668161435
Method name: self_consistency, running accuracy: 90.13452914798206
Method name: p_true, running accuracy: 90.5829596412556
Method name: normilized_likelihood, running accuracy: 88.78923766816143
Method name: normilized_entropy, running accuracy: 88.34080717488789
Method name: topk_entropy, running accuracy: 88.34080717488789
Method name: window_entropy, running accuracy: 89.23766816143498

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 222/500 [19:32:07<20:45:32, 268.82s/it, attention_weighted_confidence_acc=89.69%, cer_entropy_weighted_mean_all_acc=89.69%, cer_prob_product_log_last_acc=87.89%, self_consistency_acc=90.13%, p_true_acc=90.58%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.34%, topk_entropy_acc=88.34%, window_entropy_acc=89.24%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▍     | 223/500 [19:32:07<20:03:35, 260.71s/it, attention_weighted_confidence_acc=89.69%, cer_entropy_weighted_mean_all_acc=89.69%, cer_prob_product_log_last_acc=87.89%, self_consistency_acc=90.13%, p_true_acc=90.58%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.34%, topk_entropy_acc=88.34%, window_entropy_acc=89.24%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 8.034067136254182
    Answer: 2640
    Ground truth:  2640
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 8.034067136254182
    Answer: 2640
    Ground truth:  2640
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 6.416048687024322
    Answer: 2640
    Ground truth:  2640
Method 4: self_consistency
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 1.0
    Answer: 2640
    Ground truth:  2640
Method 5: p_true
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 12.65625
    Answer: 2640
    Ground truth:  2640
Method 6: normilized_likelihood
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 2.8192819356918335
    Answer: 2640
    Ground truth:  2640
Method 7: normilized_entropy
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 2.453766167163849
    Answer: 2640
    Ground truth:  2640
Method 8: topk_entropy
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 2.398720070719719
    Answer: 2640
    Ground truth:  2640
Method 9: window_entropy
  Batch 1:
    Text: 1. First, we need to calculate Madeline's monthly expenses for her dog.
   - Foo...
    Score: 10.541965544223785
    Answer: 2640
    Ground truth:  2640
Method name: attention_weighted_confidence, running accuracy: 89.73214285714286
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.73214285714286
Method name: cer_prob_product_log_last, running accuracy: 87.94642857142857
Method name: self_consistency, running accuracy: 90.17857142857143
Method name: p_true, running accuracy: 90.625
Method name: normilized_likelihood, running accuracy: 88.83928571428571
Method name: normilized_entropy, running accuracy: 88.39285714285714
Method name: topk_entropy, running accuracy: 88.39285714285714
Method name: window_entropy, running accuracy: 89.28571428571429

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▍     | 223/500 [19:36:43<20:03:35, 260.71s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=90.18%, p_true_acc=90.62%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.39%, topk_entropy_acc=88.39%, window_entropy_acc=89.29%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▍     | 224/500 [19:36:43<20:20:25, 265.31s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=90.18%, p_true_acc=90.62%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.39%, topk_entropy_acc=88.39%, window_entropy_acc=89.29%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 8.347530682579292
    Answer: 80
    Ground truth:  80
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 8.347530682579292
    Answer: 80
    Ground truth:  80
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 15.99993622303009
    Answer: 80
    Ground truth:  80
Method 4: self_consistency
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 1.0
    Answer: 80
    Ground truth:  80
Method 5: p_true
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 14.84375
    Answer: 80
    Ground truth:  80
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 1.6795485839247704
    Answer: 80
    Ground truth:  80
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 1.6161659806966782
    Answer: 80
    Ground truth:  80
Method 8: topk_entropy
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 1.611639365553856
    Answer: 80
    Ground truth:  80
Method 9: window_entropy
  Batch 1:
    Text: To find the total distance Mason traveled, we need to calculate the distance he ...
    Score: 5.112104743719101
    Answer: 80
    Ground truth:  80
Method name: attention_weighted_confidence, running accuracy: 89.77777777777777
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.77777777777777
Method name: cer_prob_product_log_last, running accuracy: 88.0
Method name: self_consistency, running accuracy: 90.22222222222223
Method name: p_true, running accuracy: 90.66666666666666
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.44444444444444
Method name: topk_entropy, running accuracy: 88.44444444444444
Method name: window_entropy, running accuracy: 89.33333333333333

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▍     | 224/500 [19:40:53<20:20:25, 265.31s/it, attention_weighted_confidence_acc=89.78%, cer_entropy_weighted_mean_all_acc=89.78%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=90.22%, p_true_acc=90.67%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.44%, topk_entropy_acc=88.44%, window_entropy_acc=89.33%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 225/500 [19:40:53<19:54:14, 260.56s/it, attention_weighted_confidence_acc=89.78%, cer_entropy_weighted_mean_all_acc=89.78%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=90.22%, p_true_acc=90.67%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.44%, topk_entropy_acc=88.44%, window_entropy_acc=89.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 4.822474540203808
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 4.822474540203808
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 9.901345431804657
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 0.625
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 9.69140625
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 14.88570812344551
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 14.425999015569687
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 11.897862076759338
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Calculate the ...
    Score: 16.037787973880768
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.82300884955751
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.82300884955751
Method name: cer_prob_product_log_last, running accuracy: 88.05309734513274
Method name: self_consistency, running accuracy: 90.2654867256637
Method name: p_true, running accuracy: 90.7079646017699
Method name: normilized_likelihood, running accuracy: 88.93805309734513
Method name: normilized_entropy, running accuracy: 88.49557522123894
Method name: topk_entropy, running accuracy: 88.49557522123894
Method name: window_entropy, running accuracy: 89.38053097345133

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 225/500 [19:49:58<19:54:14, 260.56s/it, attention_weighted_confidence_acc=89.82%, cer_entropy_weighted_mean_all_acc=89.82%, cer_prob_product_log_last_acc=88.05%, self_consistency_acc=90.27%, p_true_acc=90.71%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.50%, topk_entropy_acc=88.50%, window_entropy_acc=89.38%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 226/500 [19:49:58<26:20:21, 346.06s/it, attention_weighted_confidence_acc=89.82%, cer_entropy_weighted_mean_all_acc=89.82%, cer_prob_product_log_last_acc=88.05%, self_consistency_acc=90.27%, p_true_acc=90.71%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.50%, topk_entropy_acc=88.50%, window_entropy_acc=89.38%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 4.11995348149903
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 4.11995348149903
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 7.365587949752808
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 0.5
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 6.12579345703125
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 2.2636828869581223
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 2.0314991921186447
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 2.007926806807518
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the number of blue shoe boxes left, we need to subtract the number used ...
    Score: 4.115857899188995
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 89.86784140969164
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.86784140969164
Method name: cer_prob_product_log_last, running accuracy: 88.10572687224669
Method name: self_consistency, running accuracy: 90.30837004405286
Method name: p_true, running accuracy: 90.7488986784141
Method name: normilized_likelihood, running accuracy: 88.98678414096916
Method name: normilized_entropy, running accuracy: 88.54625550660793
Method name: topk_entropy, running accuracy: 88.54625550660793
Method name: window_entropy, running accuracy: 89.42731277533039

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 226/500 [19:54:37<26:20:21, 346.06s/it, attention_weighted_confidence_acc=89.87%, cer_entropy_weighted_mean_all_acc=89.87%, cer_prob_product_log_last_acc=88.11%, self_consistency_acc=90.31%, p_true_acc=90.75%, normilized_likelihood_acc=88.99%, normilized_entropy_acc=88.55%, topk_entropy_acc=88.55%, window_entropy_acc=89.43%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 227/500 [19:54:37<24:42:52, 325.91s/it, attention_weighted_confidence_acc=89.87%, cer_entropy_weighted_mean_all_acc=89.87%, cer_prob_product_log_last_acc=88.11%, self_consistency_acc=90.31%, p_true_acc=90.75%, normilized_likelihood_acc=88.99%, normilized_entropy_acc=88.55%, topk_entropy_acc=88.55%, window_entropy_acc=89.43%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 8.339194346546552
    Answer: 300
    Ground truth:  300
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 8.339194346546552
    Answer: 300
    Ground truth:  300
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 15.99998688697815
    Answer: 300
    Ground truth:  300
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 1.0
    Answer: 300
    Ground truth:  300
Method 5: p_true
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 13.1953125
    Answer: 300
    Ground truth:  300
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 1.70839062333107
    Answer: 300
    Ground truth:  300
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 1.2308588922023773
    Answer: 300
    Ground truth:  300
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 1.2315656244754791
    Answer: 300
    Ground truth:  300
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, we need to follow the steps of finding out how much Julie ...
    Score: 3.5803577601909637
    Answer: 300
    Ground truth:  300
Method name: attention_weighted_confidence, running accuracy: 89.91228070175438
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.91228070175438
Method name: cer_prob_product_log_last, running accuracy: 88.1578947368421
Method name: self_consistency, running accuracy: 90.35087719298247
Method name: p_true, running accuracy: 90.78947368421053
Method name: normilized_likelihood, running accuracy: 89.03508771929825
Method name: normilized_entropy, running accuracy: 88.59649122807018
Method name: topk_entropy, running accuracy: 88.59649122807018
Method name: window_entropy, running accuracy: 89.47368421052632

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 227/500 [19:58:41<24:42:52, 325.91s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=88.16%, self_consistency_acc=90.35%, p_true_acc=90.79%, normilized_likelihood_acc=89.04%, normilized_entropy_acc=88.60%, topk_entropy_acc=88.60%, window_entropy_acc=89.47%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 228/500 [19:58:41<22:45:53, 301.30s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=88.16%, self_consistency_acc=90.35%, p_true_acc=90.79%, normilized_likelihood_acc=89.04%, normilized_entropy_acc=88.60%, topk_entropy_acc=88.60%, window_entropy_acc=89.47%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 4.746030557328506
    Answer: 83
    Ground truth:  83
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 4.746030557328506
    Answer: 83
    Ground truth:  83
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 8.994052350521088
    Answer: 83
    Ground truth:  83
Method 4: self_consistency
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 0.5625
    Answer: 83
    Ground truth:  83
Method 5: p_true
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 8.39453125
    Answer: 83
    Ground truth:  83
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 1.226748913526535
    Answer: 83
    Ground truth:  83
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 1.0201140642166138
    Answer: 83
    Ground truth:  83
Method 8: topk_entropy
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 0.9691756665706635
    Answer: 83
    Ground truth:  83
Method 9: window_entropy
  Batch 1:
    Text: To find the total points of all three players after the extra points are distrib...
    Score: 3.6264771223068237
    Answer: 83
    Ground truth:  83
Method name: attention_weighted_confidence, running accuracy: 89.95633187772926
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.95633187772926
Method name: cer_prob_product_log_last, running accuracy: 88.20960698689956
Method name: self_consistency, running accuracy: 90.39301310043668
Method name: p_true, running accuracy: 90.82969432314411
Method name: normilized_likelihood, running accuracy: 89.08296943231441
Method name: normilized_entropy, running accuracy: 88.64628820960698
Method name: topk_entropy, running accuracy: 88.64628820960698
Method name: window_entropy, running accuracy: 89.51965065502183

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 228/500 [20:04:33<22:45:53, 301.30s/it, attention_weighted_confidence_acc=89.96%, cer_entropy_weighted_mean_all_acc=89.96%, cer_prob_product_log_last_acc=88.21%, self_consistency_acc=90.39%, p_true_acc=90.83%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.65%, topk_entropy_acc=88.65%, window_entropy_acc=89.52%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 229/500 [20:04:33<23:50:06, 316.63s/it, attention_weighted_confidence_acc=89.96%, cer_entropy_weighted_mean_all_acc=89.96%, cer_prob_product_log_last_acc=88.21%, self_consistency_acc=90.39%, p_true_acc=90.83%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.65%, topk_entropy_acc=88.65%, window_entropy_acc=89.52%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 7.764952452920191
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 7.764952452920191
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 14.99992823600769
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 0.9375
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 14.44140625
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 4.673471570014954
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 5.230811730027199
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 4.897457420825958
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Determine the nu...
    Score: 13.154166102409363
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 88.26086956521739
Method name: self_consistency, running accuracy: 90.43478260869566
Method name: p_true, running accuracy: 90.8695652173913
Method name: normilized_likelihood, running accuracy: 89.13043478260869
Method name: normilized_entropy, running accuracy: 88.69565217391305
Method name: topk_entropy, running accuracy: 88.69565217391305
Method name: window_entropy, running accuracy: 89.56521739130436

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 229/500 [20:09:46<23:50:06, 316.63s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.26%, self_consistency_acc=90.43%, p_true_acc=90.87%, normilized_likelihood_acc=89.13%, normilized_entropy_acc=88.70%, topk_entropy_acc=88.70%, window_entropy_acc=89.57%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 230/500 [20:09:46<23:39:47, 315.51s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=88.26%, self_consistency_acc=90.43%, p_true_acc=90.87%, normilized_likelihood_acc=89.13%, normilized_entropy_acc=88.70%, topk_entropy_acc=88.70%, window_entropy_acc=89.57%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 5.716125986139704
    Answer: 600
    Ground truth:  675
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 5.716125986139704
    Answer: 600
    Ground truth:  675
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 11.999367773532867
    Answer: 600
    Ground truth:  675
Method 4: self_consistency
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 0.75
    Answer: 600
    Ground truth:  675
Method 5: p_true
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 6.763671875
    Answer: 600
    Ground truth:  675
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 4.439154535531998
    Answer: 600
    Ground truth:  675
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 4.301228791475296
    Answer: 600
    Ground truth:  675
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 4.066000998020172
    Answer: 600
    Ground truth:  675
Method 9: window_entropy
  Batch 1:
    Text: To find out how many books Ahmed read in one hour, we need to divide the number ...
    Score: 10.765627145767212
    Answer: 600
    Ground truth:  675
Method name: attention_weighted_confidence, running accuracy: 89.6103896103896
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.6103896103896
Method name: cer_prob_product_log_last, running accuracy: 87.87878787878788
Method name: self_consistency, running accuracy: 90.04329004329004
Method name: p_true, running accuracy: 90.47619047619048
Method name: normilized_likelihood, running accuracy: 88.74458874458875
Method name: normilized_entropy, running accuracy: 88.31168831168831
Method name: topk_entropy, running accuracy: 88.31168831168831
Method name: window_entropy, running accuracy: 89.17748917748918

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 230/500 [20:14:31<23:39:47, 315.51s/it, attention_weighted_confidence_acc=89.61%, cer_entropy_weighted_mean_all_acc=89.61%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=90.04%, p_true_acc=90.48%, normilized_likelihood_acc=88.74%, normilized_entropy_acc=88.31%, topk_entropy_acc=88.31%, window_entropy_acc=89.18%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 231/500 [20:14:31<22:52:39, 306.17s/it, attention_weighted_confidence_acc=89.61%, cer_entropy_weighted_mean_all_acc=89.61%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=90.04%, p_true_acc=90.48%, normilized_likelihood_acc=88.74%, normilized_entropy_acc=88.31%, topk_entropy_acc=88.31%, window_entropy_acc=89.18%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 8.549071710837495
    Answer: 9
    Ground truth:  9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 8.549071710837495
    Answer: 9
    Ground truth:  9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 15.997927188873291
    Answer: 9
    Ground truth:  9
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 1.0
    Answer: 9
    Ground truth:  9
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 14.328125
    Answer: 9
    Ground truth:  9
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 2.4623024314641953
    Answer: 9
    Ground truth:  9
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 1.7007049024105072
    Answer: 9
    Ground truth:  9
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 1.7003269344568253
    Answer: 9
    Ground truth:  9
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step. 

1. A dozen has 12 items, so Cole ha...
    Score: 4.312705814838409
    Answer: 9
    Ground truth:  9
Method name: attention_weighted_confidence, running accuracy: 89.65517241379311
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.65517241379311
Method name: cer_prob_product_log_last, running accuracy: 87.93103448275862
Method name: self_consistency, running accuracy: 90.08620689655173
Method name: p_true, running accuracy: 90.51724137931035
Method name: normilized_likelihood, running accuracy: 88.79310344827587
Method name: normilized_entropy, running accuracy: 88.36206896551724
Method name: topk_entropy, running accuracy: 88.36206896551724
Method name: window_entropy, running accuracy: 89.22413793103449

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 231/500 [20:19:36<22:52:39, 306.17s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=87.93%, self_consistency_acc=90.09%, p_true_acc=90.52%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.22%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▋     | 232/500 [20:19:36<22:46:25, 305.91s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=87.93%, self_consistency_acc=90.09%, p_true_acc=90.52%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.22%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 5.044502872421339
    Answer: 284
    Ground truth:  284
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 5.044502872421339
    Answer: 284
    Ground truth:  284
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 9.997113943099976
    Answer: 284
    Ground truth:  284
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 0.625
    Answer: 284
    Ground truth:  284
Method 5: p_true
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 8.29296875
    Answer: 284
    Ground truth:  284
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 6.906527057290077
    Answer: 284
    Ground truth:  284
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 5.235718593001366
    Answer: 284
    Ground truth:  284
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 4.708209916949272
    Answer: 284
    Ground truth:  284
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the cost of the service for Bill over 2 years.

The service was...
    Score: 7.71821916103363
    Answer: 284
    Ground truth:  284
Method name: attention_weighted_confidence, running accuracy: 89.69957081545064
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.69957081545064
Method name: cer_prob_product_log_last, running accuracy: 87.98283261802575
Method name: self_consistency, running accuracy: 90.12875536480686
Method name: p_true, running accuracy: 90.55793991416309
Method name: normilized_likelihood, running accuracy: 88.8412017167382
Method name: normilized_entropy, running accuracy: 88.41201716738198
Method name: topk_entropy, running accuracy: 88.41201716738198
Method name: window_entropy, running accuracy: 89.27038626609442

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▋     | 232/500 [20:25:08<22:46:25, 305.91s/it, attention_weighted_confidence_acc=89.70%, cer_entropy_weighted_mean_all_acc=89.70%, cer_prob_product_log_last_acc=87.98%, self_consistency_acc=90.13%, p_true_acc=90.56%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.27%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 233/500 [20:25:08<23:15:48, 313.67s/it, attention_weighted_confidence_acc=89.70%, cer_entropy_weighted_mean_all_acc=89.70%, cer_prob_product_log_last_acc=87.98%, self_consistency_acc=90.13%, p_true_acc=90.56%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.27%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 7.877164368021526
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 7.877164368021526
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 15.999382138252258
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 1.0
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 15.1796875
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 2.0217419862747192
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 1.172217220067978
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 1.1616486012935638
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To find out how many cookies each of Susan's nephews will get, first, we need to...
    Score: 2.417017340660095
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 89.74358974358975
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.74358974358975
Method name: cer_prob_product_log_last, running accuracy: 88.03418803418803
Method name: self_consistency, running accuracy: 90.17094017094017
Method name: p_true, running accuracy: 90.5982905982906
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.46153846153845
Method name: topk_entropy, running accuracy: 88.46153846153845
Method name: window_entropy, running accuracy: 89.31623931623932

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 233/500 [20:28:21<23:15:48, 313.67s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=88.03%, self_consistency_acc=90.17%, p_true_acc=90.60%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.32%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 234/500 [20:28:21<20:30:45, 277.62s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=88.03%, self_consistency_acc=90.17%, p_true_acc=90.60%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.32%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 8.195055982016987
    Answer: 1920
    Ground truth:  1920
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 8.195055982016987
    Answer: 1920
    Ground truth:  1920
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 7.756108617872528
    Answer: 1920
    Ground truth:  1920
Method 4: self_consistency
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 1.0
    Answer: 1920
    Ground truth:  1920
Method 5: p_true
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 13.4609375
    Answer: 1920
    Ground truth:  1920
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 1.7817510813474655
    Answer: 1920
    Ground truth:  1920
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 0.6054854542016983
    Answer: 1920
    Ground truth:  1920
Method 8: topk_entropy
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 0.6067616194486618
    Answer: 1920
    Ground truth:  1920
Method 9: window_entropy
  Batch 1:
    Text: To find the total volume of soil Bob needs, we need to calculate the volume of e...
    Score: 2.6369694471359253
    Answer: 1920
    Ground truth:  1920
Method name: attention_weighted_confidence, running accuracy: 89.7872340425532
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.7872340425532
Method name: cer_prob_product_log_last, running accuracy: 88.08510638297872
Method name: self_consistency, running accuracy: 90.2127659574468
Method name: p_true, running accuracy: 90.63829787234042
Method name: normilized_likelihood, running accuracy: 88.93617021276596
Method name: normilized_entropy, running accuracy: 88.51063829787233
Method name: topk_entropy, running accuracy: 88.51063829787233
Method name: window_entropy, running accuracy: 89.36170212765957

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 234/500 [20:33:51<20:30:45, 277.62s/it, attention_weighted_confidence_acc=89.79%, cer_entropy_weighted_mean_all_acc=89.79%, cer_prob_product_log_last_acc=88.09%, self_consistency_acc=90.21%, p_true_acc=90.64%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.51%, topk_entropy_acc=88.51%, window_entropy_acc=89.36%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 235/500 [20:33:51<21:35:51, 293.40s/it, attention_weighted_confidence_acc=89.79%, cer_entropy_weighted_mean_all_acc=89.79%, cer_prob_product_log_last_acc=88.09%, self_consistency_acc=90.21%, p_true_acc=90.64%, normilized_likelihood_acc=88.94%, normilized_entropy_acc=88.51%, topk_entropy_acc=88.51%, window_entropy_acc=89.36%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 8.147453608897962
    Answer: 595
    Ground truth:  595
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 8.147453608897962
    Answer: 595
    Ground truth:  595
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 14.863227546215057
    Answer: 595
    Ground truth:  595
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 0.9375
    Answer: 595
    Ground truth:  595
Method 5: p_true
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 13.37890625
    Answer: 595
    Ground truth:  595
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 3.5977639704942703
    Answer: 595
    Ground truth:  595
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 1.8123541921377182
    Answer: 595
    Ground truth:  595
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 1.797491930425167
    Answer: 595
    Ground truth:  595
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of gems in the chest, we need to calculate the number o...
    Score: 2.7880937457084656
    Answer: 595
    Ground truth:  595
Method name: attention_weighted_confidence, running accuracy: 89.83050847457628
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.83050847457628
Method name: cer_prob_product_log_last, running accuracy: 88.13559322033898
Method name: self_consistency, running accuracy: 90.2542372881356
Method name: p_true, running accuracy: 90.67796610169492
Method name: normilized_likelihood, running accuracy: 88.98305084745762
Method name: normilized_entropy, running accuracy: 88.5593220338983
Method name: topk_entropy, running accuracy: 88.5593220338983
Method name: window_entropy, running accuracy: 89.40677966101694

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 235/500 [20:38:04<21:35:51, 293.40s/it, attention_weighted_confidence_acc=89.83%, cer_entropy_weighted_mean_all_acc=89.83%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=90.25%, p_true_acc=90.68%, normilized_likelihood_acc=88.98%, normilized_entropy_acc=88.56%, topk_entropy_acc=88.56%, window_entropy_acc=89.41%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 236/500 [20:38:04<20:37:40, 281.29s/it, attention_weighted_confidence_acc=89.83%, cer_entropy_weighted_mean_all_acc=89.83%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=90.25%, p_true_acc=90.68%, normilized_likelihood_acc=88.98%, normilized_entropy_acc=88.56%, topk_entropy_acc=88.56%, window_entropy_acc=89.41%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 6.904296147605924
    Answer: 92
    Ground truth:  92
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 6.904296147605924
    Answer: 92
    Ground truth:  92
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 13.998925149440765
    Answer: 92
    Ground truth:  92
Method 4: self_consistency
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 0.875
    Answer: 92
    Ground truth:  92
Method 5: p_true
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 11.8828125
    Answer: 92
    Ground truth:  92
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 6.992859870195389
    Answer: 92
    Ground truth:  92
Method 7: normilized_entropy
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 6.226479962468147
    Answer: 92
    Ground truth:  92
Method 8: topk_entropy
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 5.577155321836472
    Answer: 92
    Ground truth:  92
Method 9: window_entropy
  Batch 1:
    Text: To determine how many additional bottles Bill needs to buy, we need to follow th...
    Score: 18.10424119234085
    Answer: 92
    Ground truth:  92
Method name: attention_weighted_confidence, running accuracy: 89.87341772151899
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.87341772151899
Method name: cer_prob_product_log_last, running accuracy: 88.18565400843882
Method name: self_consistency, running accuracy: 90.29535864978902
Method name: p_true, running accuracy: 90.71729957805907
Method name: normilized_likelihood, running accuracy: 89.0295358649789
Method name: normilized_entropy, running accuracy: 88.60759493670885
Method name: topk_entropy, running accuracy: 88.60759493670885
Method name: window_entropy, running accuracy: 89.45147679324894

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 236/500 [20:42:12<20:37:40, 281.29s/it, attention_weighted_confidence_acc=89.87%, cer_entropy_weighted_mean_all_acc=89.87%, cer_prob_product_log_last_acc=88.19%, self_consistency_acc=90.30%, p_true_acc=90.72%, normilized_likelihood_acc=89.03%, normilized_entropy_acc=88.61%, topk_entropy_acc=88.61%, window_entropy_acc=89.45%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 237/500 [20:42:12<19:49:07, 271.28s/it, attention_weighted_confidence_acc=89.87%, cer_entropy_weighted_mean_all_acc=89.87%, cer_prob_product_log_last_acc=88.19%, self_consistency_acc=90.30%, p_true_acc=90.72%, normilized_likelihood_acc=89.03%, normilized_entropy_acc=88.61%, topk_entropy_acc=88.61%, window_entropy_acc=89.45%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 8.398653509403117
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 8.398653509403117
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 15.998818457126617
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 1.0
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 14.8125
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 1.4084906727075577
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 1.1354281455278397
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 1.1316384077072144
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find out how many boxes of pizza Marie ordered, we need to first calculate th...
    Score: 4.12930428981781
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 89.91596638655463
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.91596638655463
Method name: cer_prob_product_log_last, running accuracy: 88.23529411764706
Method name: self_consistency, running accuracy: 90.33613445378151
Method name: p_true, running accuracy: 90.75630252100841
Method name: normilized_likelihood, running accuracy: 89.07563025210085
Method name: normilized_entropy, running accuracy: 88.65546218487394
Method name: topk_entropy, running accuracy: 88.65546218487394
Method name: window_entropy, running accuracy: 89.49579831932773

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 237/500 [20:47:16<19:49:07, 271.28s/it, attention_weighted_confidence_acc=89.92%, cer_entropy_weighted_mean_all_acc=89.92%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=90.34%, p_true_acc=90.76%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.66%, topk_entropy_acc=88.66%, window_entropy_acc=89.50%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 238/500 [20:47:16<20:27:10, 281.03s/it, attention_weighted_confidence_acc=89.92%, cer_entropy_weighted_mean_all_acc=89.92%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=90.34%, p_true_acc=90.76%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.66%, topk_entropy_acc=88.66%, window_entropy_acc=89.50%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 7.672838175669698
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 7.672838175669698
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 14.99964314699173
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 0.9375
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 13.82421875
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 1.1177892237901688
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 1.0297983139753342
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 1.0071355551481247
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find out how much John needs to spend on hot dogs, we'll need to calculate th...
    Score: 2.756017506122589
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 89.9581589958159
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.9581589958159
Method name: cer_prob_product_log_last, running accuracy: 88.28451882845188
Method name: self_consistency, running accuracy: 90.3765690376569
Method name: p_true, running accuracy: 90.7949790794979
Method name: normilized_likelihood, running accuracy: 89.1213389121339
Method name: normilized_entropy, running accuracy: 88.70292887029288
Method name: topk_entropy, running accuracy: 88.70292887029288
Method name: window_entropy, running accuracy: 89.5397489539749

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 238/500 [20:52:25<20:27:10, 281.03s/it, attention_weighted_confidence_acc=89.96%, cer_entropy_weighted_mean_all_acc=89.96%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=90.38%, p_true_acc=90.79%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.70%, topk_entropy_acc=88.70%, window_entropy_acc=89.54%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 239/500 [20:52:25<20:58:33, 289.32s/it, attention_weighted_confidence_acc=89.96%, cer_entropy_weighted_mean_all_acc=89.96%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=90.38%, p_true_acc=90.79%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.70%, topk_entropy_acc=88.70%, window_entropy_acc=89.54%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 7.661510858000333
    Answer: 40000
    Ground truth:  40.000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 7.661510858000333
    Answer: 40000
    Ground truth:  40.000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money John had leftover after paying off his debts, let's b...
    Score: 0.9999988079071045
    Answer: 40
    Ground truth:  40.000
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 0.9375
    Answer: 40000
    Ground truth:  40.000
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 12.75390625
    Answer: 40000
    Ground truth:  40.000
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 8.670330941677094
    Answer: 40000
    Ground truth:  40.000
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 8.063452869653702
    Answer: 40000
    Ground truth:  40.000
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 7.372543632984161
    Answer: 40000
    Ground truth:  40.000
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem into steps.

1. John decides to mortgage his home, ...
    Score: 21.30587351322174
    Answer: 40000
    Ground truth:  40.000
Method name: attention_weighted_confidence, running accuracy: 89.58333333333334
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.58333333333334
Method name: cer_prob_product_log_last, running accuracy: 88.33333333333333
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.41666666666667
Method name: normilized_likelihood, running accuracy: 88.75
Method name: normilized_entropy, running accuracy: 88.33333333333333
Method name: topk_entropy, running accuracy: 88.33333333333333
Method name: window_entropy, running accuracy: 89.16666666666667

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 239/500 [20:57:11<20:58:33, 289.32s/it, attention_weighted_confidence_acc=89.58%, cer_entropy_weighted_mean_all_acc=89.58%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=90.00%, p_true_acc=90.42%, normilized_likelihood_acc=88.75%, normilized_entropy_acc=88.33%, topk_entropy_acc=88.33%, window_entropy_acc=89.17%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 240/500 [20:57:11<20:49:18, 288.30s/it, attention_weighted_confidence_acc=89.58%, cer_entropy_weighted_mean_all_acc=89.58%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=90.00%, p_true_acc=90.42%, normilized_likelihood_acc=88.75%, normilized_entropy_acc=88.33%, topk_entropy_acc=88.33%, window_entropy_acc=89.17%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 5.737140952448039
    Answer: 168
    Ground truth:  168
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 5.737140952448039
    Answer: 168
    Ground truth:  168
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 10.996725142002106
    Answer: 168
    Ground truth:  168
Method 4: self_consistency
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 0.6875
    Answer: 168
    Ground truth:  168
Method 5: p_true
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 9.9609375
    Answer: 168
    Ground truth:  168
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 5.697377160191536
    Answer: 168
    Ground truth:  168
Method 7: normilized_entropy
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 6.125652238726616
    Answer: 168
    Ground truth:  168
Method 8: topk_entropy
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 5.632380366325378
    Answer: 168
    Ground truth:  168
Method 9: window_entropy
  Batch 1:
    Text: To find out the total number of fruits, let's break it down step by step:

1. Si...
    Score: 16.128206491470337
    Answer: 168
    Ground truth:  168
Method name: attention_weighted_confidence, running accuracy: 89.62655601659752
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.62655601659752
Method name: cer_prob_product_log_last, running accuracy: 88.38174273858921
Method name: self_consistency, running accuracy: 90.04149377593362
Method name: p_true, running accuracy: 90.45643153526972
Method name: normilized_likelihood, running accuracy: 88.79668049792531
Method name: normilized_entropy, running accuracy: 88.38174273858921
Method name: topk_entropy, running accuracy: 88.38174273858921
Method name: window_entropy, running accuracy: 89.21161825726142

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 240/500 [21:03:53<20:49:18, 288.30s/it, attention_weighted_confidence_acc=89.63%, cer_entropy_weighted_mean_all_acc=89.63%, cer_prob_product_log_last_acc=88.38%, self_consistency_acc=90.04%, p_true_acc=90.46%, normilized_likelihood_acc=88.80%, normilized_entropy_acc=88.38%, topk_entropy_acc=88.38%, window_entropy_acc=89.21%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 241/500 [21:03:53<23:12:38, 322.62s/it, attention_weighted_confidence_acc=89.63%, cer_entropy_weighted_mean_all_acc=89.63%, cer_prob_product_log_last_acc=88.38%, self_consistency_acc=90.04%, p_true_acc=90.46%, normilized_likelihood_acc=88.80%, normilized_entropy_acc=88.38%, topk_entropy_acc=88.38%, window_entropy_acc=89.21%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 8.319207704903768
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 8.319207704903768
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 15.99996292591095
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 1.0
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 15.3515625
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 3.9899830147624016
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 2.3167677000164986
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 2.3030055090785027
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the information given:

1. Jeff is 10 years older than Martha.
...
    Score: 8.5011445581913
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 89.6694214876033
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.6694214876033
Method name: cer_prob_product_log_last, running accuracy: 88.42975206611571
Method name: self_consistency, running accuracy: 90.08264462809917
Method name: p_true, running accuracy: 90.49586776859503
Method name: normilized_likelihood, running accuracy: 88.84297520661157
Method name: normilized_entropy, running accuracy: 88.42975206611571
Method name: topk_entropy, running accuracy: 88.42975206611571
Method name: window_entropy, running accuracy: 89.25619834710744

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 241/500 [21:07:00<23:12:38, 322.62s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.43%, self_consistency_acc=90.08%, p_true_acc=90.50%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.43%, topk_entropy_acc=88.43%, window_entropy_acc=89.26%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 242/500 [21:07:00<20:11:58, 281.85s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.43%, self_consistency_acc=90.08%, p_true_acc=90.50%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.43%, topk_entropy_acc=88.43%, window_entropy_acc=89.26%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 6.709778099806787
    Answer: 160
    Ground truth:  160
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 6.709778099806787
    Answer: 160
    Ground truth:  160
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 12.995327949523926
    Answer: 160
    Ground truth:  160
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 0.8125
    Answer: 160
    Ground truth:  160
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 11.62890625
    Answer: 160
    Ground truth:  160
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 2.365290120244026
    Answer: 160
    Ground truth:  160
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 2.4785365611314774
    Answer: 160
    Ground truth:  160
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 2.268464148044586
    Answer: 160
    Ground truth:  160
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Let's denote the nu...
    Score: 12.848508298397064
    Answer: 160
    Ground truth:  160
Method name: attention_weighted_confidence, running accuracy: 89.7119341563786
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.7119341563786
Method name: cer_prob_product_log_last, running accuracy: 88.47736625514403
Method name: self_consistency, running accuracy: 90.12345679012346
Method name: p_true, running accuracy: 90.53497942386831
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.47736625514403
Method name: topk_entropy, running accuracy: 88.47736625514403
Method name: window_entropy, running accuracy: 89.30041152263375

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 242/500 [21:11:47<20:11:58, 281.85s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.48%, self_consistency_acc=90.12%, p_true_acc=90.53%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.30%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▊     | 243/500 [21:11:47<20:13:17, 283.26s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.48%, self_consistency_acc=90.12%, p_true_acc=90.53%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.30%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 7.846186843819675
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 7.846186843819675
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 15.999343276023865
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 15.01171875
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 10.375799596309662
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 9.664285406470299
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 8.95340171456337
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how many days Jade will need to upload all her photos, we need to di...
    Score: 16.151086151599884
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 89.75409836065575
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.75409836065575
Method name: cer_prob_product_log_last, running accuracy: 88.52459016393442
Method name: self_consistency, running accuracy: 90.1639344262295
Method name: p_true, running accuracy: 90.57377049180327
Method name: normilized_likelihood, running accuracy: 88.9344262295082
Method name: normilized_entropy, running accuracy: 88.52459016393442
Method name: topk_entropy, running accuracy: 88.52459016393442
Method name: window_entropy, running accuracy: 89.34426229508196

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▊     | 243/500 [21:15:40<20:13:17, 283.26s/it, attention_weighted_confidence_acc=89.75%, cer_entropy_weighted_mean_all_acc=89.75%, cer_prob_product_log_last_acc=88.52%, self_consistency_acc=90.16%, p_true_acc=90.57%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.52%, topk_entropy_acc=88.52%, window_entropy_acc=89.34%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 244/500 [21:15:40<19:04:11, 268.17s/it, attention_weighted_confidence_acc=89.75%, cer_entropy_weighted_mean_all_acc=89.75%, cer_prob_product_log_last_acc=88.52%, self_consistency_acc=90.16%, p_true_acc=90.57%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.52%, topk_entropy_acc=88.52%, window_entropy_acc=89.34%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 8.25788714398039
    Answer: 135
    Ground truth:  81
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 8.25788714398039
    Answer: 135
    Ground truth:  81
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 15.999020278453827
    Answer: 135
    Ground truth:  81
Method 4: self_consistency
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 1.0
    Answer: 135
    Ground truth:  81
Method 5: p_true
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 15.2890625
    Answer: 135
    Ground truth:  81
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 2.242772340774536
    Answer: 135
    Ground truth:  81
Method 7: normilized_entropy
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 1.3085357695817947
    Answer: 135
    Ground truth:  81
Method 8: topk_entropy
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 1.2855860888957977
    Answer: 135
    Ground truth:  81
Method 9: window_entropy
  Batch 1:
    Text: To find the amount of energy saved, we need to first calculate the amount of ene...
    Score: 7.692399024963379
    Answer: 135
    Ground truth:  81
Method name: attention_weighted_confidence, running accuracy: 89.38775510204081
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.38775510204081
Method name: cer_prob_product_log_last, running accuracy: 88.16326530612245
Method name: self_consistency, running accuracy: 89.79591836734694
Method name: p_true, running accuracy: 90.20408163265307
Method name: normilized_likelihood, running accuracy: 88.57142857142857
Method name: normilized_entropy, running accuracy: 88.16326530612245
Method name: topk_entropy, running accuracy: 88.16326530612245
Method name: window_entropy, running accuracy: 88.9795918367347

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 244/500 [21:22:57<19:04:11, 268.17s/it, attention_weighted_confidence_acc=89.39%, cer_entropy_weighted_mean_all_acc=89.39%, cer_prob_product_log_last_acc=88.16%, self_consistency_acc=89.80%, p_true_acc=90.20%, normilized_likelihood_acc=88.57%, normilized_entropy_acc=88.16%, topk_entropy_acc=88.16%, window_entropy_acc=88.98%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 245/500 [21:22:57<22:34:50, 318.79s/it, attention_weighted_confidence_acc=89.39%, cer_entropy_weighted_mean_all_acc=89.39%, cer_prob_product_log_last_acc=88.16%, self_consistency_acc=89.80%, p_true_acc=90.20%, normilized_likelihood_acc=88.57%, normilized_entropy_acc=88.16%, topk_entropy_acc=88.16%, window_entropy_acc=88.98%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 8.141551437981864
    Answer: 270
    Ground truth:  270
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 8.141551437981864
    Answer: 270
    Ground truth:  270
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 15.995013058185577
    Answer: 270
    Ground truth:  270
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 1.0
    Answer: 270
    Ground truth:  270
Method 5: p_true
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 13.2734375
    Answer: 270
    Ground truth:  270
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 2.068586304783821
    Answer: 270
    Ground truth:  270
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 0.7979290634393692
    Answer: 270
    Ground truth:  270
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 0.7957510054111481
    Answer: 270
    Ground truth:  270
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of marbles Maddison has now, we need to follow these st...
    Score: 2.2135103344917297
    Answer: 270
    Ground truth:  270
Method name: attention_weighted_confidence, running accuracy: 89.43089430894308
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.43089430894308
Method name: cer_prob_product_log_last, running accuracy: 88.21138211382113
Method name: self_consistency, running accuracy: 89.83739837398373
Method name: p_true, running accuracy: 90.2439024390244
Method name: normilized_likelihood, running accuracy: 88.6178861788618
Method name: normilized_entropy, running accuracy: 88.21138211382113
Method name: topk_entropy, running accuracy: 88.21138211382113
Method name: window_entropy, running accuracy: 89.02439024390245

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 245/500 [21:26:20<22:34:50, 318.79s/it, attention_weighted_confidence_acc=89.43%, cer_entropy_weighted_mean_all_acc=89.43%, cer_prob_product_log_last_acc=88.21%, self_consistency_acc=89.84%, p_true_acc=90.24%, normilized_likelihood_acc=88.62%, normilized_entropy_acc=88.21%, topk_entropy_acc=88.21%, window_entropy_acc=89.02%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 246/500 [21:26:20<20:03:13, 284.23s/it, attention_weighted_confidence_acc=89.43%, cer_entropy_weighted_mean_all_acc=89.43%, cer_prob_product_log_last_acc=88.21%, self_consistency_acc=89.84%, p_true_acc=90.24%, normilized_likelihood_acc=88.62%, normilized_entropy_acc=88.21%, topk_entropy_acc=88.21%, window_entropy_acc=89.02%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 7.030062378697493
    Answer: 5760
    Ground truth:  5760
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 7.030062378697493
    Answer: 5760
    Ground truth:  5760
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 6.946778627656528
    Answer: 5760
    Ground truth:  5760
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 0.875
    Answer: 5760
    Ground truth:  5760
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 11.5634765625
    Answer: 5760
    Ground truth:  5760
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 3.849122405052185
    Answer: 5760
    Ground truth:  5760
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 3.8798154443502426
    Answer: 5760
    Ground truth:  5760
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 3.6010373383760452
    Answer: 5760
    Ground truth:  5760
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Convert 10 years...
    Score: 14.750734150409698
    Answer: 5760
    Ground truth:  5760
Method name: attention_weighted_confidence, running accuracy: 89.47368421052632
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.47368421052632
Method name: cer_prob_product_log_last, running accuracy: 88.25910931174089
Method name: self_consistency, running accuracy: 89.87854251012146
Method name: p_true, running accuracy: 90.2834008097166
Method name: normilized_likelihood, running accuracy: 88.66396761133603
Method name: normilized_entropy, running accuracy: 88.25910931174089
Method name: topk_entropy, running accuracy: 88.25910931174089
Method name: window_entropy, running accuracy: 89.06882591093117

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 246/500 [21:32:54<20:03:13, 284.23s/it, attention_weighted_confidence_acc=89.47%, cer_entropy_weighted_mean_all_acc=89.47%, cer_prob_product_log_last_acc=88.26%, self_consistency_acc=89.88%, p_true_acc=90.28%, normilized_likelihood_acc=88.66%, normilized_entropy_acc=88.26%, topk_entropy_acc=88.26%, window_entropy_acc=89.07%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 247/500 [21:32:54<22:17:47, 317.26s/it, attention_weighted_confidence_acc=89.47%, cer_entropy_weighted_mean_all_acc=89.47%, cer_prob_product_log_last_acc=88.26%, self_consistency_acc=89.88%, p_true_acc=90.28%, normilized_likelihood_acc=88.66%, normilized_entropy_acc=88.26%, topk_entropy_acc=88.26%, window_entropy_acc=89.07%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 8.984384771491117
    Answer: 47
    Ground truth:  47
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 8.984384771491117
    Answer: 47
    Ground truth:  47
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 15.892740249633789
    Answer: 47
    Ground truth:  47
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 1.0
    Answer: 47
    Ground truth:  47
Method 5: p_true
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 14.1640625
    Answer: 47
    Ground truth:  47
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 3.7735332772135735
    Answer: 47
    Ground truth:  47
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 3.2614258602261543
    Answer: 47
    Ground truth:  47
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 3.1659227460622787
    Answer: 47
    Ground truth:  47
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of pets Larry has, we need to add up the number of each...
    Score: 10.299497961997986
    Answer: 47
    Ground truth:  47
Method name: attention_weighted_confidence, running accuracy: 89.51612903225806
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.51612903225806
Method name: cer_prob_product_log_last, running accuracy: 88.30645161290323
Method name: self_consistency, running accuracy: 89.91935483870968
Method name: p_true, running accuracy: 90.32258064516128
Method name: normilized_likelihood, running accuracy: 88.70967741935483
Method name: normilized_entropy, running accuracy: 88.30645161290323
Method name: topk_entropy, running accuracy: 88.30645161290323
Method name: window_entropy, running accuracy: 89.11290322580645

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 247/500 [21:38:28<22:17:47, 317.26s/it, attention_weighted_confidence_acc=89.52%, cer_entropy_weighted_mean_all_acc=89.52%, cer_prob_product_log_last_acc=88.31%, self_consistency_acc=89.92%, p_true_acc=90.32%, normilized_likelihood_acc=88.71%, normilized_entropy_acc=88.31%, topk_entropy_acc=88.31%, window_entropy_acc=89.11%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|████▉     | 248/500 [21:38:28<22:32:55, 322.12s/it, attention_weighted_confidence_acc=89.52%, cer_entropy_weighted_mean_all_acc=89.52%, cer_prob_product_log_last_acc=88.31%, self_consistency_acc=89.92%, p_true_acc=90.32%, normilized_likelihood_acc=88.71%, normilized_entropy_acc=88.31%, topk_entropy_acc=88.31%, window_entropy_acc=89.11%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 5.635497341477697
    Answer: 13
    Ground truth:  13
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 5.635497341477697
    Answer: 13
    Ground truth:  13
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 10.99998652935028
    Answer: 13
    Ground truth:  13
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 0.6875
    Answer: 13
    Ground truth:  13
Method 5: p_true
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 9.53125
    Answer: 13
    Ground truth:  13
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 1.900870531797409
    Answer: 13
    Ground truth:  13
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 0.9838670194149017
    Answer: 13
    Ground truth:  13
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 0.9747051000595093
    Answer: 13
    Ground truth:  13
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Janet has left, we need to calculate the total amount she s...
    Score: 5.889306366443634
    Answer: 13
    Ground truth:  13
Method name: attention_weighted_confidence, running accuracy: 89.5582329317269
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.5582329317269
Method name: cer_prob_product_log_last, running accuracy: 88.35341365461848
Method name: self_consistency, running accuracy: 89.95983935742971
Method name: p_true, running accuracy: 90.36144578313254
Method name: normilized_likelihood, running accuracy: 88.75502008032129
Method name: normilized_entropy, running accuracy: 88.35341365461848
Method name: topk_entropy, running accuracy: 88.35341365461848
Method name: window_entropy, running accuracy: 89.1566265060241

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|████▉     | 248/500 [21:42:04<22:32:55, 322.12s/it, attention_weighted_confidence_acc=89.56%, cer_entropy_weighted_mean_all_acc=89.56%, cer_prob_product_log_last_acc=88.35%, self_consistency_acc=89.96%, p_true_acc=90.36%, normilized_likelihood_acc=88.76%, normilized_entropy_acc=88.35%, topk_entropy_acc=88.35%, window_entropy_acc=89.16%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|████▉     | 249/500 [21:42:04<20:14:45, 290.38s/it, attention_weighted_confidence_acc=89.56%, cer_entropy_weighted_mean_all_acc=89.56%, cer_prob_product_log_last_acc=88.35%, self_consistency_acc=89.96%, p_true_acc=90.36%, normilized_likelihood_acc=88.76%, normilized_entropy_acc=88.35%, topk_entropy_acc=88.35%, window_entropy_acc=89.16%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 6.558788979400367
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 6.558788979400367
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 12.794275641441345
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 0.8125
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 10.828125
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 3.347247913479805
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 2.1282913833856583
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 2.0714266151189804
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Shawnda made, we need to calculate the total number of tire...
    Score: 11.432410955429077
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 89.60000000000001
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.60000000000001
Method name: cer_prob_product_log_last, running accuracy: 88.4
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.4
Method name: normilized_likelihood, running accuracy: 88.8
Method name: normilized_entropy, running accuracy: 88.4
Method name: topk_entropy, running accuracy: 88.4
Method name: window_entropy, running accuracy: 89.2

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|████▉     | 249/500 [21:47:46<20:14:45, 290.38s/it, attention_weighted_confidence_acc=89.60%, cer_entropy_weighted_mean_all_acc=89.60%, cer_prob_product_log_last_acc=88.40%, self_consistency_acc=90.00%, p_true_acc=90.40%, normilized_likelihood_acc=88.80%, normilized_entropy_acc=88.40%, topk_entropy_acc=88.40%, window_entropy_acc=89.20%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 250/500 [21:47:46<21:14:00, 305.76s/it, attention_weighted_confidence_acc=89.60%, cer_entropy_weighted_mean_all_acc=89.60%, cer_prob_product_log_last_acc=88.40%, self_consistency_acc=90.00%, p_true_acc=90.40%, normilized_likelihood_acc=88.80%, normilized_entropy_acc=88.40%, topk_entropy_acc=88.40%, window_entropy_acc=89.20%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 5.439523279923041
    Answer: 3200
    Ground truth:  3200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 5.439523279923041
    Answer: 3200
    Ground truth:  3200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 5.403468007835119
    Answer: 3200
    Ground truth:  3200
Method 4: self_consistency
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 0.6875
    Answer: 3200
    Ground truth:  3200
Method 5: p_true
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 9.8671875
    Answer: 3200
    Ground truth:  3200
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 4.2637180387973785
    Answer: 3200
    Ground truth:  3200
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 3.8801232874393463
    Answer: 3200
    Ground truth:  3200
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 3.529512971639633
    Answer: 3200
    Ground truth:  3200
Method 9: window_entropy
  Batch 1:
    Text: Step 1: First, we need to calculate how many hours Watson works in a week. He wo...
    Score: 10.447267591953278
    Answer: 3200
    Ground truth:  3200
Method name: attention_weighted_confidence, running accuracy: 89.64143426294821
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.64143426294821
Method name: cer_prob_product_log_last, running accuracy: 88.44621513944223
Method name: self_consistency, running accuracy: 90.0398406374502
Method name: p_true, running accuracy: 90.43824701195219
Method name: normilized_likelihood, running accuracy: 88.84462151394422
Method name: normilized_entropy, running accuracy: 88.44621513944223
Method name: topk_entropy, running accuracy: 88.44621513944223
Method name: window_entropy, running accuracy: 89.2430278884462

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 250/500 [21:52:55<21:14:00, 305.76s/it, attention_weighted_confidence_acc=89.64%, cer_entropy_weighted_mean_all_acc=89.64%, cer_prob_product_log_last_acc=88.45%, self_consistency_acc=90.04%, p_true_acc=90.44%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.45%, topk_entropy_acc=88.45%, window_entropy_acc=89.24%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 251/500 [21:52:55<21:12:52, 306.72s/it, attention_weighted_confidence_acc=89.64%, cer_entropy_weighted_mean_all_acc=89.64%, cer_prob_product_log_last_acc=88.45%, self_consistency_acc=90.04%, p_true_acc=90.44%, normilized_likelihood_acc=88.84%, normilized_entropy_acc=88.45%, topk_entropy_acc=88.45%, window_entropy_acc=89.24%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 7.631872745370475
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 7.631872745370475
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 14.998216927051544
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 0.9375
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 14.63671875
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 7.5976580157876015
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 6.623755216598511
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 5.967888534069061
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of cupcakes Mary can decorate with the sprinkles, we ne...
    Score: 19.478780776262283
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.68253968253968
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.68253968253968
Method name: cer_prob_product_log_last, running accuracy: 88.4920634920635
Method name: self_consistency, running accuracy: 90.07936507936508
Method name: p_true, running accuracy: 90.47619047619048
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.4920634920635
Method name: topk_entropy, running accuracy: 88.4920634920635
Method name: window_entropy, running accuracy: 89.28571428571429

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 251/500 [21:56:39<21:12:52, 306.72s/it, attention_weighted_confidence_acc=89.68%, cer_entropy_weighted_mean_all_acc=89.68%, cer_prob_product_log_last_acc=88.49%, self_consistency_acc=90.08%, p_true_acc=90.48%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.49%, topk_entropy_acc=88.49%, window_entropy_acc=89.29%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 252/500 [21:56:39<19:25:10, 281.90s/it, attention_weighted_confidence_acc=89.68%, cer_entropy_weighted_mean_all_acc=89.68%, cer_prob_product_log_last_acc=88.49%, self_consistency_acc=90.08%, p_true_acc=90.48%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.49%, topk_entropy_acc=88.49%, window_entropy_acc=89.29%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 8.192452194324332
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 8.192452194324332
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 15.780896663665771
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 1.0
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 15.2890625
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 1.9457023590803146
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 1.265764206647873
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 1.2641866207122803
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find the size of Peter's locker, we need to calculate the size of Zack's lock...
    Score: 4.197712689638138
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 89.72332015810277
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.72332015810277
Method name: cer_prob_product_log_last, running accuracy: 88.53754940711462
Method name: self_consistency, running accuracy: 90.11857707509881
Method name: p_true, running accuracy: 90.51383399209486
Method name: normilized_likelihood, running accuracy: 88.93280632411067
Method name: normilized_entropy, running accuracy: 88.53754940711462
Method name: topk_entropy, running accuracy: 88.53754940711462
Method name: window_entropy, running accuracy: 89.32806324110672

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 252/500 [21:59:39<19:25:10, 281.90s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=90.12%, p_true_acc=90.51%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.54%, topk_entropy_acc=88.54%, window_entropy_acc=89.33%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 253/500 [21:59:39<17:15:07, 251.45s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=90.12%, p_true_acc=90.51%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.54%, topk_entropy_acc=88.54%, window_entropy_acc=89.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 6.513055348525285
    Answer: 210
    Ground truth:  210
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 6.513055348525285
    Answer: 210
    Ground truth:  210
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 12.998942375183105
    Answer: 210
    Ground truth:  210
Method 4: self_consistency
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 0.8125
    Answer: 210
    Ground truth:  210
Method 5: p_true
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 11.45703125
    Answer: 210
    Ground truth:  210
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 3.0462375581264496
    Answer: 210
    Ground truth:  210
Method 7: normilized_entropy
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 1.762210950255394
    Answer: 210
    Ground truth:  210
Method 8: topk_entropy
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 1.6994287818670273
    Answer: 210
    Ground truth:  210
Method 9: window_entropy
  Batch 1:
    Text: Let's analyze the situation step by step.

1. In the first 30 seconds, 20 kernel...
    Score: 11.81902927160263
    Answer: 210
    Ground truth:  210
Method name: attention_weighted_confidence, running accuracy: 89.76377952755905
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.76377952755905
Method name: cer_prob_product_log_last, running accuracy: 88.58267716535433
Method name: self_consistency, running accuracy: 90.15748031496062
Method name: p_true, running accuracy: 90.5511811023622
Method name: normilized_likelihood, running accuracy: 88.9763779527559
Method name: normilized_entropy, running accuracy: 88.58267716535433
Method name: topk_entropy, running accuracy: 88.58267716535433
Method name: window_entropy, running accuracy: 89.37007874015748

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 253/500 [22:06:11<17:15:07, 251.45s/it, attention_weighted_confidence_acc=89.76%, cer_entropy_weighted_mean_all_acc=89.76%, cer_prob_product_log_last_acc=88.58%, self_consistency_acc=90.16%, p_true_acc=90.55%, normilized_likelihood_acc=88.98%, normilized_entropy_acc=88.58%, topk_entropy_acc=88.58%, window_entropy_acc=89.37%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 254/500 [22:06:11<20:03:06, 293.44s/it, attention_weighted_confidence_acc=89.76%, cer_entropy_weighted_mean_all_acc=89.76%, cer_prob_product_log_last_acc=88.58%, self_consistency_acc=90.16%, p_true_acc=90.55%, normilized_likelihood_acc=88.98%, normilized_entropy_acc=88.58%, topk_entropy_acc=88.58%, window_entropy_acc=89.37%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 8.113409580197235
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 8.113409580197235
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 15.999489963054657
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 13.5732421875
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 1.458001434803009
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 0.9959905445575714
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 0.9839512854814529
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find out how many packs of apples Uncle Franky can make, we need to follow th...
    Score: 5.127628356218338
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 89.80392156862746
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.80392156862746
Method name: cer_prob_product_log_last, running accuracy: 88.62745098039215
Method name: self_consistency, running accuracy: 90.19607843137256
Method name: p_true, running accuracy: 90.58823529411765
Method name: normilized_likelihood, running accuracy: 89.01960784313725
Method name: normilized_entropy, running accuracy: 88.62745098039215
Method name: topk_entropy, running accuracy: 88.62745098039215
Method name: window_entropy, running accuracy: 89.41176470588236

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 254/500 [22:09:44<20:03:06, 293.44s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=88.63%, self_consistency_acc=90.20%, p_true_acc=90.59%, normilized_likelihood_acc=89.02%, normilized_entropy_acc=88.63%, topk_entropy_acc=88.63%, window_entropy_acc=89.41%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 255/500 [22:09:44<18:20:12, 269.44s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=88.63%, self_consistency_acc=90.20%, p_true_acc=90.59%, normilized_likelihood_acc=89.02%, normilized_entropy_acc=88.63%, topk_entropy_acc=88.63%, window_entropy_acc=89.41%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 8.497199493484946
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 8.497199493484946
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 15.999786615371704
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 1.0
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 10.47265625
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 2.4800212532281876
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 2.128995180130005
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 2.1264133155345917
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To find out how many more petals the daisies have compared to the orchids, we fi...
    Score: 3.4765921533107758
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 89.84375
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.84375
Method name: cer_prob_product_log_last, running accuracy: 88.671875
Method name: self_consistency, running accuracy: 90.234375
Method name: p_true, running accuracy: 90.625
Method name: normilized_likelihood, running accuracy: 89.0625
Method name: normilized_entropy, running accuracy: 88.671875
Method name: topk_entropy, running accuracy: 88.671875
Method name: window_entropy, running accuracy: 89.453125

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 255/500 [22:13:49<18:20:12, 269.44s/it, attention_weighted_confidence_acc=89.84%, cer_entropy_weighted_mean_all_acc=89.84%, cer_prob_product_log_last_acc=88.67%, self_consistency_acc=90.23%, p_true_acc=90.62%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.67%, topk_entropy_acc=88.67%, window_entropy_acc=89.45%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 256/500 [22:13:49<17:45:52, 262.10s/it, attention_weighted_confidence_acc=89.84%, cer_entropy_weighted_mean_all_acc=89.84%, cer_prob_product_log_last_acc=88.67%, self_consistency_acc=90.23%, p_true_acc=90.62%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.67%, topk_entropy_acc=88.67%, window_entropy_acc=89.45%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 8.081421386119926
    Answer: 120
    Ground truth:  120
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 8.081421386119926
    Answer: 120
    Ground truth:  120
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 15.99989104270935
    Answer: 120
    Ground truth:  120
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 1.0
    Answer: 120
    Ground truth:  120
Method 5: p_true
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 13.40625
    Answer: 120
    Ground truth:  120
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 3.480363443493843
    Answer: 120
    Ground truth:  120
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 2.523022174835205
    Answer: 120
    Ground truth:  120
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 2.4554905146360397
    Answer: 120
    Ground truth:  120
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Karen earns, we first need to find out how much Lorie earns...
    Score: 10.244812041521072
    Answer: 120
    Ground truth:  120
Method name: attention_weighted_confidence, running accuracy: 89.88326848249028
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.88326848249028
Method name: cer_prob_product_log_last, running accuracy: 88.715953307393
Method name: self_consistency, running accuracy: 90.27237354085604
Method name: p_true, running accuracy: 90.6614785992218
Method name: normilized_likelihood, running accuracy: 89.10505836575877
Method name: normilized_entropy, running accuracy: 88.715953307393
Method name: topk_entropy, running accuracy: 88.715953307393
Method name: window_entropy, running accuracy: 89.49416342412452

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 256/500 [22:17:21<17:45:52, 262.10s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=90.27%, p_true_acc=90.66%, normilized_likelihood_acc=89.11%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.49%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████▏    | 257/500 [22:17:21<16:40:59, 247.16s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=90.27%, p_true_acc=90.66%, normilized_likelihood_acc=89.11%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.49%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 8.04632337008583
    Answer: 40
    Ground truth:  40
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 8.04632337008583
    Answer: 40
    Ground truth:  40
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 15.999926090240479
    Answer: 40
    Ground truth:  40
Method 4: self_consistency
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 1.0
    Answer: 40
    Ground truth:  40
Method 5: p_true
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 14.2421875
    Answer: 40
    Ground truth:  40
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 2.48116335272789
    Answer: 40
    Ground truth:  40
Method 7: normilized_entropy
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 1.6531567424535751
    Answer: 40
    Ground truth:  40
Method 8: topk_entropy
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 1.640078067779541
    Answer: 40
    Ground truth:  40
Method 9: window_entropy
  Batch 1:
    Text: To find the difference in the number of songs Luri and Gabriel can add to their ...
    Score: 4.527271211147308
    Answer: 40
    Ground truth:  40
Method name: attention_weighted_confidence, running accuracy: 89.92248062015504
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.92248062015504
Method name: cer_prob_product_log_last, running accuracy: 88.75968992248062
Method name: self_consistency, running accuracy: 90.31007751937985
Method name: p_true, running accuracy: 90.69767441860465
Method name: normilized_likelihood, running accuracy: 89.14728682170544
Method name: normilized_entropy, running accuracy: 88.75968992248062
Method name: topk_entropy, running accuracy: 88.75968992248062
Method name: window_entropy, running accuracy: 89.53488372093024

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████▏    | 257/500 [22:21:47<16:40:59, 247.16s/it, attention_weighted_confidence_acc=89.92%, cer_entropy_weighted_mean_all_acc=89.92%, cer_prob_product_log_last_acc=88.76%, self_consistency_acc=90.31%, p_true_acc=90.70%, normilized_likelihood_acc=89.15%, normilized_entropy_acc=88.76%, topk_entropy_acc=88.76%, window_entropy_acc=89.53%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 258/500 [22:21:47<16:59:17, 252.72s/it, attention_weighted_confidence_acc=89.92%, cer_entropy_weighted_mean_all_acc=89.92%, cer_prob_product_log_last_acc=88.76%, self_consistency_acc=90.31%, p_true_acc=90.70%, normilized_likelihood_acc=89.15%, normilized_entropy_acc=88.76%, topk_entropy_acc=88.76%, window_entropy_acc=89.53%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 8.609805483208094
    Answer: 27
    Ground truth:  27
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 8.609805483208094
    Answer: 27
    Ground truth:  27
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 15.999927759170532
    Answer: 27
    Ground truth:  27
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 1.0
    Answer: 27
    Ground truth:  27
Method 5: p_true
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 12.53125
    Answer: 27
    Ground truth:  27
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 1.7097859904170036
    Answer: 27
    Ground truth:  27
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 1.3790860921144485
    Answer: 27
    Ground truth:  27
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 1.3705805093050003
    Answer: 27
    Ground truth:  27
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Daniel has to spend on all the items, we need to calculate ...
    Score: 7.5184608697891235
    Answer: 27
    Ground truth:  27
Method name: attention_weighted_confidence, running accuracy: 89.96138996138995
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.96138996138995
Method name: cer_prob_product_log_last, running accuracy: 88.8030888030888
Method name: self_consistency, running accuracy: 90.34749034749035
Method name: p_true, running accuracy: 90.73359073359073
Method name: normilized_likelihood, running accuracy: 89.1891891891892
Method name: normilized_entropy, running accuracy: 88.8030888030888
Method name: topk_entropy, running accuracy: 88.8030888030888
Method name: window_entropy, running accuracy: 89.57528957528957

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 258/500 [22:27:58<16:59:17, 252.72s/it, attention_weighted_confidence_acc=89.96%, cer_entropy_weighted_mean_all_acc=89.96%, cer_prob_product_log_last_acc=88.80%, self_consistency_acc=90.35%, p_true_acc=90.73%, normilized_likelihood_acc=89.19%, normilized_entropy_acc=88.80%, topk_entropy_acc=88.80%, window_entropy_acc=89.58%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 259/500 [22:27:58<19:17:20, 288.14s/it, attention_weighted_confidence_acc=89.96%, cer_entropy_weighted_mean_all_acc=89.96%, cer_prob_product_log_last_acc=88.80%, self_consistency_acc=90.35%, p_true_acc=90.73%, normilized_likelihood_acc=89.19%, normilized_entropy_acc=88.80%, topk_entropy_acc=88.80%, window_entropy_acc=89.58%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Olivia put the same...
    Score: 1.639646518831277
    Answer: 18
    Ground truth:  45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Olivia put the same...
    Score: 1.639646518831277
    Answer: 18
    Ground truth:  45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Olivia put the same...
    Score: 3.900254011154175
    Answer: 18
    Ground truth:  45
Method 4: self_consistency
  Batch 1:
    Text: To determine the number of portraits and selfies, we can follow the given inform...
    Score: 0.25
    Answer: 45
    Ground truth:  45
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Olivia put the same...
    Score: 3.30078125
    Answer: 18
    Ground truth:  45
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Olivia put the same...
    Score: 4.815901219844818
    Answer: 18
    Ground truth:  45
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Olivia put the same...
    Score: 4.53495517373085
    Answer: 18
    Ground truth:  45
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Olivia put the same...
    Score: 3.7207024097442627
    Answer: 18
    Ground truth:  45
Method 9: window_entropy
  Batch 1:
    Text: To determine the number of portraits and selfies, we can follow the given inform...
    Score: 4.107958674430847
    Answer: 45
    Ground truth:  45
Method name: attention_weighted_confidence, running accuracy: 89.61538461538461
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.61538461538461
Method name: cer_prob_product_log_last, running accuracy: 88.46153846153845
Method name: self_consistency, running accuracy: 90.38461538461539
Method name: p_true, running accuracy: 90.38461538461539
Method name: normilized_likelihood, running accuracy: 88.84615384615384
Method name: normilized_entropy, running accuracy: 88.46153846153845
Method name: topk_entropy, running accuracy: 88.46153846153845
Method name: window_entropy, running accuracy: 89.61538461538461

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 259/500 [22:36:59<19:17:20, 288.14s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=90.38%, p_true_acc=90.38%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.62%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 260/500 [22:36:59<24:16:19, 364.08s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=88.46%, self_consistency_acc=90.38%, p_true_acc=90.38%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.62%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 8.196283569828331
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 8.196283569828331
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 15.993397772312164
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 1.0
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 14.6171875
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 1.5774669796228409
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 0.9261292815208435
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 0.9258780032396317
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find the cost to fill the pool, we first need to find the volume of the pool ...
    Score: 2.014005720615387
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 89.65517241379311
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.65517241379311
Method name: cer_prob_product_log_last, running accuracy: 88.50574712643679
Method name: self_consistency, running accuracy: 90.42145593869732
Method name: p_true, running accuracy: 90.42145593869732
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.50574712643679
Method name: topk_entropy, running accuracy: 88.50574712643679
Method name: window_entropy, running accuracy: 89.65517241379311

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 260/500 [22:40:46<24:16:19, 364.08s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=88.51%, self_consistency_acc=90.42%, p_true_acc=90.42%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.51%, topk_entropy_acc=88.51%, window_entropy_acc=89.66%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 261/500 [22:40:46<21:26:37, 323.00s/it, attention_weighted_confidence_acc=89.66%, cer_entropy_weighted_mean_all_acc=89.66%, cer_prob_product_log_last_acc=88.51%, self_consistency_acc=90.42%, p_true_acc=90.42%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.51%, topk_entropy_acc=88.51%, window_entropy_acc=89.66%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 6.635060898168755
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 6.635060898168755
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 10.853985786437988
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 0.8125
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 11.203125
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 3.0943102166056633
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 1.6821244060993195
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 1.6697480827569962
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Finley started out in first place.
2. She ...
    Score: 3.054222494363785
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 89.69465648854961
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.69465648854961
Method name: cer_prob_product_log_last, running accuracy: 88.54961832061069
Method name: self_consistency, running accuracy: 90.45801526717557
Method name: p_true, running accuracy: 90.45801526717557
Method name: normilized_likelihood, running accuracy: 88.93129770992367
Method name: normilized_entropy, running accuracy: 88.54961832061069
Method name: topk_entropy, running accuracy: 88.54961832061069
Method name: window_entropy, running accuracy: 89.69465648854961

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 261/500 [22:43:57<21:26:37, 323.00s/it, attention_weighted_confidence_acc=89.69%, cer_entropy_weighted_mean_all_acc=89.69%, cer_prob_product_log_last_acc=88.55%, self_consistency_acc=90.46%, p_true_acc=90.46%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.55%, topk_entropy_acc=88.55%, window_entropy_acc=89.69%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 262/500 [22:43:57<18:43:54, 283.34s/it, attention_weighted_confidence_acc=89.69%, cer_entropy_weighted_mean_all_acc=89.69%, cer_prob_product_log_last_acc=88.55%, self_consistency_acc=90.46%, p_true_acc=90.46%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.55%, topk_entropy_acc=88.55%, window_entropy_acc=89.69%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 8.039880572592088
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 8.039880572592088
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 15.999865889549255
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 1.0
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 14.73046875
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 1.5636443346738815
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 0.7386348098516464
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 0.738592803478241
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: To find the total barrels of wine produced per year, we need to break down the p...
    Score: 3.1359172463417053
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 89.73384030418251
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.73384030418251
Method name: cer_prob_product_log_last, running accuracy: 88.59315589353612
Method name: self_consistency, running accuracy: 90.49429657794677
Method name: p_true, running accuracy: 90.49429657794677
Method name: normilized_likelihood, running accuracy: 88.97338403041825
Method name: normilized_entropy, running accuracy: 88.59315589353612
Method name: topk_entropy, running accuracy: 88.59315589353612
Method name: window_entropy, running accuracy: 89.73384030418251

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 262/500 [22:47:44<18:43:54, 283.34s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=88.59%, self_consistency_acc=90.49%, p_true_acc=90.49%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.59%, topk_entropy_acc=88.59%, window_entropy_acc=89.73%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 263/500 [22:47:44<17:32:09, 266.37s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=88.59%, self_consistency_acc=90.49%, p_true_acc=90.49%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.59%, topk_entropy_acc=88.59%, window_entropy_acc=89.73%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 5.821519470044575
    Answer: 11
    Ground truth:  11
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 5.821519470044575
    Answer: 11
    Ground truth:  11
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 11.070847615599632
    Answer: 11
    Ground truth:  11
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 0.75
    Answer: 11
    Ground truth:  11
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 11.078125
    Answer: 11
    Ground truth:  11
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 5.380762159824371
    Answer: 11
    Ground truth:  11
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 3.980481266975403
    Answer: 11
    Ground truth:  11
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 3.592989981174469
    Answer: 11
    Ground truth:  11
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into several steps.

1. Let's denote ...
    Score: 9.325383961200714
    Answer: 11
    Ground truth:  11
Method name: attention_weighted_confidence, running accuracy: 89.77272727272727
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.77272727272727
Method name: cer_prob_product_log_last, running accuracy: 88.63636363636364
Method name: self_consistency, running accuracy: 90.53030303030303
Method name: p_true, running accuracy: 90.53030303030303
Method name: normilized_likelihood, running accuracy: 89.01515151515152
Method name: normilized_entropy, running accuracy: 88.63636363636364
Method name: topk_entropy, running accuracy: 88.63636363636364
Method name: window_entropy, running accuracy: 89.77272727272727

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 263/500 [22:53:40<17:32:09, 266.37s/it, attention_weighted_confidence_acc=89.77%, cer_entropy_weighted_mean_all_acc=89.77%, cer_prob_product_log_last_acc=88.64%, self_consistency_acc=90.53%, p_true_acc=90.53%, normilized_likelihood_acc=89.02%, normilized_entropy_acc=88.64%, topk_entropy_acc=88.64%, window_entropy_acc=89.77%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 264/500 [22:53:40<19:14:16, 293.46s/it, attention_weighted_confidence_acc=89.77%, cer_entropy_weighted_mean_all_acc=89.77%, cer_prob_product_log_last_acc=88.64%, self_consistency_acc=90.53%, p_true_acc=90.53%, normilized_likelihood_acc=89.02%, normilized_entropy_acc=88.64%, topk_entropy_acc=88.64%, window_entropy_acc=89.77%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 7.966693789650514
    Answer: 1198
    Ground truth:  1198
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 7.966693789650514
    Answer: 1198
    Ground truth:  1198
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 7.997646411900522
    Answer: 1198
    Ground truth:  1198
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 1.0
    Answer: 1198
    Ground truth:  1198
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 14.359375
    Answer: 1198
    Ground truth:  1198
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 2.7448505014181137
    Answer: 1198
    Ground truth:  1198
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 1.1271390914916992
    Answer: 1198
    Ground truth:  1198
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 1.1295548975467682
    Answer: 1198
    Ground truth:  1198
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to break it down into steps. 

Step 1: Find the c...
    Score: 3.547061800956726
    Answer: 1198
    Ground truth:  1198
Method name: attention_weighted_confidence, running accuracy: 89.81132075471699
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.81132075471699
Method name: cer_prob_product_log_last, running accuracy: 88.67924528301887
Method name: self_consistency, running accuracy: 90.56603773584906
Method name: p_true, running accuracy: 90.56603773584906
Method name: normilized_likelihood, running accuracy: 89.05660377358491
Method name: normilized_entropy, running accuracy: 88.67924528301887
Method name: topk_entropy, running accuracy: 88.67924528301887
Method name: window_entropy, running accuracy: 89.81132075471699

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 264/500 [23:01:02<19:14:16, 293.46s/it, attention_weighted_confidence_acc=89.81%, cer_entropy_weighted_mean_all_acc=89.81%, cer_prob_product_log_last_acc=88.68%, self_consistency_acc=90.57%, p_true_acc=90.57%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.68%, topk_entropy_acc=88.68%, window_entropy_acc=89.81%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 265/500 [23:01:02<22:02:53, 337.76s/it, attention_weighted_confidence_acc=89.81%, cer_entropy_weighted_mean_all_acc=89.81%, cer_prob_product_log_last_acc=88.68%, self_consistency_acc=90.57%, p_true_acc=90.57%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.68%, topk_entropy_acc=88.68%, window_entropy_acc=89.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 8.352610986342567
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 8.352610986342567
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 15.99695873260498
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 1.0
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 10.3134765625
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 1.9462401121854782
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 1.8594002425670624
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 1.8428347557783127
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: To find out how many roses Ford lacks to supply all the flower shops, we need to...
    Score: 4.37306672334671
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 89.84962406015038
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.84962406015038
Method name: cer_prob_product_log_last, running accuracy: 88.7218045112782
Method name: self_consistency, running accuracy: 90.6015037593985
Method name: p_true, running accuracy: 90.6015037593985
Method name: normilized_likelihood, running accuracy: 89.09774436090225
Method name: normilized_entropy, running accuracy: 88.7218045112782
Method name: topk_entropy, running accuracy: 88.7218045112782
Method name: window_entropy, running accuracy: 89.84962406015038

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 265/500 [23:05:59<22:02:53, 337.76s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=90.60%, p_true_acc=90.60%, normilized_likelihood_acc=89.10%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.85%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 266/500 [23:05:59<21:09:50, 325.60s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=90.60%, p_true_acc=90.60%, normilized_likelihood_acc=89.10%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 5.267679418137822
    Answer: 84
    Ground truth:  84
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 5.267679418137822
    Answer: 84
    Ground truth:  84
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 10.56804072856903
    Answer: 84
    Ground truth:  84
Method 4: self_consistency
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 0.6875
    Answer: 84
    Ground truth:  84
Method 5: p_true
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 9.081787109375
    Answer: 84
    Ground truth:  84
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 11.387125134468079
    Answer: 84
    Ground truth:  84
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 12.217150688171387
    Answer: 84
    Ground truth:  84
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 10.479127019643784
    Answer: 84
    Ground truth:  84
Method 9: window_entropy
  Batch 1:
    Text: To find the number of grapes at the beginning, we need to work step by step.

St...
    Score: 10.843803822994232
    Answer: 84
    Ground truth:  84
Method name: attention_weighted_confidence, running accuracy: 89.8876404494382
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.8876404494382
Method name: cer_prob_product_log_last, running accuracy: 88.76404494382022
Method name: self_consistency, running accuracy: 90.63670411985018
Method name: p_true, running accuracy: 90.63670411985018
Method name: normilized_likelihood, running accuracy: 89.13857677902621
Method name: normilized_entropy, running accuracy: 88.76404494382022
Method name: topk_entropy, running accuracy: 88.76404494382022
Method name: window_entropy, running accuracy: 89.8876404494382

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 266/500 [23:13:02<21:09:50, 325.60s/it, attention_weighted_confidence_acc=89.89%, cer_entropy_weighted_mean_all_acc=89.89%, cer_prob_product_log_last_acc=88.76%, self_consistency_acc=90.64%, p_true_acc=90.64%, normilized_likelihood_acc=89.14%, normilized_entropy_acc=88.76%, topk_entropy_acc=88.76%, window_entropy_acc=89.89%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 267/500 [23:13:02<22:58:37, 355.01s/it, attention_weighted_confidence_acc=89.89%, cer_entropy_weighted_mean_all_acc=89.89%, cer_prob_product_log_last_acc=88.76%, self_consistency_acc=90.64%, p_true_acc=90.64%, normilized_likelihood_acc=89.14%, normilized_entropy_acc=88.76%, topk_entropy_acc=88.76%, window_entropy_acc=89.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, Brenda i...
    Score: 1.5261641225452645
    Answer: 1606.81
    Ground truth:  975
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, Brenda i...
    Score: 1.5261641225452645
    Answer: 1606.81
    Ground truth:  975
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount Brenda will have after 3 years with simple interest, we...
    Score: 0.9999881982803345
    Answer: 975
    Ground truth:  975
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll calculate the interest earned each year and add it ...
    Score: 0.1875
    Answer: 1606.31
    Ground truth:  975
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll calculate the interest earned each year and add it ...
    Score: 1.5625
    Answer: 1606.31
    Ground truth:  975
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, Brenda i...
    Score: 2.518525779247284
    Answer: 1606.81
    Ground truth:  975
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, Brenda i...
    Score: 2.200576677918434
    Answer: 1606.81
    Ground truth:  975
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, Brenda i...
    Score: 1.9323410987854004
    Answer: 1606.81
    Ground truth:  975
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Initially, Brenda i...
    Score: 4.225710988044739
    Answer: 1606.81
    Ground truth:  975
Method name: attention_weighted_confidence, running accuracy: 89.55223880597015
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.55223880597015
Method name: cer_prob_product_log_last, running accuracy: 88.80597014925374
Method name: self_consistency, running accuracy: 90.29850746268657
Method name: p_true, running accuracy: 90.29850746268657
Method name: normilized_likelihood, running accuracy: 88.80597014925374
Method name: normilized_entropy, running accuracy: 88.43283582089553
Method name: topk_entropy, running accuracy: 88.43283582089553
Method name: window_entropy, running accuracy: 89.55223880597015

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 267/500 [23:18:51<22:58:37, 355.01s/it, attention_weighted_confidence_acc=89.55%, cer_entropy_weighted_mean_all_acc=89.55%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=90.30%, p_true_acc=90.30%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.43%, topk_entropy_acc=88.43%, window_entropy_acc=89.55%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▎    | 268/500 [23:18:51<22:45:42, 353.20s/it, attention_weighted_confidence_acc=89.55%, cer_entropy_weighted_mean_all_acc=89.55%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=90.30%, p_true_acc=90.30%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.43%, topk_entropy_acc=88.43%, window_entropy_acc=89.55%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 7.392636522006249
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 7.392636522006249
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 12.969326414167881
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 0.9375
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 13.625
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 10.788578450679779
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 9.683767929673195
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 8.785453617572784
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: To find out how many pints of gasoline Josey needed, we need to break down the p...
    Score: 16.379413962364197
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 89.59107806691449
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.59107806691449
Method name: cer_prob_product_log_last, running accuracy: 88.84758364312268
Method name: self_consistency, running accuracy: 90.33457249070632
Method name: p_true, running accuracy: 90.33457249070632
Method name: normilized_likelihood, running accuracy: 88.84758364312268
Method name: normilized_entropy, running accuracy: 88.47583643122677
Method name: topk_entropy, running accuracy: 88.47583643122677
Method name: window_entropy, running accuracy: 89.59107806691449

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▎    | 268/500 [23:24:01<22:45:42, 353.20s/it, attention_weighted_confidence_acc=89.59%, cer_entropy_weighted_mean_all_acc=89.59%, cer_prob_product_log_last_acc=88.85%, self_consistency_acc=90.33%, p_true_acc=90.33%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.59%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 269/500 [23:24:01<21:49:36, 340.16s/it, attention_weighted_confidence_acc=89.59%, cer_entropy_weighted_mean_all_acc=89.59%, cer_prob_product_log_last_acc=88.85%, self_consistency_acc=90.33%, p_true_acc=90.33%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.48%, topk_entropy_acc=88.48%, window_entropy_acc=89.59%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 7.874155176963215
    Answer: 1
    Ground truth:  1
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 7.874155176963215
    Answer: 1
    Ground truth:  1
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 12.316847771406174
    Answer: 1
    Ground truth:  1
Method 4: self_consistency
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 0.9375
    Answer: 1
    Ground truth:  1
Method 5: p_true
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 12.0625
    Answer: 1
    Ground truth:  1
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 6.265213131904602
    Answer: 1
    Ground truth:  1
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 7.964159041643143
    Answer: 1
    Ground truth:  1
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 7.284169405698776
    Answer: 1
    Ground truth:  1
Method 9: window_entropy
  Batch 1:
    Text: To find out how much more money Dora needs, we first need to add up the total co...
    Score: 23.40784665942192
    Answer: 1
    Ground truth:  1
Method name: attention_weighted_confidence, running accuracy: 89.62962962962962
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.62962962962962
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 90.37037037037037
Method name: p_true, running accuracy: 90.37037037037037
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.51851851851852
Method name: topk_entropy, running accuracy: 88.51851851851852
Method name: window_entropy, running accuracy: 89.62962962962962

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 269/500 [23:28:07<21:49:36, 340.16s/it, attention_weighted_confidence_acc=89.63%, cer_entropy_weighted_mean_all_acc=89.63%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.37%, p_true_acc=90.37%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.52%, topk_entropy_acc=88.52%, window_entropy_acc=89.63%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 270/500 [23:28:07<19:55:34, 311.89s/it, attention_weighted_confidence_acc=89.63%, cer_entropy_weighted_mean_all_acc=89.63%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.37%, p_true_acc=90.37%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.52%, topk_entropy_acc=88.52%, window_entropy_acc=89.63%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 7.53115700601359
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 7.53115700601359
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 14.759799003601074
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 0.9375
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 11.66015625
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 23.01977004110813
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 21.765018850564957
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 18.526654824614525
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To find out how many apple slices are left, we need to follow these steps:

1. F...
    Score: 27.068817675113678
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 89.66789667896678
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.66789667896678
Method name: cer_prob_product_log_last, running accuracy: 88.92988929889299
Method name: self_consistency, running accuracy: 90.40590405904058
Method name: p_true, running accuracy: 90.40590405904058
Method name: normilized_likelihood, running accuracy: 88.92988929889299
Method name: normilized_entropy, running accuracy: 88.56088560885608
Method name: topk_entropy, running accuracy: 88.56088560885608
Method name: window_entropy, running accuracy: 89.66789667896678

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 270/500 [23:32:44<19:55:34, 311.89s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.93%, self_consistency_acc=90.41%, p_true_acc=90.41%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.56%, topk_entropy_acc=88.56%, window_entropy_acc=89.67%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 271/500 [23:32:44<19:10:06, 301.34s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.93%, self_consistency_acc=90.41%, p_true_acc=90.41%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.56%, topk_entropy_acc=88.56%, window_entropy_acc=89.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 6.650013827468758
    Answer: 70
    Ground truth:  70
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 6.650013827468758
    Answer: 70
    Ground truth:  70
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 12.95447301864624
    Answer: 70
    Ground truth:  70
Method 4: self_consistency
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 0.8125
    Answer: 70
    Ground truth:  70
Method 5: p_true
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 9.3671875
    Answer: 70
    Ground truth:  70
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 1.5233654528856277
    Answer: 70
    Ground truth:  70
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 0.9939613044261932
    Answer: 70
    Ground truth:  70
Method 8: topk_entropy
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 0.9832957684993744
    Answer: 70
    Ground truth:  70
Method 9: window_entropy
  Batch 1:
    Text: To solve this, let's break down the information step by step:

1. There are 20 h...
    Score: 3.636587679386139
    Answer: 70
    Ground truth:  70
Method name: attention_weighted_confidence, running accuracy: 89.70588235294117
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.70588235294117
Method name: cer_prob_product_log_last, running accuracy: 88.97058823529412
Method name: self_consistency, running accuracy: 90.44117647058823
Method name: p_true, running accuracy: 90.44117647058823
Method name: normilized_likelihood, running accuracy: 88.97058823529412
Method name: normilized_entropy, running accuracy: 88.60294117647058
Method name: topk_entropy, running accuracy: 88.60294117647058
Method name: window_entropy, running accuracy: 89.70588235294117

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 271/500 [23:36:28<19:10:06, 301.34s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=90.44%, p_true_acc=90.44%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.60%, topk_entropy_acc=88.60%, window_entropy_acc=89.71%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 272/500 [23:36:28<17:37:14, 278.22s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=90.44%, p_true_acc=90.44%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.60%, topk_entropy_acc=88.60%, window_entropy_acc=89.71%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 8.6554055289684
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 8.6554055289684
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 15.999775171279907
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 15.81640625
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 2.1195749416947365
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 1.5210020840168
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 1.5152433216571808
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Bethany can run 10 laps in one hour.
2. ...
    Score: 7.433777302503586
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 89.74358974358975
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.74358974358975
Method name: cer_prob_product_log_last, running accuracy: 89.01098901098901
Method name: self_consistency, running accuracy: 90.47619047619048
Method name: p_true, running accuracy: 90.47619047619048
Method name: normilized_likelihood, running accuracy: 89.01098901098901
Method name: normilized_entropy, running accuracy: 88.64468864468864
Method name: topk_entropy, running accuracy: 88.64468864468864
Method name: window_entropy, running accuracy: 89.74358974358975

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 272/500 [23:40:58<17:37:14, 278.22s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=89.01%, self_consistency_acc=90.48%, p_true_acc=90.48%, normilized_likelihood_acc=89.01%, normilized_entropy_acc=88.64%, topk_entropy_acc=88.64%, window_entropy_acc=89.74%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▍    | 273/500 [23:40:58<17:23:38, 275.85s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=89.01%, self_consistency_acc=90.48%, p_true_acc=90.48%, normilized_likelihood_acc=89.01%, normilized_entropy_acc=88.64%, topk_entropy_acc=88.64%, window_entropy_acc=89.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 6.096022326601682
    Answer: 207
    Ground truth:  247
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 6.096022326601682
    Answer: 207
    Ground truth:  247
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 11.866170346736908
    Answer: 207
    Ground truth:  247
Method 4: self_consistency
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 0.75
    Answer: 207
    Ground truth:  247
Method 5: p_true
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 8.28125
    Answer: 207
    Ground truth:  247
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 1.8940601199865341
    Answer: 207
    Ground truth:  247
Method 7: normilized_entropy
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 1.6972086876630783
    Answer: 207
    Ground truth:  247
Method 8: topk_entropy
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 1.669624701142311
    Answer: 207
    Ground truth:  247
Method 9: window_entropy
  Batch 1:
    Text: To find out the total number of classes that visited the Science Center, we need...
    Score: 6.750114977359772
    Answer: 207
    Ground truth:  247
Method name: attention_weighted_confidence, running accuracy: 89.41605839416059
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.41605839416059
Method name: cer_prob_product_log_last, running accuracy: 88.6861313868613
Method name: self_consistency, running accuracy: 90.14598540145985
Method name: p_true, running accuracy: 90.14598540145985
Method name: normilized_likelihood, running accuracy: 88.6861313868613
Method name: normilized_entropy, running accuracy: 88.32116788321169
Method name: topk_entropy, running accuracy: 88.32116788321169
Method name: window_entropy, running accuracy: 89.41605839416059

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▍    | 273/500 [23:44:23<17:23:38, 275.85s/it, attention_weighted_confidence_acc=89.42%, cer_entropy_weighted_mean_all_acc=89.42%, cer_prob_product_log_last_acc=88.69%, self_consistency_acc=90.15%, p_true_acc=90.15%, normilized_likelihood_acc=88.69%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.42%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▍    | 274/500 [23:44:23<15:58:37, 254.50s/it, attention_weighted_confidence_acc=89.42%, cer_entropy_weighted_mean_all_acc=89.42%, cer_prob_product_log_last_acc=88.69%, self_consistency_acc=90.15%, p_true_acc=90.15%, normilized_likelihood_acc=88.69%, normilized_entropy_acc=88.32%, topk_entropy_acc=88.32%, window_entropy_acc=89.42%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 7.555770264804275
    Answer: 18
    Ground truth:  18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 7.555770264804275
    Answer: 18
    Ground truth:  18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 14.999318599700928
    Answer: 18
    Ground truth:  18
Method 4: self_consistency
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 0.9375
    Answer: 18
    Ground truth:  18
Method 5: p_true
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 12.8828125
    Answer: 18
    Ground truth:  18
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 7.736007682979107
    Answer: 18
    Ground truth:  18
Method 7: normilized_entropy
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 7.807769224047661
    Answer: 18
    Ground truth:  18
Method 8: topk_entropy
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 7.013334155082703
    Answer: 18
    Ground truth:  18
Method 9: window_entropy
  Batch 1:
    Text: To find the length of the couch, we need to follow these steps:

1. Find the wid...
    Score: 21.14661803841591
    Answer: 18
    Ground truth:  18
Method name: attention_weighted_confidence, running accuracy: 89.45454545454545
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.45454545454545
Method name: cer_prob_product_log_last, running accuracy: 88.72727272727273
Method name: self_consistency, running accuracy: 90.18181818181819
Method name: p_true, running accuracy: 90.18181818181819
Method name: normilized_likelihood, running accuracy: 88.72727272727273
Method name: normilized_entropy, running accuracy: 88.36363636363636
Method name: topk_entropy, running accuracy: 88.36363636363636
Method name: window_entropy, running accuracy: 89.45454545454545

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▍    | 274/500 [23:48:02<15:58:37, 254.50s/it, attention_weighted_confidence_acc=89.45%, cer_entropy_weighted_mean_all_acc=89.45%, cer_prob_product_log_last_acc=88.73%, self_consistency_acc=90.18%, p_true_acc=90.18%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.45%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 275/500 [23:48:02<15:14:03, 243.75s/it, attention_weighted_confidence_acc=89.45%, cer_entropy_weighted_mean_all_acc=89.45%, cer_prob_product_log_last_acc=88.73%, self_consistency_acc=90.18%, p_true_acc=90.18%, normilized_likelihood_acc=88.73%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.45%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 6.657895173492328
    Answer: 74
    Ground truth:  74
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 6.657895173492328
    Answer: 74
    Ground truth:  74
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 12.649628579616547
    Answer: 74
    Ground truth:  74
Method 4: self_consistency
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 0.8125
    Answer: 74
    Ground truth:  74
Method 5: p_true
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 9.7109375
    Answer: 74
    Ground truth:  74
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 6.676935628056526
    Answer: 74
    Ground truth:  74
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 6.5872015208005905
    Answer: 74
    Ground truth:  74
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 5.856986254453659
    Answer: 74
    Ground truth:  74
Method 9: window_entropy
  Batch 1:
    Text: To find the number of loaves of bread left, we need to subtract the total number...
    Score: 16.12973341345787
    Answer: 74
    Ground truth:  74
Method name: attention_weighted_confidence, running accuracy: 89.4927536231884
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.4927536231884
Method name: cer_prob_product_log_last, running accuracy: 88.76811594202898
Method name: self_consistency, running accuracy: 90.21739130434783
Method name: p_true, running accuracy: 90.21739130434783
Method name: normilized_likelihood, running accuracy: 88.76811594202898
Method name: normilized_entropy, running accuracy: 88.40579710144928
Method name: topk_entropy, running accuracy: 88.40579710144928
Method name: window_entropy, running accuracy: 89.4927536231884

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 275/500 [23:52:04<15:14:03, 243.75s/it, attention_weighted_confidence_acc=89.49%, cer_entropy_weighted_mean_all_acc=89.49%, cer_prob_product_log_last_acc=88.77%, self_consistency_acc=90.22%, p_true_acc=90.22%, normilized_likelihood_acc=88.77%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.49%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 276/500 [23:52:04<15:08:05, 243.24s/it, attention_weighted_confidence_acc=89.49%, cer_entropy_weighted_mean_all_acc=89.49%, cer_prob_product_log_last_acc=88.77%, self_consistency_acc=90.22%, p_true_acc=90.22%, normilized_likelihood_acc=88.77%, normilized_entropy_acc=88.41%, topk_entropy_acc=88.41%, window_entropy_acc=89.49%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 7.520715723468489
    Answer: 156
    Ground truth:  156
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 7.520715723468489
    Answer: 156
    Ground truth:  156
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 14.99627286195755
    Answer: 156
    Ground truth:  156
Method 4: self_consistency
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 0.9375
    Answer: 156
    Ground truth:  156
Method 5: p_true
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 12.30078125
    Answer: 156
    Ground truth:  156
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 1.5172527581453323
    Answer: 156
    Ground truth:  156
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 1.7254090011119843
    Answer: 156
    Ground truth:  156
Method 8: topk_entropy
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 1.6695157289505005
    Answer: 156
    Ground truth:  156
Method 9: window_entropy
  Batch 1:
    Text: To find the total weight of marbles Solomon has in the store, let's break down t...
    Score: 7.924713671207428
    Answer: 156
    Ground truth:  156
Method name: attention_weighted_confidence, running accuracy: 89.53068592057761
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.53068592057761
Method name: cer_prob_product_log_last, running accuracy: 88.8086642599278
Method name: self_consistency, running accuracy: 90.25270758122743
Method name: p_true, running accuracy: 90.25270758122743
Method name: normilized_likelihood, running accuracy: 88.8086642599278
Method name: normilized_entropy, running accuracy: 88.4476534296029
Method name: topk_entropy, running accuracy: 88.4476534296029
Method name: window_entropy, running accuracy: 89.53068592057761

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 276/500 [23:57:33<15:08:05, 243.24s/it, attention_weighted_confidence_acc=89.53%, cer_entropy_weighted_mean_all_acc=89.53%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=90.25%, p_true_acc=90.25%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.45%, topk_entropy_acc=88.45%, window_entropy_acc=89.53%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 277/500 [23:57:33<16:40:14, 269.13s/it, attention_weighted_confidence_acc=89.53%, cer_entropy_weighted_mean_all_acc=89.53%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=90.25%, p_true_acc=90.25%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.45%, topk_entropy_acc=88.45%, window_entropy_acc=89.53%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 3.974960122393723
    Answer: 300
    Ground truth:  300
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 3.974960122393723
    Answer: 300
    Ground truth:  300
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 8.634890079498291
    Answer: 300
    Ground truth:  300
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 0.5625
    Answer: 300
    Ground truth:  300
Method 5: p_true
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 7.19140625
    Answer: 300
    Ground truth:  300
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 4.538135886192322
    Answer: 300
    Ground truth:  300
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 4.453017950057983
    Answer: 300
    Ground truth:  300
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 3.9723451137542725
    Answer: 300
    Ground truth:  300
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of pages in the book, we need to find the total number ...
    Score: 7.775196135044098
    Answer: 300
    Ground truth:  300
Method name: attention_weighted_confidence, running accuracy: 89.568345323741
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.568345323741
Method name: cer_prob_product_log_last, running accuracy: 88.84892086330936
Method name: self_consistency, running accuracy: 90.28776978417267
Method name: p_true, running accuracy: 90.28776978417267
Method name: normilized_likelihood, running accuracy: 88.84892086330936
Method name: normilized_entropy, running accuracy: 88.48920863309353
Method name: topk_entropy, running accuracy: 88.48920863309353
Method name: window_entropy, running accuracy: 89.568345323741

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 277/500 [24:01:16<16:40:14, 269.13s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.85%, self_consistency_acc=90.29%, p_true_acc=90.29%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.49%, topk_entropy_acc=88.49%, window_entropy_acc=89.57%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 278/500 [24:01:16<15:44:15, 255.20s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.85%, self_consistency_acc=90.29%, p_true_acc=90.29%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.49%, topk_entropy_acc=88.49%, window_entropy_acc=89.57%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 7.946050403469437
    Answer: 110
    Ground truth:  110
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 7.946050403469437
    Answer: 110
    Ground truth:  110
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 14.962521731853485
    Answer: 110
    Ground truth:  110
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 0.9375
    Answer: 110
    Ground truth:  110
Method 5: p_true
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 13.98828125
    Answer: 110
    Ground truth:  110
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 1.8473890572786331
    Answer: 110
    Ground truth:  110
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 0.861415445804596
    Answer: 110
    Ground truth:  110
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 0.8562870770692825
    Answer: 110
    Ground truth:  110
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of coins Gretchen has, we need to first determine the n...
    Score: 3.8539021015167236
    Answer: 110
    Ground truth:  110
Method name: attention_weighted_confidence, running accuracy: 89.6057347670251
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.6057347670251
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 90.32258064516128
Method name: p_true, running accuracy: 90.32258064516128
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.5304659498208
Method name: topk_entropy, running accuracy: 88.5304659498208
Method name: window_entropy, running accuracy: 89.6057347670251

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 278/500 [24:05:43<15:44:15, 255.20s/it, attention_weighted_confidence_acc=89.61%, cer_entropy_weighted_mean_all_acc=89.61%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.32%, p_true_acc=90.32%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.53%, topk_entropy_acc=88.53%, window_entropy_acc=89.61%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 279/500 [24:05:43<15:53:21, 258.83s/it, attention_weighted_confidence_acc=89.61%, cer_entropy_weighted_mean_all_acc=89.61%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.32%, p_true_acc=90.32%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.53%, topk_entropy_acc=88.53%, window_entropy_acc=89.61%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 7.982859517442728
    Answer: 800
    Ground truth:  800
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 7.982859517442728
    Answer: 800
    Ground truth:  800
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 15.999953031539917
    Answer: 800
    Ground truth:  800
Method 4: self_consistency
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 1.0
    Answer: 800
    Ground truth:  800
Method 5: p_true
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 11.53515625
    Answer: 800
    Ground truth:  800
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 2.2627238035202026
    Answer: 800
    Ground truth:  800
Method 7: normilized_entropy
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 1.3713666051626205
    Answer: 800
    Ground truth:  800
Method 8: topk_entropy
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 1.360089749097824
    Answer: 800
    Ground truth:  800
Method 9: window_entropy
  Batch 1:
    Text: To find the amount you have to pay after a 20% discount, follow these steps:

1....
    Score: 3.878837287425995
    Answer: 800
    Ground truth:  800
Method name: attention_weighted_confidence, running accuracy: 89.64285714285715
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.64285714285715
Method name: cer_prob_product_log_last, running accuracy: 88.92857142857142
Method name: self_consistency, running accuracy: 90.35714285714286
Method name: p_true, running accuracy: 90.35714285714286
Method name: normilized_likelihood, running accuracy: 88.92857142857142
Method name: normilized_entropy, running accuracy: 88.57142857142857
Method name: topk_entropy, running accuracy: 88.57142857142857
Method name: window_entropy, running accuracy: 89.64285714285715

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 279/500 [24:08:27<15:53:21, 258.83s/it, attention_weighted_confidence_acc=89.64%, cer_entropy_weighted_mean_all_acc=89.64%, cer_prob_product_log_last_acc=88.93%, self_consistency_acc=90.36%, p_true_acc=90.36%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=89.64%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 280/500 [24:08:27<14:04:35, 230.34s/it, attention_weighted_confidence_acc=89.64%, cer_entropy_weighted_mean_all_acc=89.64%, cer_prob_product_log_last_acc=88.93%, self_consistency_acc=90.36%, p_true_acc=90.36%, normilized_likelihood_acc=88.93%, normilized_entropy_acc=88.57%, topk_entropy_acc=88.57%, window_entropy_acc=89.64%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 7.785785504864925
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 7.785785504864925
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 15.999993562698364
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 11.79296875
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 2.4337657392024994
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 1.5205787122249603
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 1.5026133954524994
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how many pounds of insects a flock of ten ducks need per day, we nee...
    Score: 3.600340962409973
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 89.6797153024911
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.6797153024911
Method name: cer_prob_product_log_last, running accuracy: 88.96797153024912
Method name: self_consistency, running accuracy: 90.39145907473309
Method name: p_true, running accuracy: 90.39145907473309
Method name: normilized_likelihood, running accuracy: 88.96797153024912
Method name: normilized_entropy, running accuracy: 88.61209964412812
Method name: topk_entropy, running accuracy: 88.61209964412812
Method name: window_entropy, running accuracy: 89.6797153024911

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 280/500 [24:11:40<14:04:35, 230.34s/it, attention_weighted_confidence_acc=89.68%, cer_entropy_weighted_mean_all_acc=89.68%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=90.39%, p_true_acc=90.39%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.61%, topk_entropy_acc=88.61%, window_entropy_acc=89.68%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 281/500 [24:11:40<13:19:40, 219.09s/it, attention_weighted_confidence_acc=89.68%, cer_entropy_weighted_mean_all_acc=89.68%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=90.39%, p_true_acc=90.39%, normilized_likelihood_acc=88.97%, normilized_entropy_acc=88.61%, topk_entropy_acc=88.61%, window_entropy_acc=89.68%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 7.386752015196848
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 7.386752015196848
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 14.979521870613098
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 0.9375
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 14.4609375
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 4.43102602660656
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 4.796353787183762
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 4.415627062320709
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of boxes they have, we need to calculate the number of ...
    Score: 17.38249284029007
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 89.71631205673759
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.71631205673759
Method name: cer_prob_product_log_last, running accuracy: 89.00709219858156
Method name: self_consistency, running accuracy: 90.42553191489363
Method name: p_true, running accuracy: 90.42553191489363
Method name: normilized_likelihood, running accuracy: 89.00709219858156
Method name: normilized_entropy, running accuracy: 88.65248226950354
Method name: topk_entropy, running accuracy: 88.65248226950354
Method name: window_entropy, running accuracy: 89.71631205673759

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 281/500 [24:16:01<13:19:40, 219.09s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=89.01%, self_consistency_acc=90.43%, p_true_acc=90.43%, normilized_likelihood_acc=89.01%, normilized_entropy_acc=88.65%, topk_entropy_acc=88.65%, window_entropy_acc=89.72%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▋    | 282/500 [24:16:01<14:01:44, 231.67s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=89.01%, self_consistency_acc=90.43%, p_true_acc=90.43%, normilized_likelihood_acc=89.01%, normilized_entropy_acc=88.65%, topk_entropy_acc=88.65%, window_entropy_acc=89.72%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 7.633008362595759
    Answer: 75
    Ground truth:  75
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 7.633008362595759
    Answer: 75
    Ground truth:  75
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 14.987100541591644
    Answer: 75
    Ground truth:  75
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 0.9375
    Answer: 75
    Ground truth:  75
Method 5: p_true
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 11.63671875
    Answer: 75
    Ground truth:  75
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 2.240886129438877
    Answer: 75
    Ground truth:  75
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 1.5780376344919205
    Answer: 75
    Ground truth:  75
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 1.5670659244060516
    Answer: 75
    Ground truth:  75
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of insects in the garden, we first need to calculate th...
    Score: 4.031376361846924
    Answer: 75
    Ground truth:  75
Method name: attention_weighted_confidence, running accuracy: 89.75265017667844
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.75265017667844
Method name: cer_prob_product_log_last, running accuracy: 89.04593639575971
Method name: self_consistency, running accuracy: 90.45936395759718
Method name: p_true, running accuracy: 90.45936395759718
Method name: normilized_likelihood, running accuracy: 89.04593639575971
Method name: normilized_entropy, running accuracy: 88.69257950530034
Method name: topk_entropy, running accuracy: 88.69257950530034
Method name: window_entropy, running accuracy: 89.75265017667844

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▋    | 282/500 [24:19:16<14:01:44, 231.67s/it, attention_weighted_confidence_acc=89.75%, cer_entropy_weighted_mean_all_acc=89.75%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.46%, p_true_acc=90.46%, normilized_likelihood_acc=89.05%, normilized_entropy_acc=88.69%, topk_entropy_acc=88.69%, window_entropy_acc=89.75%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 283/500 [24:19:16<13:17:27, 220.50s/it, attention_weighted_confidence_acc=89.75%, cer_entropy_weighted_mean_all_acc=89.75%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.46%, p_true_acc=90.46%, normilized_likelihood_acc=89.05%, normilized_entropy_acc=88.69%, topk_entropy_acc=88.69%, window_entropy_acc=89.75%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 6.432281319716572
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 6.432281319716572
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 12.937313854694366
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 0.8125
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 12.24609375
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 8.515055730938911
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 7.7356424778699875
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 7.102262616157532
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: To find out how many OLED TVs were sold, we need to determine the total percenta...
    Score: 10.746624112129211
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 89.7887323943662
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.7887323943662
Method name: cer_prob_product_log_last, running accuracy: 89.08450704225352
Method name: self_consistency, running accuracy: 90.49295774647888
Method name: p_true, running accuracy: 90.49295774647888
Method name: normilized_likelihood, running accuracy: 89.08450704225352
Method name: normilized_entropy, running accuracy: 88.73239436619718
Method name: topk_entropy, running accuracy: 88.73239436619718
Method name: window_entropy, running accuracy: 89.7887323943662

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 283/500 [24:25:26<13:17:27, 220.50s/it, attention_weighted_confidence_acc=89.79%, cer_entropy_weighted_mean_all_acc=89.79%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=90.49%, p_true_acc=90.49%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.73%, topk_entropy_acc=88.73%, window_entropy_acc=89.79%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 284/500 [24:25:26<15:55:45, 265.49s/it, attention_weighted_confidence_acc=89.79%, cer_entropy_weighted_mean_all_acc=89.79%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=90.49%, p_true_acc=90.49%, normilized_likelihood_acc=89.08%, normilized_entropy_acc=88.73%, topk_entropy_acc=88.73%, window_entropy_acc=89.79%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 8.24718598643018
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 8.24718598643018
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 14.999963402748108
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 0.9375
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 11.02734375
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 4.930775873363018
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 3.5063495486974716
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 3.1585784181952477
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of each offer, we need to add the advance payment to the ...
    Score: 21.031192898750305
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 89.82456140350877
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.82456140350877
Method name: cer_prob_product_log_last, running accuracy: 89.12280701754386
Method name: self_consistency, running accuracy: 90.52631578947368
Method name: p_true, running accuracy: 90.52631578947368
Method name: normilized_likelihood, running accuracy: 89.12280701754386
Method name: normilized_entropy, running accuracy: 88.7719298245614
Method name: topk_entropy, running accuracy: 88.7719298245614
Method name: window_entropy, running accuracy: 89.82456140350877

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 284/500 [24:30:01<15:55:45, 265.49s/it, attention_weighted_confidence_acc=89.82%, cer_entropy_weighted_mean_all_acc=89.82%, cer_prob_product_log_last_acc=89.12%, self_consistency_acc=90.53%, p_true_acc=90.53%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.77%, topk_entropy_acc=88.77%, window_entropy_acc=89.82%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 285/500 [24:30:01<16:01:12, 268.25s/it, attention_weighted_confidence_acc=89.82%, cer_entropy_weighted_mean_all_acc=89.82%, cer_prob_product_log_last_acc=89.12%, self_consistency_acc=90.53%, p_true_acc=90.53%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.77%, topk_entropy_acc=88.77%, window_entropy_acc=89.82%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 6.878529385034126
    Answer: 42
    Ground truth:  128
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 6.878529385034126
    Answer: 42
    Ground truth:  128
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 13.999839901924133
    Answer: 42
    Ground truth:  128
Method 4: self_consistency
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 0.875
    Answer: 42
    Ground truth:  128
Method 5: p_true
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 12.28515625
    Answer: 42
    Ground truth:  128
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 2.2842065542936325
    Answer: 42
    Ground truth:  128
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 1.0754331350326538
    Answer: 42
    Ground truth:  128
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 1.065087541937828
    Answer: 42
    Ground truth:  128
Method 9: window_entropy
  Batch 1:
    Text: To find out how many glasses and plates Jeff has now, we need to start with the ...
    Score: 3.5219603180885315
    Answer: 42
    Ground truth:  128
Method name: attention_weighted_confidence, running accuracy: 89.5104895104895
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.5104895104895
Method name: cer_prob_product_log_last, running accuracy: 88.81118881118881
Method name: self_consistency, running accuracy: 90.20979020979021
Method name: p_true, running accuracy: 90.20979020979021
Method name: normilized_likelihood, running accuracy: 88.81118881118881
Method name: normilized_entropy, running accuracy: 88.46153846153845
Method name: topk_entropy, running accuracy: 88.46153846153845
Method name: window_entropy, running accuracy: 89.5104895104895

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 285/500 [24:33:59<16:01:12, 268.25s/it, attention_weighted_confidence_acc=89.51%, cer_entropy_weighted_mean_all_acc=89.51%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=90.21%, p_true_acc=90.21%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.51%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 286/500 [24:33:59<15:25:11, 259.40s/it, attention_weighted_confidence_acc=89.51%, cer_entropy_weighted_mean_all_acc=89.51%, cer_prob_product_log_last_acc=88.81%, self_consistency_acc=90.21%, p_true_acc=90.21%, normilized_likelihood_acc=88.81%, normilized_entropy_acc=88.46%, topk_entropy_acc=88.46%, window_entropy_acc=89.51%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 8.236634457853079
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 8.236634457853079
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 15.969787061214447
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 15.1875
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 2.6162382438778877
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 2.8974518477916718
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 2.806344598531723
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's follow step by step.

Let's denote the number of co...
    Score: 9.85300961136818
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 89.54703832752612
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.54703832752612
Method name: cer_prob_product_log_last, running accuracy: 88.85017421602788
Method name: self_consistency, running accuracy: 90.2439024390244
Method name: p_true, running accuracy: 90.2439024390244
Method name: normilized_likelihood, running accuracy: 88.85017421602788
Method name: normilized_entropy, running accuracy: 88.50174216027874
Method name: topk_entropy, running accuracy: 88.50174216027874
Method name: window_entropy, running accuracy: 89.54703832752612

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 286/500 [24:38:04<15:25:11, 259.40s/it, attention_weighted_confidence_acc=89.55%, cer_entropy_weighted_mean_all_acc=89.55%, cer_prob_product_log_last_acc=88.85%, self_consistency_acc=90.24%, p_true_acc=90.24%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.50%, topk_entropy_acc=88.50%, window_entropy_acc=89.55%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 287/500 [24:38:04<15:05:05, 254.96s/it, attention_weighted_confidence_acc=89.55%, cer_entropy_weighted_mean_all_acc=89.55%, cer_prob_product_log_last_acc=88.85%, self_consistency_acc=90.24%, p_true_acc=90.24%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.50%, topk_entropy_acc=88.50%, window_entropy_acc=89.55%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 2.9153789619641004
    Answer: 6
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 2.9153789619641004
    Answer: 6
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 5.99973601102829
    Answer: 6
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 0.375
    Answer: 6
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 4.50390625
    Answer: 6
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 3.569044128060341
    Answer: 6
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 3.012045383453369
    Answer: 6
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 2.6091727912425995
    Answer: 6
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Josh starts with 3 balls and adds 1 ball...
    Score: 5.388335406780243
    Answer: 6
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.23611111111111
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.23611111111111
Method name: cer_prob_product_log_last, running accuracy: 88.54166666666666
Method name: self_consistency, running accuracy: 89.93055555555556
Method name: p_true, running accuracy: 89.93055555555556
Method name: normilized_likelihood, running accuracy: 88.54166666666666
Method name: normilized_entropy, running accuracy: 88.19444444444444
Method name: topk_entropy, running accuracy: 88.19444444444444
Method name: window_entropy, running accuracy: 89.23611111111111

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 287/500 [24:43:14<15:05:05, 254.96s/it, attention_weighted_confidence_acc=89.24%, cer_entropy_weighted_mean_all_acc=89.24%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=89.93%, p_true_acc=89.93%, normilized_likelihood_acc=88.54%, normilized_entropy_acc=88.19%, topk_entropy_acc=88.19%, window_entropy_acc=89.24%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 288/500 [24:43:14<15:59:27, 271.54s/it, attention_weighted_confidence_acc=89.24%, cer_entropy_weighted_mean_all_acc=89.24%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=89.93%, p_true_acc=89.93%, normilized_likelihood_acc=88.54%, normilized_entropy_acc=88.19%, topk_entropy_acc=88.19%, window_entropy_acc=89.24%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many good days are left in the month, we need to determine the n...
    Score: 0.8720721321838918
    Answer: 4
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many good days are left in the month, we need to determine the n...
    Score: 0.8720721321838918
    Answer: 4
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many good days are left in the month, we need to determine the n...
    Score: 1.9993517398834229
    Answer: 4
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the information given:

1. The first 8 days of the month were g...
    Score: 0.125
    Answer: 12
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find out how many good days were left in the month, we first need to find the...
    Score: 1.58203125
    Answer: 1
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many good days were left in the month, we first need to find the...
    Score: 1.755481481552124
    Answer: 1
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many good days were left in the month, we first need to find the...
    Score: 1.5629243850708008
    Answer: 1
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many good days were left in the month, we first need to find the...
    Score: 1.3413598835468292
    Answer: 1
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find out how many good days were left in the month, we first need to find the...
    Score: 2.0637826919555664
    Answer: 1
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 88.92733564013841
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.92733564013841
Method name: cer_prob_product_log_last, running accuracy: 88.23529411764706
Method name: self_consistency, running accuracy: 89.61937716262976
Method name: p_true, running accuracy: 89.61937716262976
Method name: normilized_likelihood, running accuracy: 88.23529411764706
Method name: normilized_entropy, running accuracy: 87.88927335640139
Method name: topk_entropy, running accuracy: 87.88927335640139
Method name: window_entropy, running accuracy: 88.92733564013841

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 288/500 [24:50:20<15:59:27, 271.54s/it, attention_weighted_confidence_acc=88.93%, cer_entropy_weighted_mean_all_acc=88.93%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=89.62%, p_true_acc=89.62%, normilized_likelihood_acc=88.24%, normilized_entropy_acc=87.89%, topk_entropy_acc=87.89%, window_entropy_acc=88.93%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 289/500 [24:50:20<18:37:16, 317.71s/it, attention_weighted_confidence_acc=88.93%, cer_entropy_weighted_mean_all_acc=88.93%, cer_prob_product_log_last_acc=88.24%, self_consistency_acc=89.62%, p_true_acc=89.62%, normilized_likelihood_acc=88.24%, normilized_entropy_acc=87.89%, topk_entropy_acc=87.89%, window_entropy_acc=88.93%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 7.548401675666479
    Answer: 31
    Ground truth:  31
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 7.548401675666479
    Answer: 31
    Ground truth:  31
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 14.417165398597717
    Answer: 31
    Ground truth:  31
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 0.9375
    Answer: 31
    Ground truth:  31
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 14.38671875
    Answer: 31
    Ground truth:  31
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 7.867976889014244
    Answer: 31
    Ground truth:  31
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 5.985968172550201
    Answer: 31
    Ground truth:  31
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 5.706785202026367
    Answer: 31
    Ground truth:  31
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let the number of p...
    Score: 13.769123375415802
    Answer: 31
    Ground truth:  31
Method name: attention_weighted_confidence, running accuracy: 88.96551724137932
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.96551724137932
Method name: cer_prob_product_log_last, running accuracy: 88.27586206896552
Method name: self_consistency, running accuracy: 89.65517241379311
Method name: p_true, running accuracy: 89.65517241379311
Method name: normilized_likelihood, running accuracy: 88.27586206896552
Method name: normilized_entropy, running accuracy: 87.93103448275862
Method name: topk_entropy, running accuracy: 87.93103448275862
Method name: window_entropy, running accuracy: 88.96551724137932

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 289/500 [24:53:50<18:37:16, 317.71s/it, attention_weighted_confidence_acc=88.97%, cer_entropy_weighted_mean_all_acc=88.97%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=89.66%, p_true_acc=89.66%, normilized_likelihood_acc=88.28%, normilized_entropy_acc=87.93%, topk_entropy_acc=87.93%, window_entropy_acc=88.97%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 290/500 [24:53:50<16:39:37, 285.61s/it, attention_weighted_confidence_acc=88.97%, cer_entropy_weighted_mean_all_acc=88.97%, cer_prob_product_log_last_acc=88.28%, self_consistency_acc=89.66%, p_true_acc=89.66%, normilized_likelihood_acc=88.28%, normilized_entropy_acc=87.93%, topk_entropy_acc=87.93%, window_entropy_acc=88.97%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 8.094256013953366
    Answer: 112
    Ground truth:  112
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 8.094256013953366
    Answer: 112
    Ground truth:  112
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 15.99051582813263
    Answer: 112
    Ground truth:  112
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 1.0
    Answer: 112
    Ground truth:  112
Method 5: p_true
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 15.01953125
    Answer: 112
    Ground truth:  112
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 1.162596419453621
    Answer: 112
    Ground truth:  112
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 1.1114230006933212
    Answer: 112
    Ground truth:  112
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 1.1091354489326477
    Answer: 112
    Ground truth:  112
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of colors of crayons Beatrice bought, we need to find o...
    Score: 3.6144556999206543
    Answer: 112
    Ground truth:  112
Method name: attention_weighted_confidence, running accuracy: 89.00343642611683
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.00343642611683
Method name: cer_prob_product_log_last, running accuracy: 88.31615120274914
Method name: self_consistency, running accuracy: 89.69072164948454
Method name: p_true, running accuracy: 89.69072164948454
Method name: normilized_likelihood, running accuracy: 88.31615120274914
Method name: normilized_entropy, running accuracy: 87.97250859106529
Method name: topk_entropy, running accuracy: 87.97250859106529
Method name: window_entropy, running accuracy: 89.00343642611683

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 290/500 [24:57:31<16:39:37, 285.61s/it, attention_weighted_confidence_acc=89.00%, cer_entropy_weighted_mean_all_acc=89.00%, cer_prob_product_log_last_acc=88.32%, self_consistency_acc=89.69%, p_true_acc=89.69%, normilized_likelihood_acc=88.32%, normilized_entropy_acc=87.97%, topk_entropy_acc=87.97%, window_entropy_acc=89.00%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 291/500 [24:57:31<15:27:17, 266.21s/it, attention_weighted_confidence_acc=89.00%, cer_entropy_weighted_mean_all_acc=89.00%, cer_prob_product_log_last_acc=88.32%, self_consistency_acc=89.69%, p_true_acc=89.69%, normilized_likelihood_acc=88.32%, normilized_entropy_acc=87.97%, topk_entropy_acc=87.97%, window_entropy_acc=89.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 6.1727844152057365
    Answer: 58
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 6.1727844152057365
    Answer: 58
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 11.998222529888153
    Answer: 58
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 0.75
    Answer: 58
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 11.42578125
    Answer: 58
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 1.525862768292427
    Answer: 58
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 0.5201418548822403
    Answer: 58
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 0.5035523474216461
    Answer: 58
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find the time it takes for the second team to complete the 4x400 meter relay,...
    Score: 3.411011278629303
    Answer: 58
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 88.6986301369863
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.6986301369863
Method name: cer_prob_product_log_last, running accuracy: 88.01369863013699
Method name: self_consistency, running accuracy: 89.38356164383562
Method name: p_true, running accuracy: 89.38356164383562
Method name: normilized_likelihood, running accuracy: 88.01369863013699
Method name: normilized_entropy, running accuracy: 87.67123287671232
Method name: topk_entropy, running accuracy: 87.67123287671232
Method name: window_entropy, running accuracy: 88.6986301369863

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 291/500 [25:03:42<15:27:17, 266.21s/it, attention_weighted_confidence_acc=88.70%, cer_entropy_weighted_mean_all_acc=88.70%, cer_prob_product_log_last_acc=88.01%, self_consistency_acc=89.38%, p_true_acc=89.38%, normilized_likelihood_acc=88.01%, normilized_entropy_acc=87.67%, topk_entropy_acc=87.67%, window_entropy_acc=88.70%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 292/500 [25:03:42<17:11:49, 297.64s/it, attention_weighted_confidence_acc=88.70%, cer_entropy_weighted_mean_all_acc=88.70%, cer_prob_product_log_last_acc=88.01%, self_consistency_acc=89.38%, p_true_acc=89.38%, normilized_likelihood_acc=88.01%, normilized_entropy_acc=87.67%, topk_entropy_acc=87.67%, window_entropy_acc=88.70%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 5.037884755217954
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 5.037884755217954
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 10.734756112098694
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 0.6875
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 9.1328125
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 6.430174350738525
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 6.594910174608231
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 5.706844210624695
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: Step 1: First, we find the ratio of the first package's price to its number of s...
    Score: 11.7824387550354
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 88.73720136518772
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.73720136518772
Method name: cer_prob_product_log_last, running accuracy: 88.05460750853243
Method name: self_consistency, running accuracy: 89.419795221843
Method name: p_true, running accuracy: 89.419795221843
Method name: normilized_likelihood, running accuracy: 88.05460750853243
Method name: normilized_entropy, running accuracy: 87.71331058020478
Method name: topk_entropy, running accuracy: 87.71331058020478
Method name: window_entropy, running accuracy: 88.73720136518772

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 292/500 [25:09:45<17:11:49, 297.64s/it, attention_weighted_confidence_acc=88.74%, cer_entropy_weighted_mean_all_acc=88.74%, cer_prob_product_log_last_acc=88.05%, self_consistency_acc=89.42%, p_true_acc=89.42%, normilized_likelihood_acc=88.05%, normilized_entropy_acc=87.71%, topk_entropy_acc=87.71%, window_entropy_acc=88.74%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▊    | 293/500 [25:09:45<18:14:15, 317.18s/it, attention_weighted_confidence_acc=88.74%, cer_entropy_weighted_mean_all_acc=88.74%, cer_prob_product_log_last_acc=88.05%, self_consistency_acc=89.42%, p_true_acc=89.42%, normilized_likelihood_acc=88.05%, normilized_entropy_acc=87.71%, topk_entropy_acc=87.71%, window_entropy_acc=88.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 6.016764255243874
    Answer: 750
    Ground truth:  750
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 6.016764255243874
    Answer: 750
    Ground truth:  750
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 12.99954891204834
    Answer: 750
    Ground truth:  750
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 0.8125
    Answer: 750
    Ground truth:  750
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 11.42578125
    Answer: 750
    Ground truth:  750
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 4.714698135852814
    Answer: 750
    Ground truth:  750
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 4.682806879281998
    Answer: 750
    Ground truth:  750
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 4.444253474473953
    Answer: 750
    Ground truth:  750
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Teddy finished half of a 500-piece puzzle....
    Score: 7.884121596813202
    Answer: 750
    Ground truth:  750
Method name: attention_weighted_confidence, running accuracy: 88.77551020408163
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.77551020408163
Method name: cer_prob_product_log_last, running accuracy: 88.09523809523809
Method name: self_consistency, running accuracy: 89.45578231292517
Method name: p_true, running accuracy: 89.45578231292517
Method name: normilized_likelihood, running accuracy: 88.09523809523809
Method name: normilized_entropy, running accuracy: 87.75510204081633
Method name: topk_entropy, running accuracy: 87.75510204081633
Method name: window_entropy, running accuracy: 88.77551020408163

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▊    | 293/500 [25:12:55<18:14:15, 317.18s/it, attention_weighted_confidence_acc=88.78%, cer_entropy_weighted_mean_all_acc=88.78%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=89.46%, p_true_acc=89.46%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=87.76%, topk_entropy_acc=87.76%, window_entropy_acc=88.78%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 294/500 [25:12:55<15:57:28, 278.88s/it, attention_weighted_confidence_acc=88.78%, cer_entropy_weighted_mean_all_acc=88.78%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=89.46%, p_true_acc=89.46%, normilized_likelihood_acc=88.10%, normilized_entropy_acc=87.76%, topk_entropy_acc=87.76%, window_entropy_acc=88.78%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 8.321367242323362
    Answer: 159
    Ground truth:  159
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 8.321367242323362
    Answer: 159
    Ground truth:  159
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 15.99957424402237
    Answer: 159
    Ground truth:  159
Method 4: self_consistency
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 1.0
    Answer: 159
    Ground truth:  159
Method 5: p_true
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 14.77734375
    Answer: 159
    Ground truth:  159
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 1.6429124623537064
    Answer: 159
    Ground truth:  159
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 0.9984723776578903
    Answer: 159
    Ground truth:  159
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 0.999192088842392
    Answer: 159
    Ground truth:  159
Method 9: window_entropy
  Batch 1:
    Text: To find out how many pounds of hay Nate needs, we need to calculate the total ha...
    Score: 3.398992270231247
    Answer: 159
    Ground truth:  159
Method name: attention_weighted_confidence, running accuracy: 88.8135593220339
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.8135593220339
Method name: cer_prob_product_log_last, running accuracy: 88.13559322033898
Method name: self_consistency, running accuracy: 89.49152542372882
Method name: p_true, running accuracy: 89.49152542372882
Method name: normilized_likelihood, running accuracy: 88.13559322033898
Method name: normilized_entropy, running accuracy: 87.79661016949153
Method name: topk_entropy, running accuracy: 87.79661016949153
Method name: window_entropy, running accuracy: 88.8135593220339

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 294/500 [25:18:08<15:57:28, 278.88s/it, attention_weighted_confidence_acc=88.81%, cer_entropy_weighted_mean_all_acc=88.81%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=89.49%, p_true_acc=89.49%, normilized_likelihood_acc=88.14%, normilized_entropy_acc=87.80%, topk_entropy_acc=87.80%, window_entropy_acc=88.81%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 295/500 [25:18:08<16:27:52, 289.13s/it, attention_weighted_confidence_acc=88.81%, cer_entropy_weighted_mean_all_acc=88.81%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=89.49%, p_true_acc=89.49%, normilized_likelihood_acc=88.14%, normilized_entropy_acc=87.80%, topk_entropy_acc=87.80%, window_entropy_acc=88.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 8.467967762180074
    Answer: 90000
    Ground truth:  90000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 8.467967762180074
    Answer: 90000
    Ground truth:  90000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 0.0020349525832246695
    Answer: 90000
    Ground truth:  90000
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 1.0
    Answer: 90000
    Ground truth:  90000
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 13.12109375
    Answer: 90000
    Ground truth:  90000
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 1.4623061567544937
    Answer: 90000
    Ground truth:  90000
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 0.6769500225782394
    Answer: 90000
    Ground truth:  90000
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 0.6759171038866043
    Answer: 90000
    Ground truth:  90000
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can let x be the cost of the lot. Since the house cost...
    Score: 3.079786092042923
    Answer: 90000
    Ground truth:  90000
Method name: attention_weighted_confidence, running accuracy: 88.85135135135135
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.85135135135135
Method name: cer_prob_product_log_last, running accuracy: 88.17567567567568
Method name: self_consistency, running accuracy: 89.52702702702703
Method name: p_true, running accuracy: 89.52702702702703
Method name: normilized_likelihood, running accuracy: 88.17567567567568
Method name: normilized_entropy, running accuracy: 87.83783783783784
Method name: topk_entropy, running accuracy: 87.83783783783784
Method name: window_entropy, running accuracy: 88.85135135135135

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 295/500 [25:22:31<16:27:52, 289.13s/it, attention_weighted_confidence_acc=88.85%, cer_entropy_weighted_mean_all_acc=88.85%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=89.53%, p_true_acc=89.53%, normilized_likelihood_acc=88.18%, normilized_entropy_acc=87.84%, topk_entropy_acc=87.84%, window_entropy_acc=88.85%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 296/500 [25:22:31<15:56:45, 281.40s/it, attention_weighted_confidence_acc=88.85%, cer_entropy_weighted_mean_all_acc=88.85%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=89.53%, p_true_acc=89.53%, normilized_likelihood_acc=88.18%, normilized_entropy_acc=87.84%, topk_entropy_acc=87.84%, window_entropy_acc=88.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 4.692833384061577
    Answer: 71
    Ground truth:  91
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 4.692833384061577
    Answer: 71
    Ground truth:  91
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 8.949140548706055
    Answer: 71
    Ground truth:  91
Method 4: self_consistency
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 0.5625
    Answer: 71
    Ground truth:  91
Method 5: p_true
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 8.64453125
    Answer: 71
    Ground truth:  91
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 5.669138729572296
    Answer: 71
    Ground truth:  91
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 5.094448089599609
    Answer: 71
    Ground truth:  91
Method 8: topk_entropy
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 4.650368347764015
    Answer: 71
    Ground truth:  91
Method 9: window_entropy
  Batch 1:
    Text: To find the total time from starting the oil to having cooked chicken, let's bre...
    Score: 11.584617674350739
    Answer: 71
    Ground truth:  91
Method name: attention_weighted_confidence, running accuracy: 88.55218855218855
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.55218855218855
Method name: cer_prob_product_log_last, running accuracy: 87.87878787878788
Method name: self_consistency, running accuracy: 89.22558922558923
Method name: p_true, running accuracy: 89.22558922558923
Method name: normilized_likelihood, running accuracy: 87.87878787878788
Method name: normilized_entropy, running accuracy: 87.54208754208754
Method name: topk_entropy, running accuracy: 87.54208754208754
Method name: window_entropy, running accuracy: 88.55218855218855

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 296/500 [25:28:15<15:56:45, 281.40s/it, attention_weighted_confidence_acc=88.55%, cer_entropy_weighted_mean_all_acc=88.55%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=89.23%, p_true_acc=89.23%, normilized_likelihood_acc=87.88%, normilized_entropy_acc=87.54%, topk_entropy_acc=87.54%, window_entropy_acc=88.55%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 297/500 [25:28:15<16:55:49, 300.24s/it, attention_weighted_confidence_acc=88.55%, cer_entropy_weighted_mean_all_acc=88.55%, cer_prob_product_log_last_acc=87.88%, self_consistency_acc=89.23%, p_true_acc=89.23%, normilized_likelihood_acc=87.88%, normilized_entropy_acc=87.54%, topk_entropy_acc=87.54%, window_entropy_acc=88.55%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 6.651140517475125
    Answer: 14
    Ground truth:  14
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 6.651140517475125
    Answer: 14
    Ground truth:  14
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 12.364574134349823
    Answer: 14
    Ground truth:  14
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 0.8125
    Answer: 14
    Ground truth:  14
Method 5: p_true
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 11.96875
    Answer: 14
    Ground truth:  14
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 4.667289927601814
    Answer: 14
    Ground truth:  14
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 3.347589448094368
    Answer: 14
    Ground truth:  14
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 3.2983635514974594
    Answer: 14
    Ground truth:  14
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the information:

1. The total number of people at the third st...
    Score: 6.267393708229065
    Answer: 14
    Ground truth:  14
Method name: attention_weighted_confidence, running accuracy: 88.59060402684564
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.59060402684564
Method name: cer_prob_product_log_last, running accuracy: 87.91946308724832
Method name: self_consistency, running accuracy: 89.26174496644296
Method name: p_true, running accuracy: 89.26174496644296
Method name: normilized_likelihood, running accuracy: 87.91946308724832
Method name: normilized_entropy, running accuracy: 87.58389261744966
Method name: topk_entropy, running accuracy: 87.58389261744966
Method name: window_entropy, running accuracy: 88.59060402684564

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 297/500 [25:32:54<16:55:49, 300.24s/it, attention_weighted_confidence_acc=88.59%, cer_entropy_weighted_mean_all_acc=88.59%, cer_prob_product_log_last_acc=87.92%, self_consistency_acc=89.26%, p_true_acc=89.26%, normilized_likelihood_acc=87.92%, normilized_entropy_acc=87.58%, topk_entropy_acc=87.58%, window_entropy_acc=88.59%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|█████▉    | 298/500 [25:32:54<16:28:48, 293.70s/it, attention_weighted_confidence_acc=88.59%, cer_entropy_weighted_mean_all_acc=88.59%, cer_prob_product_log_last_acc=87.92%, self_consistency_acc=89.26%, p_true_acc=89.26%, normilized_likelihood_acc=87.92%, normilized_entropy_acc=87.58%, topk_entropy_acc=87.58%, window_entropy_acc=88.59%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 8.001321015769225
    Answer: 133
    Ground truth:  133
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 8.001321015769225
    Answer: 133
    Ground truth:  133
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 15.999698162078857
    Answer: 133
    Ground truth:  133
Method 4: self_consistency
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 1.0
    Answer: 133
    Ground truth:  133
Method 5: p_true
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 14.9140625
    Answer: 133
    Ground truth:  133
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 2.0521581172943115
    Answer: 133
    Ground truth:  133
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 1.165696233510971
    Answer: 133
    Ground truth:  133
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 1.1612476855516434
    Answer: 133
    Ground truth:  133
Method 9: window_entropy
  Batch 1:
    Text: To find out how much you will pay after the discount, we need to calculate 5% of...
    Score: 2.8799726963043213
    Answer: 133
    Ground truth:  133
Method name: attention_weighted_confidence, running accuracy: 88.62876254180601
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.62876254180601
Method name: cer_prob_product_log_last, running accuracy: 87.95986622073578
Method name: self_consistency, running accuracy: 89.29765886287625
Method name: p_true, running accuracy: 89.29765886287625
Method name: normilized_likelihood, running accuracy: 87.95986622073578
Method name: normilized_entropy, running accuracy: 87.62541806020067
Method name: topk_entropy, running accuracy: 87.62541806020067
Method name: window_entropy, running accuracy: 88.62876254180601

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|█████▉    | 298/500 [25:35:36<16:28:48, 293.70s/it, attention_weighted_confidence_acc=88.63%, cer_entropy_weighted_mean_all_acc=88.63%, cer_prob_product_log_last_acc=87.96%, self_consistency_acc=89.30%, p_true_acc=89.30%, normilized_likelihood_acc=87.96%, normilized_entropy_acc=87.63%, topk_entropy_acc=87.63%, window_entropy_acc=88.63%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|█████▉    | 299/500 [25:35:36<14:12:18, 254.42s/it, attention_weighted_confidence_acc=88.63%, cer_entropy_weighted_mean_all_acc=88.63%, cer_prob_product_log_last_acc=87.96%, self_consistency_acc=89.30%, p_true_acc=89.30%, normilized_likelihood_acc=87.96%, normilized_entropy_acc=87.63%, topk_entropy_acc=87.63%, window_entropy_acc=88.63%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 6.058910886691696
    Answer: 40
    Ground truth:  40
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 6.058910886691696
    Answer: 40
    Ground truth:  40
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 10.155396342277527
    Answer: 40
    Ground truth:  40
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 0.75
    Answer: 40
    Ground truth:  40
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 8.791015625
    Answer: 40
    Ground truth:  40
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 5.319029062986374
    Answer: 40
    Ground truth:  40
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 4.369431704282761
    Answer: 40
    Ground truth:  40
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 4.089954495429993
    Answer: 40
    Ground truth:  40
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Let's assume M...
    Score: 12.199460923671722
    Answer: 40
    Ground truth:  40
Method name: attention_weighted_confidence, running accuracy: 88.66666666666667
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.66666666666667
Method name: cer_prob_product_log_last, running accuracy: 88.0
Method name: self_consistency, running accuracy: 89.33333333333333
Method name: p_true, running accuracy: 89.33333333333333
Method name: normilized_likelihood, running accuracy: 88.0
Method name: normilized_entropy, running accuracy: 87.66666666666667
Method name: topk_entropy, running accuracy: 87.66666666666667
Method name: window_entropy, running accuracy: 88.66666666666667

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|█████▉    | 299/500 [25:40:34<14:12:18, 254.42s/it, attention_weighted_confidence_acc=88.67%, cer_entropy_weighted_mean_all_acc=88.67%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=89.33%, p_true_acc=89.33%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.67%, topk_entropy_acc=87.67%, window_entropy_acc=88.67%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 300/500 [25:40:34<14:51:05, 267.33s/it, attention_weighted_confidence_acc=88.67%, cer_entropy_weighted_mean_all_acc=88.67%, cer_prob_product_log_last_acc=88.00%, self_consistency_acc=89.33%, p_true_acc=89.33%, normilized_likelihood_acc=88.00%, normilized_entropy_acc=87.67%, topk_entropy_acc=87.67%, window_entropy_acc=88.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 5.862520287601879
    Answer: 110
    Ground truth:  110
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 5.862520287601879
    Answer: 110
    Ground truth:  110
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 11.999749898910522
    Answer: 110
    Ground truth:  110
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 0.75
    Answer: 110
    Ground truth:  110
Method 5: p_true
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 9.38671875
    Answer: 110
    Ground truth:  110
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 8.981930211186409
    Answer: 110
    Ground truth:  110
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 8.959876000881195
    Answer: 110
    Ground truth:  110
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 7.902071893215179
    Answer: 110
    Ground truth:  110
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Tanya earned, we need to calculate her earnings for each da...
    Score: 14.009633243083954
    Answer: 110
    Ground truth:  110
Method name: attention_weighted_confidence, running accuracy: 88.70431893687709
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.70431893687709
Method name: cer_prob_product_log_last, running accuracy: 88.03986710963456
Method name: self_consistency, running accuracy: 89.3687707641196
Method name: p_true, running accuracy: 89.3687707641196
Method name: normilized_likelihood, running accuracy: 88.03986710963456
Method name: normilized_entropy, running accuracy: 87.70764119601328
Method name: topk_entropy, running accuracy: 87.70764119601328
Method name: window_entropy, running accuracy: 88.70431893687709

Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 300/500 [25:46:27<14:51:05, 267.33s/it, attention_weighted_confidence_acc=88.70%, cer_entropy_weighted_mean_all_acc=88.70%, cer_prob_product_log_last_acc=88.04%, self_consistency_acc=89.37%, p_true_acc=89.37%, normilized_likelihood_acc=88.04%, normilized_entropy_acc=87.71%, topk_entropy_acc=87.71%, window_entropy_acc=88.70%]
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 301/500 [25:46:27<16:12:02, 293.08s/it, attention_weighted_confidence_acc=88.70%, cer_entropy_weighted_mean_all_acc=88.70%, cer_prob_product_log_last_acc=88.04%, self_consistency_acc=89.37%, p_true_acc=89.37%, normilized_likelihood_acc=88.04%, normilized_entropy_acc=87.71%, topk_entropy_acc=87.71%, window_entropy_acc=88.70%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 3.4570346963229532
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 3.4570346963229532
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 6.764853239059448
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 0.4375
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 4.98974609375
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 8.549356251955032
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 8.166424944996834
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 6.688195511698723
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

1. We know the total age...
    Score: 9.303233206272125
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 88.74172185430463
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.74172185430463
Method name: cer_prob_product_log_last, running accuracy: 88.0794701986755
Method name: self_consistency, running accuracy: 89.40397350993378
Method name: p_true, running accuracy: 89.40397350993378
Method name: normilized_likelihood, running accuracy: 88.0794701986755
Method name: normilized_entropy, running accuracy: 87.74834437086093
Method name: topk_entropy, running accuracy: 87.74834437086093
Method name: window_entropy, running accuracy: 88.74172185430463
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 301/500 [25:58:13<16:12:02, 293.08s/it, attention_weighted_confidence_acc=88.74%, cer_entropy_weighted_mean_all_acc=88.74%, cer_prob_product_log_last_acc=88.08%, self_consistency_acc=89.40%, p_true_acc=89.40%, normilized_likelihood_acc=88.08%, normilized_entropy_acc=87.75%, topk_entropy_acc=87.75%, window_entropy_acc=88.74%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 302/500 [25:58:13<22:56:13, 417.04s/it, attention_weighted_confidence_acc=88.74%, cer_entropy_weighted_mean_all_acc=88.74%, cer_prob_product_log_last_acc=88.08%, self_consistency_acc=89.40%, p_true_acc=89.40%, normilized_likelihood_acc=88.08%, normilized_entropy_acc=87.75%, topk_entropy_acc=87.75%, window_entropy_acc=88.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 7.986767095886504
    Answer: 112
    Ground truth:  112
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 7.986767095886504
    Answer: 112
    Ground truth:  112
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 14.936211228370667
    Answer: 112
    Ground truth:  112
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 0.9375
    Answer: 112
    Ground truth:  112
Method 5: p_true
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 12.58984375
    Answer: 112
    Ground truth:  112
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 1.544636219739914
    Answer: 112
    Ground truth:  112
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 0.8343164771795273
    Answer: 112
    Ground truth:  112
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 0.8304881006479263
    Answer: 112
    Ground truth:  112
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of windows smashed, we need to calculate the number of ...
    Score: 2.841622769832611
    Answer: 112
    Ground truth:  112
Method name: attention_weighted_confidence, running accuracy: 88.77887788778878
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.77887788778878
Method name: cer_prob_product_log_last, running accuracy: 88.11881188118812
Method name: self_consistency, running accuracy: 89.43894389438944
Method name: p_true, running accuracy: 89.43894389438944
Method name: normilized_likelihood, running accuracy: 88.11881188118812
Method name: normilized_entropy, running accuracy: 87.78877887788778
Method name: topk_entropy, running accuracy: 87.78877887788778
Method name: window_entropy, running accuracy: 88.77887788778878
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 302/500 [26:04:03<22:56:13, 417.04s/it, attention_weighted_confidence_acc=88.78%, cer_entropy_weighted_mean_all_acc=88.78%, cer_prob_product_log_last_acc=88.12%, self_consistency_acc=89.44%, p_true_acc=89.44%, normilized_likelihood_acc=88.12%, normilized_entropy_acc=87.79%, topk_entropy_acc=87.79%, window_entropy_acc=88.78%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 303/500 [26:04:03<21:43:11, 396.91s/it, attention_weighted_confidence_acc=88.78%, cer_entropy_weighted_mean_all_acc=88.78%, cer_prob_product_log_last_acc=88.12%, self_consistency_acc=89.44%, p_true_acc=89.44%, normilized_likelihood_acc=88.12%, normilized_entropy_acc=87.79%, topk_entropy_acc=87.79%, window_entropy_acc=88.78%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Tasha made from lemonade sales, we need to first determine ...
    Score: 1.0005027207729607
    Answer: 26
    Ground truth:  26
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Tasha made from lemonade sales, we need to first determine ...
    Score: 1.0005027207729607
    Answer: 26
    Ground truth:  26
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  To find out how much Tasha made from lemonade sales, we first need to d...
    Score: 1.9933528900146484
    Answer: 6
    Ground truth:  26
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  To find out how much Tasha made from lemonade sales, we first need to d...
    Score: 0.125
    Answer: 6
    Ground truth:  26
Method 5: p_true
  Batch 1:
    Text: To find out how much Tasha made from lemonade sales, we need to first determine ...
    Score: 1.59375
    Answer: 26
    Ground truth:  26
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Tasha made from lemonade sales, we need to first determine ...
    Score: 2.6246642470359802
    Answer: 26
    Ground truth:  26
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Tasha made from lemonade sales, we need to first determine ...
    Score: 2.5347977578639984
    Answer: 26
    Ground truth:  26
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Tasha made from lemonade sales, we need to first determine ...
    Score: 2.1299409568309784
    Answer: 26
    Ground truth:  26
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Tasha made from lemonade sales, we need to first determine ...
    Score: 2.9698179960250854
    Answer: 26
    Ground truth:  26
Method name: attention_weighted_confidence, running accuracy: 88.81578947368422
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.81578947368422
Method name: cer_prob_product_log_last, running accuracy: 87.82894736842105
Method name: self_consistency, running accuracy: 89.14473684210526
Method name: p_true, running accuracy: 89.47368421052632
Method name: normilized_likelihood, running accuracy: 88.1578947368421
Method name: normilized_entropy, running accuracy: 87.82894736842105
Method name: topk_entropy, running accuracy: 87.82894736842105
Method name: window_entropy, running accuracy: 88.81578947368422
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 303/500 [26:19:45<21:43:11, 396.91s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=87.83%, self_consistency_acc=89.14%, p_true_acc=89.47%, normilized_likelihood_acc=88.16%, normilized_entropy_acc=87.83%, topk_entropy_acc=87.83%, window_entropy_acc=88.82%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 304/500 [26:19:45<30:30:30, 560.36s/it, attention_weighted_confidence_acc=88.82%, cer_entropy_weighted_mean_all_acc=88.82%, cer_prob_product_log_last_acc=87.83%, self_consistency_acc=89.14%, p_true_acc=89.47%, normilized_likelihood_acc=88.16%, normilized_entropy_acc=87.83%, topk_entropy_acc=87.83%, window_entropy_acc=88.82%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 8.541318682247448
    Answer: 27
    Ground truth:  27
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 8.541318682247448
    Answer: 27
    Ground truth:  27
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 15.998148798942566
    Answer: 27
    Ground truth:  27
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 1.0
    Answer: 27
    Ground truth:  27
Method 5: p_true
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 10.455078125
    Answer: 27
    Ground truth:  27
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 2.5765509009361267
    Answer: 27
    Ground truth:  27
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 1.0726467072963715
    Answer: 27
    Ground truth:  27
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 1.0723775774240494
    Answer: 27
    Ground truth:  27
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step.

1. Morisette brought 5 apples and 8 oranges.
2. Kael ...
    Score: 4.778425216674805
    Answer: 27
    Ground truth:  27
Method name: attention_weighted_confidence, running accuracy: 88.85245901639345
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.85245901639345
Method name: cer_prob_product_log_last, running accuracy: 87.8688524590164
Method name: self_consistency, running accuracy: 89.18032786885246
Method name: p_true, running accuracy: 89.50819672131148
Method name: normilized_likelihood, running accuracy: 88.19672131147541
Method name: normilized_entropy, running accuracy: 87.8688524590164
Method name: topk_entropy, running accuracy: 87.8688524590164
Method name: window_entropy, running accuracy: 88.85245901639345
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 304/500 [26:24:08<30:30:30, 560.36s/it, attention_weighted_confidence_acc=88.85%, cer_entropy_weighted_mean_all_acc=88.85%, cer_prob_product_log_last_acc=87.87%, self_consistency_acc=89.18%, p_true_acc=89.51%, normilized_likelihood_acc=88.20%, normilized_entropy_acc=87.87%, topk_entropy_acc=87.87%, window_entropy_acc=88.85%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 305/500 [26:24:08<25:30:53, 471.04s/it, attention_weighted_confidence_acc=88.85%, cer_entropy_weighted_mean_all_acc=88.85%, cer_prob_product_log_last_acc=87.87%, self_consistency_acc=89.18%, p_true_acc=89.51%, normilized_likelihood_acc=88.20%, normilized_entropy_acc=87.87%, topk_entropy_acc=87.87%, window_entropy_acc=88.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 5.8789195322858046
    Answer: 92
    Ground truth:  92
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 5.8789195322858046
    Answer: 92
    Ground truth:  92
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 11.999852776527405
    Answer: 92
    Ground truth:  92
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 0.75
    Answer: 92
    Ground truth:  92
Method 5: p_true
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 10.71875
    Answer: 92
    Ground truth:  92
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 3.1487506181001663
    Answer: 92
    Ground truth:  92
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 3.3925454318523407
    Answer: 92
    Ground truth:  92
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 3.071043372154236
    Answer: 92
    Ground truth:  92
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of books Wendy needs to carry, we need to first determi...
    Score: 15.79758208990097
    Answer: 92
    Ground truth:  92
Method name: attention_weighted_confidence, running accuracy: 88.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 87.90849673202614
Method name: self_consistency, running accuracy: 89.2156862745098
Method name: p_true, running accuracy: 89.54248366013073
Method name: normilized_likelihood, running accuracy: 88.23529411764706
Method name: normilized_entropy, running accuracy: 87.90849673202614
Method name: topk_entropy, running accuracy: 87.90849673202614
Method name: window_entropy, running accuracy: 88.88888888888889
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 305/500 [26:28:29<25:30:53, 471.04s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=87.91%, self_consistency_acc=89.22%, p_true_acc=89.54%, normilized_likelihood_acc=88.24%, normilized_entropy_acc=87.91%, topk_entropy_acc=87.91%, window_entropy_acc=88.89%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 306/500 [26:28:29<21:59:35, 408.12s/it, attention_weighted_confidence_acc=88.89%, cer_entropy_weighted_mean_all_acc=88.89%, cer_prob_product_log_last_acc=87.91%, self_consistency_acc=89.22%, p_true_acc=89.54%, normilized_likelihood_acc=88.24%, normilized_entropy_acc=87.91%, topk_entropy_acc=87.91%, window_entropy_acc=88.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 7.242352736387135
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 7.242352736387135
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 13.990510821342468
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 0.875
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 10.96875
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 11.136730030179024
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 11.347258672118187
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 10.18436148762703
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the information step by step:

1. Doxa sliced an apple into 8 p...
    Score: 21.56286245584488
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 88.92508143322475
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.92508143322475
Method name: cer_prob_product_log_last, running accuracy: 87.94788273615634
Method name: self_consistency, running accuracy: 89.25081433224756
Method name: p_true, running accuracy: 89.57654723127035
Method name: normilized_likelihood, running accuracy: 88.27361563517914
Method name: normilized_entropy, running accuracy: 87.94788273615634
Method name: topk_entropy, running accuracy: 87.94788273615634
Method name: window_entropy, running accuracy: 88.92508143322475
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 306/500 [26:32:50<21:59:35, 408.12s/it, attention_weighted_confidence_acc=88.93%, cer_entropy_weighted_mean_all_acc=88.93%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=89.25%, p_true_acc=89.58%, normilized_likelihood_acc=88.27%, normilized_entropy_acc=87.95%, topk_entropy_acc=87.95%, window_entropy_acc=88.93%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████▏   | 307/500 [26:32:50<19:31:13, 364.11s/it, attention_weighted_confidence_acc=88.93%, cer_entropy_weighted_mean_all_acc=88.93%, cer_prob_product_log_last_acc=87.95%, self_consistency_acc=89.25%, p_true_acc=89.58%, normilized_likelihood_acc=88.27%, normilized_entropy_acc=87.95%, topk_entropy_acc=87.95%, window_entropy_acc=88.93%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 7.644003821011487
    Answer: 27
    Ground truth:  27
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 7.644003821011487
    Answer: 27
    Ground truth:  27
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 15.96985536813736
    Answer: 27
    Ground truth:  27
Method 4: self_consistency
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 1.0
    Answer: 27
    Ground truth:  27
Method 5: p_true
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 10.515625
    Answer: 27
    Ground truth:  27
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 12.448548927903175
    Answer: 27
    Ground truth:  27
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 12.124543368816376
    Answer: 27
    Ground truth:  27
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 10.550054460763931
    Answer: 27
    Ground truth:  27
Method 9: window_entropy
  Batch 1:
    Text: To find out how many great grand-babies will be there for Great Grandma Jones to...
    Score: 20.89380669593811
    Answer: 27
    Ground truth:  27
Method name: attention_weighted_confidence, running accuracy: 88.96103896103897
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.96103896103897
Method name: cer_prob_product_log_last, running accuracy: 87.98701298701299
Method name: self_consistency, running accuracy: 89.28571428571429
Method name: p_true, running accuracy: 89.6103896103896
Method name: normilized_likelihood, running accuracy: 88.31168831168831
Method name: normilized_entropy, running accuracy: 87.98701298701299
Method name: topk_entropy, running accuracy: 87.98701298701299
Method name: window_entropy, running accuracy: 88.96103896103897
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████▏   | 307/500 [26:36:19<19:31:13, 364.11s/it, attention_weighted_confidence_acc=88.96%, cer_entropy_weighted_mean_all_acc=88.96%, cer_prob_product_log_last_acc=87.99%, self_consistency_acc=89.29%, p_true_acc=89.61%, normilized_likelihood_acc=88.31%, normilized_entropy_acc=87.99%, topk_entropy_acc=87.99%, window_entropy_acc=88.96%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 308/500 [26:36:19<16:56:03, 317.52s/it, attention_weighted_confidence_acc=88.96%, cer_entropy_weighted_mean_all_acc=88.96%, cer_prob_product_log_last_acc=87.99%, self_consistency_acc=89.29%, p_true_acc=89.61%, normilized_likelihood_acc=88.31%, normilized_entropy_acc=87.99%, topk_entropy_acc=87.99%, window_entropy_acc=88.96%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 7.694518628595496
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 7.694518628595496
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 14.998909056186676
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 0.9375
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 14.3046875
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 2.103845953941345
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 1.164743572473526
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 1.167289286851883
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To calculate Melissa's net hourly pay, we'll start by finding her total income a...
    Score: 3.0707847476005554
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 88.9967637540453
Method name: cer_entropy_weighted_mean_all, running accuracy: 88.9967637540453
Method name: cer_prob_product_log_last, running accuracy: 88.02588996763754
Method name: self_consistency, running accuracy: 89.32038834951457
Method name: p_true, running accuracy: 89.64401294498381
Method name: normilized_likelihood, running accuracy: 88.3495145631068
Method name: normilized_entropy, running accuracy: 88.02588996763754
Method name: topk_entropy, running accuracy: 88.02588996763754
Method name: window_entropy, running accuracy: 88.9967637540453
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 308/500 [26:41:26<16:56:03, 317.52s/it, attention_weighted_confidence_acc=89.00%, cer_entropy_weighted_mean_all_acc=89.00%, cer_prob_product_log_last_acc=88.03%, self_consistency_acc=89.32%, p_true_acc=89.64%, normilized_likelihood_acc=88.35%, normilized_entropy_acc=88.03%, topk_entropy_acc=88.03%, window_entropy_acc=89.00%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 309/500 [26:41:26<16:40:44, 314.37s/it, attention_weighted_confidence_acc=89.00%, cer_entropy_weighted_mean_all_acc=89.00%, cer_prob_product_log_last_acc=88.03%, self_consistency_acc=89.32%, p_true_acc=89.64%, normilized_likelihood_acc=88.35%, normilized_entropy_acc=88.03%, topk_entropy_acc=88.03%, window_entropy_acc=89.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 7.205312210169389
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 7.205312210169389
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 14.999360740184784
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 0.9375
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 13.9921875
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 2.831634134054184
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 3.673030346632004
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 3.5501484721899033
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of slices John's friends will eat, we multiply the numb...
    Score: 10.082521796226501
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 89.03225806451613
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.03225806451613
Method name: cer_prob_product_log_last, running accuracy: 88.06451612903226
Method name: self_consistency, running accuracy: 89.35483870967742
Method name: p_true, running accuracy: 89.6774193548387
Method name: normilized_likelihood, running accuracy: 88.38709677419355
Method name: normilized_entropy, running accuracy: 88.06451612903226
Method name: topk_entropy, running accuracy: 88.06451612903226
Method name: window_entropy, running accuracy: 89.03225806451613
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 309/500 [26:45:11<16:40:44, 314.37s/it, attention_weighted_confidence_acc=89.03%, cer_entropy_weighted_mean_all_acc=89.03%, cer_prob_product_log_last_acc=88.06%, self_consistency_acc=89.35%, p_true_acc=89.68%, normilized_likelihood_acc=88.39%, normilized_entropy_acc=88.06%, topk_entropy_acc=88.06%, window_entropy_acc=89.03%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 310/500 [26:45:11<15:10:21, 287.48s/it, attention_weighted_confidence_acc=89.03%, cer_entropy_weighted_mean_all_acc=89.03%, cer_prob_product_log_last_acc=88.06%, self_consistency_acc=89.35%, p_true_acc=89.68%, normilized_likelihood_acc=88.39%, normilized_entropy_acc=88.06%, topk_entropy_acc=88.06%, window_entropy_acc=89.03%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 8.000527549637765
    Answer: 80
    Ground truth:  80
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 8.000527549637765
    Answer: 80
    Ground truth:  80
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 15.999879240989685
    Answer: 80
    Ground truth:  80
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 1.0
    Answer: 80
    Ground truth:  80
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 14.08203125
    Answer: 80
    Ground truth:  80
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 3.04349085688591
    Answer: 80
    Ground truth:  80
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 2.1357623636722565
    Answer: 80
    Ground truth:  80
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 2.128881514072418
    Answer: 80
    Ground truth:  80
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Greta bakes 30 cookies.
2. Celinda bakes t...
    Score: 3.4038230180740356
    Answer: 80
    Ground truth:  80
Method name: attention_weighted_confidence, running accuracy: 89.06752411575563
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.06752411575563
Method name: cer_prob_product_log_last, running accuracy: 88.10289389067523
Method name: self_consistency, running accuracy: 89.38906752411575
Method name: p_true, running accuracy: 89.71061093247589
Method name: normilized_likelihood, running accuracy: 88.42443729903538
Method name: normilized_entropy, running accuracy: 88.10289389067523
Method name: topk_entropy, running accuracy: 88.10289389067523
Method name: window_entropy, running accuracy: 89.06752411575563
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 310/500 [26:48:25<15:10:21, 287.48s/it, attention_weighted_confidence_acc=89.07%, cer_entropy_weighted_mean_all_acc=89.07%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=89.39%, p_true_acc=89.71%, normilized_likelihood_acc=88.42%, normilized_entropy_acc=88.10%, topk_entropy_acc=88.10%, window_entropy_acc=89.07%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 311/500 [26:48:25<13:37:19, 259.47s/it, attention_weighted_confidence_acc=89.07%, cer_entropy_weighted_mean_all_acc=89.07%, cer_prob_product_log_last_acc=88.10%, self_consistency_acc=89.39%, p_true_acc=89.71%, normilized_likelihood_acc=88.42%, normilized_entropy_acc=88.10%, topk_entropy_acc=88.10%, window_entropy_acc=89.07%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 8.469700714902439
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 8.469700714902439
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 15.999720573425293
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 15.51953125
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 1.4242845177650452
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 1.0526202768087387
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 1.0496131479740143
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find the number of books each child got, we need to first find the total numb...
    Score: 3.9562183916568756
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 89.1025641025641
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.1025641025641
Method name: cer_prob_product_log_last, running accuracy: 88.14102564102564
Method name: self_consistency, running accuracy: 89.42307692307693
Method name: p_true, running accuracy: 89.74358974358975
Method name: normilized_likelihood, running accuracy: 88.46153846153845
Method name: normilized_entropy, running accuracy: 88.14102564102564
Method name: topk_entropy, running accuracy: 88.14102564102564
Method name: window_entropy, running accuracy: 89.1025641025641
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 311/500 [26:52:04<13:37:19, 259.47s/it, attention_weighted_confidence_acc=89.10%, cer_entropy_weighted_mean_all_acc=89.10%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=89.42%, p_true_acc=89.74%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=88.14%, topk_entropy_acc=88.14%, window_entropy_acc=89.10%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 312/500 [26:52:04<12:55:07, 247.38s/it, attention_weighted_confidence_acc=89.10%, cer_entropy_weighted_mean_all_acc=89.10%, cer_prob_product_log_last_acc=88.14%, self_consistency_acc=89.42%, p_true_acc=89.74%, normilized_likelihood_acc=88.46%, normilized_entropy_acc=88.14%, topk_entropy_acc=88.14%, window_entropy_acc=89.10%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 8.129785592163183
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 8.129785592163183
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 15.775696098804474
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 1.0
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 15.1015625
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 2.561115100979805
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 1.7993010878562927
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 1.7893832176923752
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To find out how many miles Ahito ran, we need to first determine how many miles ...
    Score: 5.649755835533142
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 89.1373801916933
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.1373801916933
Method name: cer_prob_product_log_last, running accuracy: 88.17891373801918
Method name: self_consistency, running accuracy: 89.45686900958466
Method name: p_true, running accuracy: 89.77635782747603
Method name: normilized_likelihood, running accuracy: 88.49840255591054
Method name: normilized_entropy, running accuracy: 88.17891373801918
Method name: topk_entropy, running accuracy: 88.17891373801918
Method name: window_entropy, running accuracy: 89.1373801916933
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 312/500 [26:56:04<12:55:07, 247.38s/it, attention_weighted_confidence_acc=89.14%, cer_entropy_weighted_mean_all_acc=89.14%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=89.46%, p_true_acc=89.78%, normilized_likelihood_acc=88.50%, normilized_entropy_acc=88.18%, topk_entropy_acc=88.18%, window_entropy_acc=89.14%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 313/500 [26:56:04<12:44:09, 245.19s/it, attention_weighted_confidence_acc=89.14%, cer_entropy_weighted_mean_all_acc=89.14%, cer_prob_product_log_last_acc=88.18%, self_consistency_acc=89.46%, p_true_acc=89.78%, normilized_likelihood_acc=88.50%, normilized_entropy_acc=88.18%, topk_entropy_acc=88.18%, window_entropy_acc=89.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 5.868235356999139
    Answer: 21
    Ground truth:  21
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 5.868235356999139
    Answer: 21
    Ground truth:  21
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 12.995718121528625
    Answer: 21
    Ground truth:  21
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 0.8125
    Answer: 21
    Ground truth:  21
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 6.236083984375
    Answer: 21
    Ground truth:  21
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 9.810336992144585
    Answer: 21
    Ground truth:  21
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 9.173043102025986
    Answer: 21
    Ground truth:  21
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 7.970292419195175
    Answer: 21
    Ground truth:  21
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Andrea had $36 in the beginning.
2. She sp...
    Score: 17.18349003791809
    Answer: 21
    Ground truth:  21
Method name: attention_weighted_confidence, running accuracy: 89.171974522293
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.171974522293
Method name: cer_prob_product_log_last, running accuracy: 88.21656050955414
Method name: self_consistency, running accuracy: 89.49044585987261
Method name: p_true, running accuracy: 89.80891719745223
Method name: normilized_likelihood, running accuracy: 88.53503184713377
Method name: normilized_entropy, running accuracy: 88.21656050955414
Method name: topk_entropy, running accuracy: 88.21656050955414
Method name: window_entropy, running accuracy: 89.171974522293
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 313/500 [27:00:08<12:44:09, 245.19s/it, attention_weighted_confidence_acc=89.17%, cer_entropy_weighted_mean_all_acc=89.17%, cer_prob_product_log_last_acc=88.22%, self_consistency_acc=89.49%, p_true_acc=89.81%, normilized_likelihood_acc=88.54%, normilized_entropy_acc=88.22%, topk_entropy_acc=88.22%, window_entropy_acc=89.17%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 314/500 [27:00:08<12:38:47, 244.77s/it, attention_weighted_confidence_acc=89.17%, cer_entropy_weighted_mean_all_acc=89.17%, cer_prob_product_log_last_acc=88.22%, self_consistency_acc=89.49%, p_true_acc=89.81%, normilized_likelihood_acc=88.54%, normilized_entropy_acc=88.22%, topk_entropy_acc=88.22%, window_entropy_acc=89.17%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 8.604373734249094
    Answer: 150
    Ground truth:  150
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 8.604373734249094
    Answer: 150
    Ground truth:  150
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 15.922604262828827
    Answer: 150
    Ground truth:  150
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 1.0
    Answer: 150
    Ground truth:  150
Method 5: p_true
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 14.3515625
    Answer: 150
    Ground truth:  150
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 1.9772878140211105
    Answer: 150
    Ground truth:  150
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 0.8319527208805084
    Answer: 150
    Ground truth:  150
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 0.8268779814243317
    Answer: 150
    Ground truth:  150
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of each uniform, we need to calculate the cost of each it...
    Score: 4.490492194890976
    Answer: 150
    Ground truth:  150
Method name: attention_weighted_confidence, running accuracy: 89.20634920634922
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.20634920634922
Method name: cer_prob_product_log_last, running accuracy: 88.25396825396825
Method name: self_consistency, running accuracy: 89.52380952380953
Method name: p_true, running accuracy: 89.84126984126985
Method name: normilized_likelihood, running accuracy: 88.57142857142857
Method name: normilized_entropy, running accuracy: 88.25396825396825
Method name: topk_entropy, running accuracy: 88.25396825396825
Method name: window_entropy, running accuracy: 89.20634920634922
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 314/500 [27:04:40<12:38:47, 244.77s/it, attention_weighted_confidence_acc=89.21%, cer_entropy_weighted_mean_all_acc=89.21%, cer_prob_product_log_last_acc=88.25%, self_consistency_acc=89.52%, p_true_acc=89.84%, normilized_likelihood_acc=88.57%, normilized_entropy_acc=88.25%, topk_entropy_acc=88.25%, window_entropy_acc=89.21%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 315/500 [27:04:40<13:00:08, 253.02s/it, attention_weighted_confidence_acc=89.21%, cer_entropy_weighted_mean_all_acc=89.21%, cer_prob_product_log_last_acc=88.25%, self_consistency_acc=89.52%, p_true_acc=89.84%, normilized_likelihood_acc=88.57%, normilized_entropy_acc=88.25%, topk_entropy_acc=88.25%, window_entropy_acc=89.21%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 6.119303796843999
    Answer: 272
    Ground truth:  272
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 6.119303796843999
    Answer: 272
    Ground truth:  272
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 11.875432252883911
    Answer: 272
    Ground truth:  272
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 0.75
    Answer: 272
    Ground truth:  272
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 11.1484375
    Answer: 272
    Ground truth:  272
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 1.6923639923334122
    Answer: 272
    Ground truth:  272
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 1.791192889213562
    Answer: 272
    Ground truth:  272
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 1.7406153976917267
    Answer: 272
    Ground truth:  272
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to calculate the total hours Kimo spends in class...
    Score: 6.7957698702812195
    Answer: 272
    Ground truth:  272
Method name: attention_weighted_confidence, running accuracy: 89.24050632911393
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.24050632911393
Method name: cer_prob_product_log_last, running accuracy: 88.29113924050634
Method name: self_consistency, running accuracy: 89.55696202531645
Method name: p_true, running accuracy: 89.87341772151899
Method name: normilized_likelihood, running accuracy: 88.60759493670885
Method name: normilized_entropy, running accuracy: 88.29113924050634
Method name: topk_entropy, running accuracy: 88.29113924050634
Method name: window_entropy, running accuracy: 89.24050632911393
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 315/500 [27:10:06<13:00:08, 253.02s/it, attention_weighted_confidence_acc=89.24%, cer_entropy_weighted_mean_all_acc=89.24%, cer_prob_product_log_last_acc=88.29%, self_consistency_acc=89.56%, p_true_acc=89.87%, normilized_likelihood_acc=88.61%, normilized_entropy_acc=88.29%, topk_entropy_acc=88.29%, window_entropy_acc=89.24%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 316/500 [27:10:06<14:02:45, 274.81s/it, attention_weighted_confidence_acc=89.24%, cer_entropy_weighted_mean_all_acc=89.24%, cer_prob_product_log_last_acc=88.29%, self_consistency_acc=89.56%, p_true_acc=89.87%, normilized_likelihood_acc=88.61%, normilized_entropy_acc=88.29%, topk_entropy_acc=88.29%, window_entropy_acc=89.24%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 8.185013078525618
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 8.185013078525618
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 15.99998414516449
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 1.0
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 14.375
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 2.4175145030021667
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 2.1014866530895233
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 1.9835238605737686
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how many hours Gary spends walking around the edge of the park, we n...
    Score: 10.405138194561005
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 89.27444794952682
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.27444794952682
Method name: cer_prob_product_log_last, running accuracy: 88.32807570977917
Method name: self_consistency, running accuracy: 89.58990536277602
Method name: p_true, running accuracy: 89.90536277602523
Method name: normilized_likelihood, running accuracy: 88.64353312302839
Method name: normilized_entropy, running accuracy: 88.32807570977917
Method name: topk_entropy, running accuracy: 88.32807570977917
Method name: window_entropy, running accuracy: 89.27444794952682
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 316/500 [27:14:43<14:02:45, 274.81s/it, attention_weighted_confidence_acc=89.27%, cer_entropy_weighted_mean_all_acc=89.27%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=89.59%, p_true_acc=89.91%, normilized_likelihood_acc=88.64%, normilized_entropy_acc=88.33%, topk_entropy_acc=88.33%, window_entropy_acc=89.27%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 317/500 [27:14:43<14:00:16, 275.50s/it, attention_weighted_confidence_acc=89.27%, cer_entropy_weighted_mean_all_acc=89.27%, cer_prob_product_log_last_acc=88.33%, self_consistency_acc=89.59%, p_true_acc=89.91%, normilized_likelihood_acc=88.64%, normilized_entropy_acc=88.33%, topk_entropy_acc=88.33%, window_entropy_acc=89.27%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 7.609963394290685
    Answer: 2800
    Ground truth:  2800
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 7.609963394290685
    Answer: 2800
    Ground truth:  2800
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 2.4058469642828575
    Answer: 2800
    Ground truth:  2800
Method 4: self_consistency
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 0.9375
    Answer: 2800
    Ground truth:  2800
Method 5: p_true
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 14.00390625
    Answer: 2800
    Ground truth:  2800
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 9.028344377875328
    Answer: 2800
    Ground truth:  2800
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 9.65963651239872
    Answer: 2800
    Ground truth:  2800
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 8.493820667266846
    Answer: 2800
    Ground truth:  2800
Method 9: window_entropy
  Batch 1:
    Text: To find out how much gravel Gary's truck can carry, we need to first determine h...
    Score: 21.786310255527496
    Answer: 2800
    Ground truth:  2800
Method name: attention_weighted_confidence, running accuracy: 89.30817610062893
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.30817610062893
Method name: cer_prob_product_log_last, running accuracy: 88.36477987421384
Method name: self_consistency, running accuracy: 89.62264150943396
Method name: p_true, running accuracy: 89.937106918239
Method name: normilized_likelihood, running accuracy: 88.67924528301887
Method name: normilized_entropy, running accuracy: 88.36477987421384
Method name: topk_entropy, running accuracy: 88.36477987421384
Method name: window_entropy, running accuracy: 89.30817610062893
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 317/500 [27:20:58<14:00:16, 275.50s/it, attention_weighted_confidence_acc=89.31%, cer_entropy_weighted_mean_all_acc=89.31%, cer_prob_product_log_last_acc=88.36%, self_consistency_acc=89.62%, p_true_acc=89.94%, normilized_likelihood_acc=88.68%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.31%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▎   | 318/500 [27:20:58<15:26:06, 305.31s/it, attention_weighted_confidence_acc=89.31%, cer_entropy_weighted_mean_all_acc=89.31%, cer_prob_product_log_last_acc=88.36%, self_consistency_acc=89.62%, p_true_acc=89.94%, normilized_likelihood_acc=88.68%, normilized_entropy_acc=88.36%, topk_entropy_acc=88.36%, window_entropy_acc=89.31%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 7.407802894369947
    Answer: 93
    Ground truth:  93
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 7.407802894369947
    Answer: 93
    Ground truth:  93
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 14.948509693145752
    Answer: 93
    Ground truth:  93
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 0.9375
    Answer: 93
    Ground truth:  93
Method 5: p_true
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 13.21484375
    Answer: 93
    Ground truth:  93
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 1.1650633066892624
    Answer: 93
    Ground truth:  93
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 0.9090677350759506
    Answer: 93
    Ground truth:  93
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 0.9085796624422073
    Answer: 93
    Ground truth:  93
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Jen spends on food in the month of May, we need to follow t...
    Score: 3.6858684420585632
    Answer: 93
    Ground truth:  93
Method name: attention_weighted_confidence, running accuracy: 89.34169278996865
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.34169278996865
Method name: cer_prob_product_log_last, running accuracy: 88.40125391849529
Method name: self_consistency, running accuracy: 89.65517241379311
Method name: p_true, running accuracy: 89.96865203761756
Method name: normilized_likelihood, running accuracy: 88.71473354231975
Method name: normilized_entropy, running accuracy: 88.40125391849529
Method name: topk_entropy, running accuracy: 88.40125391849529
Method name: window_entropy, running accuracy: 89.34169278996865
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▎   | 318/500 [27:24:23<15:26:06, 305.31s/it, attention_weighted_confidence_acc=89.34%, cer_entropy_weighted_mean_all_acc=89.34%, cer_prob_product_log_last_acc=88.40%, self_consistency_acc=89.66%, p_true_acc=89.97%, normilized_likelihood_acc=88.71%, normilized_entropy_acc=88.40%, topk_entropy_acc=88.40%, window_entropy_acc=89.34%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 319/500 [27:24:23<13:50:17, 275.23s/it, attention_weighted_confidence_acc=89.34%, cer_entropy_weighted_mean_all_acc=89.34%, cer_prob_product_log_last_acc=88.40%, self_consistency_acc=89.66%, p_true_acc=89.97%, normilized_likelihood_acc=88.71%, normilized_entropy_acc=88.40%, topk_entropy_acc=88.40%, window_entropy_acc=89.34%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 8.230438078017535
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 8.230438078017535
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 15.999727606773376
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 1.0
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 14.421875
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 1.9248526096343994
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 1.5144479870796204
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 1.5074065178632736
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: First, let's find the total number of Chinese people in the race. We know there ...
    Score: 3.980224132537842
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 89.375
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.375
Method name: cer_prob_product_log_last, running accuracy: 88.4375
Method name: self_consistency, running accuracy: 89.6875
Method name: p_true, running accuracy: 90.0
Method name: normilized_likelihood, running accuracy: 88.75
Method name: normilized_entropy, running accuracy: 88.4375
Method name: topk_entropy, running accuracy: 88.4375
Method name: window_entropy, running accuracy: 89.375
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 319/500 [27:28:06<13:50:17, 275.23s/it, attention_weighted_confidence_acc=89.38%, cer_entropy_weighted_mean_all_acc=89.38%, cer_prob_product_log_last_acc=88.44%, self_consistency_acc=89.69%, p_true_acc=90.00%, normilized_likelihood_acc=88.75%, normilized_entropy_acc=88.44%, topk_entropy_acc=88.44%, window_entropy_acc=89.38%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 320/500 [27:28:06<12:59:02, 259.68s/it, attention_weighted_confidence_acc=89.38%, cer_entropy_weighted_mean_all_acc=89.38%, cer_prob_product_log_last_acc=88.44%, self_consistency_acc=89.69%, p_true_acc=90.00%, normilized_likelihood_acc=88.75%, normilized_entropy_acc=88.44%, topk_entropy_acc=88.44%, window_entropy_acc=89.38%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 8.115369174440323
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 8.115369174440323
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 15.82299554347992
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 1.0
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 14.62890625
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 2.7375826686620712
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 1.8806803673505783
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 1.8571446686983109
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step to solve the problem.

Step 1: We know that the tower h...
    Score: 3.3880075812339783
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 89.40809968847351
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.40809968847351
Method name: cer_prob_product_log_last, running accuracy: 88.47352024922118
Method name: self_consistency, running accuracy: 89.7196261682243
Method name: p_true, running accuracy: 90.03115264797508
Method name: normilized_likelihood, running accuracy: 88.78504672897196
Method name: normilized_entropy, running accuracy: 88.47352024922118
Method name: topk_entropy, running accuracy: 88.47352024922118
Method name: window_entropy, running accuracy: 89.40809968847351
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 320/500 [27:32:01<12:59:02, 259.68s/it, attention_weighted_confidence_acc=89.41%, cer_entropy_weighted_mean_all_acc=89.41%, cer_prob_product_log_last_acc=88.47%, self_consistency_acc=89.72%, p_true_acc=90.03%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.47%, topk_entropy_acc=88.47%, window_entropy_acc=89.41%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 321/500 [27:32:01<12:32:26, 252.21s/it, attention_weighted_confidence_acc=89.41%, cer_entropy_weighted_mean_all_acc=89.41%, cer_prob_product_log_last_acc=88.47%, self_consistency_acc=89.72%, p_true_acc=90.03%, normilized_likelihood_acc=88.79%, normilized_entropy_acc=88.47%, topk_entropy_acc=88.47%, window_entropy_acc=89.41%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 2.369977875930073
    Answer: 13
    Ground truth:  13
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 2.369977875930073
    Answer: 13
    Ground truth:  13
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 4.284999459981918
    Answer: 13
    Ground truth:  13
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 0.3125
    Answer: 13
    Ground truth:  13
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 4.67578125
    Answer: 13
    Ground truth:  13
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 4.533355414867401
    Answer: 13
    Ground truth:  13
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 4.358180999755859
    Answer: 13
    Ground truth:  13
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 3.5901553332805634
    Answer: 13
    Ground truth:  13
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Let's represent the...
    Score: 5.76947557926178
    Answer: 13
    Ground truth:  13
Method name: attention_weighted_confidence, running accuracy: 89.44099378881988
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.44099378881988
Method name: cer_prob_product_log_last, running accuracy: 88.50931677018633
Method name: self_consistency, running accuracy: 89.75155279503106
Method name: p_true, running accuracy: 90.06211180124224
Method name: normilized_likelihood, running accuracy: 88.81987577639751
Method name: normilized_entropy, running accuracy: 88.50931677018633
Method name: topk_entropy, running accuracy: 88.50931677018633
Method name: window_entropy, running accuracy: 89.44099378881988
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 321/500 [27:44:20<12:32:26, 252.21s/it, attention_weighted_confidence_acc=89.44%, cer_entropy_weighted_mean_all_acc=89.44%, cer_prob_product_log_last_acc=88.51%, self_consistency_acc=89.75%, p_true_acc=90.06%, normilized_likelihood_acc=88.82%, normilized_entropy_acc=88.51%, topk_entropy_acc=88.51%, window_entropy_acc=89.44%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 322/500 [27:44:20<19:41:41, 398.32s/it, attention_weighted_confidence_acc=89.44%, cer_entropy_weighted_mean_all_acc=89.44%, cer_prob_product_log_last_acc=88.51%, self_consistency_acc=89.75%, p_true_acc=90.06%, normilized_likelihood_acc=88.82%, normilized_entropy_acc=88.51%, topk_entropy_acc=88.51%, window_entropy_acc=89.44%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 6.861991877920178
    Answer: 3160
    Ground truth:  3160
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 6.861991877920178
    Answer: 3160
    Ground truth:  3160
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 6.750339243129069
    Answer: 3160
    Ground truth:  3160
Method 4: self_consistency
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 0.875
    Answer: 3160
    Ground truth:  3160
Method 5: p_true
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 9.208984375
    Answer: 3160
    Ground truth:  3160
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 10.883069306612015
    Answer: 3160
    Ground truth:  3160
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 9.294779136776924
    Answer: 3160
    Ground truth:  3160
Method 8: topk_entropy
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 8.031628519296646
    Answer: 3160
    Ground truth:  3160
Method 9: window_entropy
  Batch 1:
    Text: To find the total area of the house, we need to find the area of each room and a...
    Score: 18.767773032188416
    Answer: 3160
    Ground truth:  3160
Method name: attention_weighted_confidence, running accuracy: 89.47368421052632
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.47368421052632
Method name: cer_prob_product_log_last, running accuracy: 88.54489164086688
Method name: self_consistency, running accuracy: 89.78328173374614
Method name: p_true, running accuracy: 90.09287925696594
Method name: normilized_likelihood, running accuracy: 88.85448916408669
Method name: normilized_entropy, running accuracy: 88.54489164086688
Method name: topk_entropy, running accuracy: 88.54489164086688
Method name: window_entropy, running accuracy: 89.47368421052632
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 322/500 [27:50:56<19:41:41, 398.32s/it, attention_weighted_confidence_acc=89.47%, cer_entropy_weighted_mean_all_acc=89.47%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=89.78%, p_true_acc=90.09%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.54%, topk_entropy_acc=88.54%, window_entropy_acc=89.47%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▍   | 323/500 [27:50:56<19:32:47, 397.56s/it, attention_weighted_confidence_acc=89.47%, cer_entropy_weighted_mean_all_acc=89.47%, cer_prob_product_log_last_acc=88.54%, self_consistency_acc=89.78%, p_true_acc=90.09%, normilized_likelihood_acc=88.85%, normilized_entropy_acc=88.54%, topk_entropy_acc=88.54%, window_entropy_acc=89.47%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Greta has left to spend, we'll follow these steps:

1...
    Score: 4.173170305977438
    Answer: 720
    Ground truth:  720
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Greta has left to spend, we'll follow these steps:

1...
    Score: 4.173170305977438
    Answer: 720
    Ground truth:  720
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Greta has left to spend, we'll follow these steps:

1...
    Score: 7.9999120235443115
    Answer: 720
    Ground truth:  720
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Greta has left to spend, we'll follow these steps:

1...
    Score: 0.5
    Answer: 720
    Ground truth:  720
Method 5: p_true
  Batch 1:
    Text: To find out how much money Greta has left to spend, we'll follow these steps:

1...
    Score: 6.51171875
    Answer: 720
    Ground truth:  720
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Greta has left to spend, we'll follow these steps:

1...
    Score: 0.726006492972374
    Answer: 720
    Ground truth:  720
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Greta has left to spend, we need to calculate the amo...
    Score: 0.6247670948505402
    Answer: 720.0
    Ground truth:  720
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Greta has left to spend, we need to calculate the amo...
    Score: 0.6164112091064453
    Answer: 720.0
    Ground truth:  720
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Greta has left to spend, we'll follow these steps:

1...
    Score: 1.2664377093315125
    Answer: 720
    Ground truth:  720
Method name: attention_weighted_confidence, running accuracy: 89.50617283950618
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.50617283950618
Method name: cer_prob_product_log_last, running accuracy: 88.58024691358025
Method name: self_consistency, running accuracy: 89.81481481481481
Method name: p_true, running accuracy: 90.12345679012346
Method name: normilized_likelihood, running accuracy: 88.88888888888889
Method name: normilized_entropy, running accuracy: 88.58024691358025
Method name: topk_entropy, running accuracy: 88.58024691358025
Method name: window_entropy, running accuracy: 89.50617283950618
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▍   | 323/500 [27:55:30<19:32:47, 397.56s/it, attention_weighted_confidence_acc=89.51%, cer_entropy_weighted_mean_all_acc=89.51%, cer_prob_product_log_last_acc=88.58%, self_consistency_acc=89.81%, p_true_acc=90.12%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.58%, topk_entropy_acc=88.58%, window_entropy_acc=89.51%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▍   | 324/500 [27:55:30<17:37:25, 360.48s/it, attention_weighted_confidence_acc=89.51%, cer_entropy_weighted_mean_all_acc=89.51%, cer_prob_product_log_last_acc=88.58%, self_consistency_acc=89.81%, p_true_acc=90.12%, normilized_likelihood_acc=88.89%, normilized_entropy_acc=88.58%, topk_entropy_acc=88.58%, window_entropy_acc=89.51%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 8.456471605224392
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 8.456471605224392
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 15.997018456459045
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 1.0
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 14.28515625
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 2.013170763850212
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 0.9966024309396744
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 0.9943434447050095
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find out how much more Juice Box C is than Juice Box B, we first need to dete...
    Score: 3.855051189661026
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 89.53846153846153
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.53846153846153
Method name: cer_prob_product_log_last, running accuracy: 88.61538461538461
Method name: self_consistency, running accuracy: 89.84615384615384
Method name: p_true, running accuracy: 90.15384615384615
Method name: normilized_likelihood, running accuracy: 88.92307692307693
Method name: normilized_entropy, running accuracy: 88.61538461538461
Method name: topk_entropy, running accuracy: 88.61538461538461
Method name: window_entropy, running accuracy: 89.53846153846153
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▍   | 324/500 [27:59:31<17:37:25, 360.48s/it, attention_weighted_confidence_acc=89.54%, cer_entropy_weighted_mean_all_acc=89.54%, cer_prob_product_log_last_acc=88.62%, self_consistency_acc=89.85%, p_true_acc=90.15%, normilized_likelihood_acc=88.92%, normilized_entropy_acc=88.62%, topk_entropy_acc=88.62%, window_entropy_acc=89.54%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 325/500 [27:59:31<15:46:18, 324.45s/it, attention_weighted_confidence_acc=89.54%, cer_entropy_weighted_mean_all_acc=89.54%, cer_prob_product_log_last_acc=88.62%, self_consistency_acc=89.85%, p_true_acc=90.15%, normilized_likelihood_acc=88.92%, normilized_entropy_acc=88.62%, topk_entropy_acc=88.62%, window_entropy_acc=89.54%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 3.411593726362479
    Answer: 240
    Ground truth:  240
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 3.411593726362479
    Answer: 240
    Ground truth:  240
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 6.999812602996826
    Answer: 240
    Ground truth:  240
Method 4: self_consistency
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 0.4375
    Answer: 240
    Ground truth:  240
Method 5: p_true
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 6.41015625
    Answer: 240
    Ground truth:  240
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 9.295573145151138
    Answer: 240
    Ground truth:  240
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 9.388904839754105
    Answer: 240
    Ground truth:  240
Method 8: topk_entropy
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 7.935277819633484
    Answer: 240
    Ground truth:  240
Method 9: window_entropy
  Batch 1:
    Text: To solve this, let's break it down step by step.

1. Thomas withdraws $1000 in 2...
    Score: 9.605348289012909
    Answer: 240
    Ground truth:  240
Method name: attention_weighted_confidence, running accuracy: 89.57055214723927
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.57055214723927
Method name: cer_prob_product_log_last, running accuracy: 88.65030674846625
Method name: self_consistency, running accuracy: 89.87730061349694
Method name: p_true, running accuracy: 90.1840490797546
Method name: normilized_likelihood, running accuracy: 88.95705521472392
Method name: normilized_entropy, running accuracy: 88.65030674846625
Method name: topk_entropy, running accuracy: 88.65030674846625
Method name: window_entropy, running accuracy: 89.57055214723927
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 325/500 [28:07:12<15:46:18, 324.45s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.65%, self_consistency_acc=89.88%, p_true_acc=90.18%, normilized_likelihood_acc=88.96%, normilized_entropy_acc=88.65%, topk_entropy_acc=88.65%, window_entropy_acc=89.57%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 326/500 [28:07:12<17:39:57, 365.50s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.65%, self_consistency_acc=89.88%, p_true_acc=90.18%, normilized_likelihood_acc=88.96%, normilized_entropy_acc=88.65%, topk_entropy_acc=88.65%, window_entropy_acc=89.57%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 3.8129673248777687
    Answer: 26
    Ground truth:  26
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 3.8129673248777687
    Answer: 26
    Ground truth:  26
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 7.926541745662689
    Answer: 26
    Ground truth:  26
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 0.5
    Answer: 26
    Ground truth:  26
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 6.1435546875
    Answer: 26
    Ground truth:  26
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 2.2796583771705627
    Answer: 26
    Ground truth:  26
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 2.2479501366615295
    Answer: 26
    Ground truth:  26
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 2.0630760490894318
    Answer: 26
    Ground truth:  26
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into parts:

1. John has 4 glasses of...
    Score: 7.620373249053955
    Answer: 26
    Ground truth:  26
Method name: attention_weighted_confidence, running accuracy: 89.60244648318043
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.60244648318043
Method name: cer_prob_product_log_last, running accuracy: 88.68501529051987
Method name: self_consistency, running accuracy: 89.90825688073394
Method name: p_true, running accuracy: 90.21406727828746
Method name: normilized_likelihood, running accuracy: 88.9908256880734
Method name: normilized_entropy, running accuracy: 88.68501529051987
Method name: topk_entropy, running accuracy: 88.68501529051987
Method name: window_entropy, running accuracy: 89.60244648318043
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 326/500 [28:12:22<17:39:57, 365.50s/it, attention_weighted_confidence_acc=89.60%, cer_entropy_weighted_mean_all_acc=89.60%, cer_prob_product_log_last_acc=88.69%, self_consistency_acc=89.91%, p_true_acc=90.21%, normilized_likelihood_acc=88.99%, normilized_entropy_acc=88.69%, topk_entropy_acc=88.69%, window_entropy_acc=89.60%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 327/500 [28:12:22<16:46:09, 348.95s/it, attention_weighted_confidence_acc=89.60%, cer_entropy_weighted_mean_all_acc=89.60%, cer_prob_product_log_last_acc=88.69%, self_consistency_acc=89.91%, p_true_acc=90.21%, normilized_likelihood_acc=88.99%, normilized_entropy_acc=88.69%, topk_entropy_acc=88.69%, window_entropy_acc=89.60%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 8.757407692037003
    Answer: 48
    Ground truth:  48
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 8.757407692037003
    Answer: 48
    Ground truth:  48
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 15.99988043308258
    Answer: 48
    Ground truth:  48
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 1.0
    Answer: 48
    Ground truth:  48
Method 5: p_true
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 11.09375
    Answer: 48
    Ground truth:  48
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 1.8691703900694847
    Answer: 48
    Ground truth:  48
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 0.8282057344913483
    Answer: 48
    Ground truth:  48
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 0.8237738758325577
    Answer: 48
    Ground truth:  48
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of legs in the store, we first need to know the number ...
    Score: 2.8014823496341705
    Answer: 48
    Ground truth:  48
Method name: attention_weighted_confidence, running accuracy: 89.63414634146342
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.63414634146342
Method name: cer_prob_product_log_last, running accuracy: 88.71951219512195
Method name: self_consistency, running accuracy: 89.9390243902439
Method name: p_true, running accuracy: 90.2439024390244
Method name: normilized_likelihood, running accuracy: 89.02439024390245
Method name: normilized_entropy, running accuracy: 88.71951219512195
Method name: topk_entropy, running accuracy: 88.71951219512195
Method name: window_entropy, running accuracy: 89.63414634146342
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 327/500 [28:16:24<16:46:09, 348.95s/it, attention_weighted_confidence_acc=89.63%, cer_entropy_weighted_mean_all_acc=89.63%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=89.94%, p_true_acc=90.24%, normilized_likelihood_acc=89.02%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.63%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 328/500 [28:16:24<15:07:56, 316.73s/it, attention_weighted_confidence_acc=89.63%, cer_entropy_weighted_mean_all_acc=89.63%, cer_prob_product_log_last_acc=88.72%, self_consistency_acc=89.94%, p_true_acc=90.24%, normilized_likelihood_acc=89.02%, normilized_entropy_acc=88.72%, topk_entropy_acc=88.72%, window_entropy_acc=89.63%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 5.945127924693857
    Answer: 54
    Ground truth:  54
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 5.945127924693857
    Answer: 54
    Ground truth:  54
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 12.993876993656158
    Answer: 54
    Ground truth:  54
Method 4: self_consistency
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 0.8125
    Answer: 54
    Ground truth:  54
Method 5: p_true
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 11.28125
    Answer: 54
    Ground truth:  54
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 8.199735671281815
    Answer: 54
    Ground truth:  54
Method 7: normilized_entropy
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 6.987693756818771
    Answer: 54
    Ground truth:  54
Method 8: topk_entropy
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 6.414347141981125
    Answer: 54
    Ground truth:  54
Method 9: window_entropy
  Batch 1:
    Text: To find out the total number of cupcakes Howie should buy, we need to determine ...
    Score: 15.930335521697998
    Answer: 54
    Ground truth:  54
Method name: attention_weighted_confidence, running accuracy: 89.66565349544074
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.66565349544074
Method name: cer_prob_product_log_last, running accuracy: 88.75379939209726
Method name: self_consistency, running accuracy: 89.96960486322189
Method name: p_true, running accuracy: 90.27355623100304
Method name: normilized_likelihood, running accuracy: 89.05775075987842
Method name: normilized_entropy, running accuracy: 88.75379939209726
Method name: topk_entropy, running accuracy: 88.75379939209726
Method name: window_entropy, running accuracy: 89.66565349544074
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 328/500 [28:20:08<15:07:56, 316.73s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.75%, self_consistency_acc=89.97%, p_true_acc=90.27%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.75%, topk_entropy_acc=88.75%, window_entropy_acc=89.67%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 329/500 [28:20:08<13:44:01, 289.13s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.75%, self_consistency_acc=89.97%, p_true_acc=90.27%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.75%, topk_entropy_acc=88.75%, window_entropy_acc=89.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 4.095346013032804
    Answer: 2350
    Ground truth:  2350
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 4.095346013032804
    Answer: 2350
    Ground truth:  2350
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 3.504730640128237
    Answer: 2350
    Ground truth:  2350
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 0.5
    Answer: 2350
    Ground truth:  2350
Method 5: p_true
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 5.97265625
    Answer: 2350
    Ground truth:  2350
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 6.1177642196416855
    Answer: 2350
    Ground truth:  2350
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 5.076276987791061
    Answer: 2350
    Ground truth:  2350
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 4.447497218847275
    Answer: 2350
    Ground truth:  2350
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of fruits an avocado tree can produce in its 10 years o...
    Score: 8.242649137973785
    Answer: 2350
    Ground truth:  2350
Method name: attention_weighted_confidence, running accuracy: 89.6969696969697
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.6969696969697
Method name: cer_prob_product_log_last, running accuracy: 88.7878787878788
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.30303030303031
Method name: normilized_likelihood, running accuracy: 89.0909090909091
Method name: normilized_entropy, running accuracy: 88.7878787878788
Method name: topk_entropy, running accuracy: 88.7878787878788
Method name: window_entropy, running accuracy: 89.6969696969697
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 329/500 [28:31:19<13:44:01, 289.13s/it, attention_weighted_confidence_acc=89.70%, cer_entropy_weighted_mean_all_acc=89.70%, cer_prob_product_log_last_acc=88.79%, self_consistency_acc=90.00%, p_true_acc=90.30%, normilized_likelihood_acc=89.09%, normilized_entropy_acc=88.79%, topk_entropy_acc=88.79%, window_entropy_acc=89.70%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 330/500 [28:31:19<19:02:59, 403.41s/it, attention_weighted_confidence_acc=89.70%, cer_entropy_weighted_mean_all_acc=89.70%, cer_prob_product_log_last_acc=88.79%, self_consistency_acc=90.00%, p_true_acc=90.30%, normilized_likelihood_acc=89.09%, normilized_entropy_acc=88.79%, topk_entropy_acc=88.79%, window_entropy_acc=89.70%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 8.361292128104756
    Answer: 32
    Ground truth:  32
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 8.361292128104756
    Answer: 32
    Ground truth:  32
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 15.972479343414307
    Answer: 32
    Ground truth:  32
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 1.0
    Answer: 32
    Ground truth:  32
Method 5: p_true
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 11.2734375
    Answer: 32
    Ground truth:  32
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 2.3778606057167053
    Answer: 32
    Ground truth:  32
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 2.1191695779561996
    Answer: 32
    Ground truth:  32
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 2.099461615085602
    Answer: 32
    Ground truth:  32
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of blue birds, we need to add up the number of blue bir...
    Score: 8.499509274959564
    Answer: 32
    Ground truth:  32
Method name: attention_weighted_confidence, running accuracy: 89.72809667673715
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.72809667673715
Method name: cer_prob_product_log_last, running accuracy: 88.82175226586104
Method name: self_consistency, running accuracy: 90.03021148036254
Method name: p_true, running accuracy: 90.33232628398792
Method name: normilized_likelihood, running accuracy: 89.12386706948641
Method name: normilized_entropy, running accuracy: 88.82175226586104
Method name: topk_entropy, running accuracy: 88.82175226586104
Method name: window_entropy, running accuracy: 89.72809667673715
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 330/500 [28:35:03<19:02:59, 403.41s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=88.82%, self_consistency_acc=90.03%, p_true_acc=90.33%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.82%, topk_entropy_acc=88.82%, window_entropy_acc=89.73%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 331/500 [28:35:03<16:25:18, 349.81s/it, attention_weighted_confidence_acc=89.73%, cer_entropy_weighted_mean_all_acc=89.73%, cer_prob_product_log_last_acc=88.82%, self_consistency_acc=90.03%, p_true_acc=90.33%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.82%, topk_entropy_acc=88.82%, window_entropy_acc=89.73%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 7.985633107817565
    Answer: 60
    Ground truth:  60
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 7.985633107817565
    Answer: 60
    Ground truth:  60
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 14.892833054065704
    Answer: 60
    Ground truth:  60
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 0.9375
    Answer: 60
    Ground truth:  60
Method 5: p_true
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 12.9765625
    Answer: 60
    Ground truth:  60
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 10.038261964917183
    Answer: 60
    Ground truth:  60
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 9.58820553123951
    Answer: 60
    Ground truth:  60
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 8.650030076503754
    Answer: 60
    Ground truth:  60
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Frances has left, we need to calculate her total earn...
    Score: 21.735685646533966
    Answer: 60
    Ground truth:  60
Method name: attention_weighted_confidence, running accuracy: 89.7590361445783
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.7590361445783
Method name: cer_prob_product_log_last, running accuracy: 88.85542168674698
Method name: self_consistency, running accuracy: 90.06024096385542
Method name: p_true, running accuracy: 90.36144578313254
Method name: normilized_likelihood, running accuracy: 89.1566265060241
Method name: normilized_entropy, running accuracy: 88.85542168674698
Method name: topk_entropy, running accuracy: 88.85542168674698
Method name: window_entropy, running accuracy: 89.7590361445783
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 331/500 [28:40:13<16:25:18, 349.81s/it, attention_weighted_confidence_acc=89.76%, cer_entropy_weighted_mean_all_acc=89.76%, cer_prob_product_log_last_acc=88.86%, self_consistency_acc=90.06%, p_true_acc=90.36%, normilized_likelihood_acc=89.16%, normilized_entropy_acc=88.86%, topk_entropy_acc=88.86%, window_entropy_acc=89.76%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▋   | 332/500 [28:40:13<15:45:54, 337.82s/it, attention_weighted_confidence_acc=89.76%, cer_entropy_weighted_mean_all_acc=89.76%, cer_prob_product_log_last_acc=88.86%, self_consistency_acc=90.06%, p_true_acc=90.36%, normilized_likelihood_acc=89.16%, normilized_entropy_acc=88.86%, topk_entropy_acc=88.86%, window_entropy_acc=89.76%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 7.678361732671128
    Answer: 3000
    Ground truth:  3000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 7.678361732671128
    Answer: 3000
    Ground truth:  3000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 6.7200341297569715
    Answer: 3000
    Ground truth:  3000
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 1.0
    Answer: 3000
    Ground truth:  3000
Method 5: p_true
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 14.53515625
    Answer: 3000
    Ground truth:  3000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 2.7049289494752884
    Answer: 3000
    Ground truth:  3000
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 1.4425355941057205
    Answer: 3000
    Ground truth:  3000
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 1.4244653433561325
    Answer: 3000
    Ground truth:  3000
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of the physical therapy, we need to first find the total ...
    Score: 1.6882573366165161
    Answer: 3000
    Ground truth:  3000
Method name: attention_weighted_confidence, running accuracy: 89.7897897897898
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.7897897897898
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 90.09009009009009
Method name: p_true, running accuracy: 90.39039039039038
Method name: normilized_likelihood, running accuracy: 89.1891891891892
Method name: normilized_entropy, running accuracy: 88.88888888888889
Method name: topk_entropy, running accuracy: 88.88888888888889
Method name: window_entropy, running accuracy: 89.7897897897898
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▋   | 332/500 [28:43:52<15:45:54, 337.82s/it, attention_weighted_confidence_acc=89.79%, cer_entropy_weighted_mean_all_acc=89.79%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.09%, p_true_acc=90.39%, normilized_likelihood_acc=89.19%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=89.79%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 333/500 [28:43:52<14:00:58, 302.15s/it, attention_weighted_confidence_acc=89.79%, cer_entropy_weighted_mean_all_acc=89.79%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.09%, p_true_acc=90.39%, normilized_likelihood_acc=89.19%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=89.79%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 4.894034723882578
    Answer: 153
    Ground truth:  153
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 4.894034723882578
    Answer: 153
    Ground truth:  153
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 8.81281852722168
    Answer: 153
    Ground truth:  153
Method 4: self_consistency
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 0.5625
    Answer: 153
    Ground truth:  153
Method 5: p_true
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 8.8828125
    Answer: 153
    Ground truth:  153
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 1.5712177753448486
    Answer: 153
    Ground truth:  153
Method 7: normilized_entropy
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 1.1575907915830612
    Answer: 153
    Ground truth:  153
Method 8: topk_entropy
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 1.099725067615509
    Answer: 153
    Ground truth:  153
Method 9: window_entropy
  Batch 1:
    Text: Let's denote the number of cards PJ has as x.

Since Smendrick has 3 times the a...
    Score: 10.76666334271431
    Answer: 153
    Ground truth:  153
Method name: attention_weighted_confidence, running accuracy: 89.82035928143712
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.82035928143712
Method name: cer_prob_product_log_last, running accuracy: 88.92215568862275
Method name: self_consistency, running accuracy: 90.11976047904191
Method name: p_true, running accuracy: 90.41916167664671
Method name: normilized_likelihood, running accuracy: 89.22155688622755
Method name: normilized_entropy, running accuracy: 88.92215568862275
Method name: topk_entropy, running accuracy: 88.92215568862275
Method name: window_entropy, running accuracy: 89.82035928143712
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 333/500 [28:49:24<14:00:58, 302.15s/it, attention_weighted_confidence_acc=89.82%, cer_entropy_weighted_mean_all_acc=89.82%, cer_prob_product_log_last_acc=88.92%, self_consistency_acc=90.12%, p_true_acc=90.42%, normilized_likelihood_acc=89.22%, normilized_entropy_acc=88.92%, topk_entropy_acc=88.92%, window_entropy_acc=89.82%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 334/500 [28:49:24<14:20:22, 310.98s/it, attention_weighted_confidence_acc=89.82%, cer_entropy_weighted_mean_all_acc=89.82%, cer_prob_product_log_last_acc=88.92%, self_consistency_acc=90.12%, p_true_acc=90.42%, normilized_likelihood_acc=89.22%, normilized_entropy_acc=88.92%, topk_entropy_acc=88.92%, window_entropy_acc=89.82%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 8.069238028019827
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 8.069238028019827
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 15.996949434280396
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 1.0
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 11.986328125
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 3.8887083679437637
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 3.2627160400152206
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 3.116990342736244
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to follow the steps to find out how many blue gol...
    Score: 16.42506915330887
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.8507462686567
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.8507462686567
Method name: cer_prob_product_log_last, running accuracy: 88.95522388059702
Method name: self_consistency, running accuracy: 90.14925373134328
Method name: p_true, running accuracy: 90.44776119402985
Method name: normilized_likelihood, running accuracy: 89.25373134328358
Method name: normilized_entropy, running accuracy: 88.95522388059702
Method name: topk_entropy, running accuracy: 88.95522388059702
Method name: window_entropy, running accuracy: 89.8507462686567
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 334/500 [28:52:17<14:20:22, 310.98s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=88.96%, self_consistency_acc=90.15%, p_true_acc=90.45%, normilized_likelihood_acc=89.25%, normilized_entropy_acc=88.96%, topk_entropy_acc=88.96%, window_entropy_acc=89.85%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 335/500 [28:52:17<12:21:18, 269.57s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=88.96%, self_consistency_acc=90.15%, p_true_acc=90.45%, normilized_likelihood_acc=89.25%, normilized_entropy_acc=88.96%, topk_entropy_acc=88.96%, window_entropy_acc=89.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 8.328573054902948
    Answer: 300
    Ground truth:  300
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 8.328573054902948
    Answer: 300
    Ground truth:  300
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 15.999983429908752
    Answer: 300
    Ground truth:  300
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 1.0
    Answer: 300
    Ground truth:  300
Method 5: p_true
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 14.41796875
    Answer: 300
    Ground truth:  300
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 4.371483683586121
    Answer: 300
    Ground truth:  300
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 2.3443280309438705
    Answer: 300
    Ground truth:  300
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 2.2035016268491745
    Answer: 300
    Ground truth:  300
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Suzanne has left, we need to calculate her total earn...
    Score: 13.800285995006561
    Answer: 300
    Ground truth:  300
Method name: attention_weighted_confidence, running accuracy: 89.88095238095238
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.88095238095238
Method name: cer_prob_product_log_last, running accuracy: 88.98809523809523
Method name: self_consistency, running accuracy: 90.17857142857143
Method name: p_true, running accuracy: 90.47619047619048
Method name: normilized_likelihood, running accuracy: 89.28571428571429
Method name: normilized_entropy, running accuracy: 88.98809523809523
Method name: topk_entropy, running accuracy: 88.98809523809523
Method name: window_entropy, running accuracy: 89.88095238095238
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 335/500 [28:57:21<12:21:18, 269.57s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=88.99%, self_consistency_acc=90.18%, p_true_acc=90.48%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=88.99%, topk_entropy_acc=88.99%, window_entropy_acc=89.88%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 336/500 [28:57:21<12:45:24, 280.03s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=88.99%, self_consistency_acc=90.18%, p_true_acc=90.48%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=88.99%, topk_entropy_acc=88.99%, window_entropy_acc=89.88%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 8.462328200176268
    Answer: 9
    Ground truth:  9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 8.462328200176268
    Answer: 9
    Ground truth:  9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 15.999891877174377
    Answer: 9
    Ground truth:  9
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 1.0
    Answer: 9
    Ground truth:  9
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 15.28125
    Answer: 9
    Ground truth:  9
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 3.42709182202816
    Answer: 9
    Ground truth:  9
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 1.9448031783103943
    Answer: 9
    Ground truth:  9
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 1.9343960583209991
    Answer: 9
    Ground truth:  9
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down step by step.

First, we're told that...
    Score: 4.360631287097931
    Answer: 9
    Ground truth:  9
Method name: attention_weighted_confidence, running accuracy: 89.91097922848664
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.91097922848664
Method name: cer_prob_product_log_last, running accuracy: 89.02077151335311
Method name: self_consistency, running accuracy: 90.20771513353115
Method name: p_true, running accuracy: 90.50445103857567
Method name: normilized_likelihood, running accuracy: 89.31750741839762
Method name: normilized_entropy, running accuracy: 89.02077151335311
Method name: topk_entropy, running accuracy: 89.02077151335311
Method name: window_entropy, running accuracy: 89.91097922848664
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 336/500 [29:01:08<12:45:24, 280.03s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=89.02%, self_consistency_acc=90.21%, p_true_acc=90.50%, normilized_likelihood_acc=89.32%, normilized_entropy_acc=89.02%, topk_entropy_acc=89.02%, window_entropy_acc=89.91%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 337/500 [29:01:08<11:57:39, 264.17s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=89.02%, self_consistency_acc=90.21%, p_true_acc=90.50%, normilized_likelihood_acc=89.32%, normilized_entropy_acc=89.02%, topk_entropy_acc=89.02%, window_entropy_acc=89.91%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 7.849098009029581
    Answer: 360
    Ground truth:  360
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 7.849098009029581
    Answer: 360
    Ground truth:  360
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 15.79627388715744
    Answer: 360
    Ground truth:  360
Method 4: self_consistency
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 1.0
    Answer: 360
    Ground truth:  360
Method 5: p_true
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 15.35546875
    Answer: 360
    Ground truth:  360
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 2.2027655839920044
    Answer: 360
    Ground truth:  360
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 1.3172594457864761
    Answer: 360
    Ground truth:  360
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 1.3134448379278183
    Answer: 360
    Ground truth:  360
Method 9: window_entropy
  Batch 1:
    Text: To find out how many peaches John collects in 3 hours, we need to break it down ...
    Score: 1.8802401721477509
    Answer: 360
    Ground truth:  360
Method name: attention_weighted_confidence, running accuracy: 89.94082840236686
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.94082840236686
Method name: cer_prob_product_log_last, running accuracy: 89.05325443786982
Method name: self_consistency, running accuracy: 90.23668639053254
Method name: p_true, running accuracy: 90.53254437869822
Method name: normilized_likelihood, running accuracy: 89.3491124260355
Method name: normilized_entropy, running accuracy: 89.05325443786982
Method name: topk_entropy, running accuracy: 89.05325443786982
Method name: window_entropy, running accuracy: 89.94082840236686
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 337/500 [29:04:46<11:57:39, 264.17s/it, attention_weighted_confidence_acc=89.94%, cer_entropy_weighted_mean_all_acc=89.94%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.24%, p_true_acc=90.53%, normilized_likelihood_acc=89.35%, normilized_entropy_acc=89.05%, topk_entropy_acc=89.05%, window_entropy_acc=89.94%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 338/500 [29:04:46<11:15:32, 250.20s/it, attention_weighted_confidence_acc=89.94%, cer_entropy_weighted_mean_all_acc=89.94%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.24%, p_true_acc=90.53%, normilized_likelihood_acc=89.35%, normilized_entropy_acc=89.05%, topk_entropy_acc=89.05%, window_entropy_acc=89.94%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 8.549990934148774
    Answer: 52
    Ground truth:  52
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 8.549990934148774
    Answer: 52
    Ground truth:  52
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 15.999609470367432
    Answer: 52
    Ground truth:  52
Method 4: self_consistency
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 1.0
    Answer: 52
    Ground truth:  52
Method 5: p_true
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 12.53125
    Answer: 52
    Ground truth:  52
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 1.126576729118824
    Answer: 52
    Ground truth:  52
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 0.6625774130225182
    Answer: 52
    Ground truth:  52
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 0.6617722809314728
    Answer: 52
    Ground truth:  52
Method 9: window_entropy
  Batch 1:
    Text: To find out how many more tins of cat food than dog food Kimberly bought, we nee...
    Score: 3.309601068496704
    Answer: 52
    Ground truth:  52
Method name: attention_weighted_confidence, running accuracy: 89.97050147492625
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.97050147492625
Method name: cer_prob_product_log_last, running accuracy: 89.08554572271386
Method name: self_consistency, running accuracy: 90.2654867256637
Method name: p_true, running accuracy: 90.56047197640117
Method name: normilized_likelihood, running accuracy: 89.38053097345133
Method name: normilized_entropy, running accuracy: 89.08554572271386
Method name: topk_entropy, running accuracy: 89.08554572271386
Method name: window_entropy, running accuracy: 89.97050147492625
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 338/500 [29:08:53<11:15:32, 250.20s/it, attention_weighted_confidence_acc=89.97%, cer_entropy_weighted_mean_all_acc=89.97%, cer_prob_product_log_last_acc=89.09%, self_consistency_acc=90.27%, p_true_acc=90.56%, normilized_likelihood_acc=89.38%, normilized_entropy_acc=89.09%, topk_entropy_acc=89.09%, window_entropy_acc=89.97%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 339/500 [29:08:53<11:08:56, 249.29s/it, attention_weighted_confidence_acc=89.97%, cer_entropy_weighted_mean_all_acc=89.97%, cer_prob_product_log_last_acc=89.09%, self_consistency_acc=90.27%, p_true_acc=90.56%, normilized_likelihood_acc=89.38%, normilized_entropy_acc=89.09%, topk_entropy_acc=89.09%, window_entropy_acc=89.97%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 1.4014478026912687
    Answer: 49
    Ground truth:  150
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 1.4014478026912687
    Answer: 49
    Ground truth:  150
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 2.793676435947418
    Answer: 49
    Ground truth:  150
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 0.1875
    Answer: 49
    Ground truth:  150
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 2.267578125
    Answer: 49
    Ground truth:  150
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 1.8953605890274048
    Answer: 49
    Ground truth:  150
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 1.6406706869602203
    Answer: 49
    Ground truth:  150
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Since Kenny is saving 1/3 of his cards, ...
    Score: 1.478078544139862
    Answer: 49
    Ground truth:  150
Method 9: window_entropy
  Batch 1:
    Text: To find out how many cards Kenny started with, we can break it down into steps. ...
    Score: 2.502891421318054
    Answer: 50
    Ground truth:  150
Method name: attention_weighted_confidence, running accuracy: 89.70588235294117
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.70588235294117
Method name: cer_prob_product_log_last, running accuracy: 88.8235294117647
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.29411764705883
Method name: normilized_likelihood, running accuracy: 89.11764705882354
Method name: normilized_entropy, running accuracy: 88.8235294117647
Method name: topk_entropy, running accuracy: 88.8235294117647
Method name: window_entropy, running accuracy: 89.70588235294117
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 339/500 [29:17:15<11:08:56, 249.29s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.82%, self_consistency_acc=90.00%, p_true_acc=90.29%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.82%, topk_entropy_acc=88.82%, window_entropy_acc=89.71%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 340/500 [29:17:15<14:27:21, 325.26s/it, attention_weighted_confidence_acc=89.71%, cer_entropy_weighted_mean_all_acc=89.71%, cer_prob_product_log_last_acc=88.82%, self_consistency_acc=90.00%, p_true_acc=90.29%, normilized_likelihood_acc=89.12%, normilized_entropy_acc=88.82%, topk_entropy_acc=88.82%, window_entropy_acc=89.71%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 8.721395475113026
    Answer: 72
    Ground truth:  72
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 8.721395475113026
    Answer: 72
    Ground truth:  72
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 14.567207992076874
    Answer: 72
    Ground truth:  72
Method 4: self_consistency
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 1.0
    Answer: 72
    Ground truth:  72
Method 5: p_true
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 8.4609375
    Answer: 72
    Ground truth:  72
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 2.4181395024061203
    Answer: 72
    Ground truth:  72
Method 7: normilized_entropy
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 2.214009039103985
    Answer: 72
    Ground truth:  72
Method 8: topk_entropy
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 2.175083950161934
    Answer: 72
    Ground truth:  72
Method 9: window_entropy
  Batch 1:
    Text: To find the height of the fourth child in inches, let's convert the height of th...
    Score: 7.03680545091629
    Answer: 72
    Ground truth:  72
Method name: attention_weighted_confidence, running accuracy: 89.73607038123167
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.73607038123167
Method name: cer_prob_product_log_last, running accuracy: 88.85630498533725
Method name: self_consistency, running accuracy: 90.02932551319648
Method name: p_true, running accuracy: 90.32258064516128
Method name: normilized_likelihood, running accuracy: 89.14956011730204
Method name: normilized_entropy, running accuracy: 88.85630498533725
Method name: topk_entropy, running accuracy: 88.85630498533725
Method name: window_entropy, running accuracy: 89.73607038123167
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 340/500 [29:21:32<14:27:21, 325.26s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=88.86%, self_consistency_acc=90.03%, p_true_acc=90.32%, normilized_likelihood_acc=89.15%, normilized_entropy_acc=88.86%, topk_entropy_acc=88.86%, window_entropy_acc=89.74%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 341/500 [29:21:32<13:27:01, 304.54s/it, attention_weighted_confidence_acc=89.74%, cer_entropy_weighted_mean_all_acc=89.74%, cer_prob_product_log_last_acc=88.86%, self_consistency_acc=90.03%, p_true_acc=90.32%, normilized_likelihood_acc=89.15%, normilized_entropy_acc=88.86%, topk_entropy_acc=88.86%, window_entropy_acc=89.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 7.353524116173119
    Answer: 7
    Ground truth:  7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 7.353524116173119
    Answer: 7
    Ground truth:  7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 14.9989715218544
    Answer: 7
    Ground truth:  7
Method 4: self_consistency
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 0.9375
    Answer: 7
    Ground truth:  7
Method 5: p_true
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 14.12109375
    Answer: 7
    Ground truth:  7
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 3.7508550584316254
    Answer: 7
    Ground truth:  7
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 3.0748445093631744
    Answer: 7
    Ground truth:  7
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 2.79000386595726
    Answer: 7
    Ground truth:  7
Method 9: window_entropy
  Batch 1:
    Text: To find out how many marbles Brendan ended up with, we'll follow the events step...
    Score: 14.99020105600357
    Answer: 7
    Ground truth:  7
Method name: attention_weighted_confidence, running accuracy: 89.76608187134502
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.76608187134502
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 90.05847953216374
Method name: p_true, running accuracy: 90.35087719298247
Method name: normilized_likelihood, running accuracy: 89.18128654970761
Method name: normilized_entropy, running accuracy: 88.88888888888889
Method name: topk_entropy, running accuracy: 88.88888888888889
Method name: window_entropy, running accuracy: 89.76608187134502
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 341/500 [29:25:30<13:27:01, 304.54s/it, attention_weighted_confidence_acc=89.77%, cer_entropy_weighted_mean_all_acc=89.77%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.06%, p_true_acc=90.35%, normilized_likelihood_acc=89.18%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=89.77%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 342/500 [29:25:30<12:29:50, 284.75s/it, attention_weighted_confidence_acc=89.77%, cer_entropy_weighted_mean_all_acc=89.77%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=90.06%, p_true_acc=90.35%, normilized_likelihood_acc=89.18%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=89.77%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 4.6701664019424
    Answer: 27
    Ground truth:  27
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 4.6701664019424
    Answer: 27
    Ground truth:  27
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 9.986237287521362
    Answer: 27
    Ground truth:  27
Method 4: self_consistency
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 0.625
    Answer: 27
    Ground truth:  27
Method 5: p_true
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 7.58203125
    Answer: 27
    Ground truth:  27
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 7.971204608678818
    Answer: 27
    Ground truth:  27
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 7.242259979248047
    Answer: 27
    Ground truth:  27
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 6.064775437116623
    Answer: 27
    Ground truth:  27
Method 9: window_entropy
  Batch 1:
    Text: To find out how many marbles Mazie gave to Darla, we need to follow these steps:...
    Score: 12.178543210029602
    Answer: 27
    Ground truth:  27
Method name: attention_weighted_confidence, running accuracy: 89.79591836734694
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.79591836734694
Method name: cer_prob_product_log_last, running accuracy: 88.92128279883383
Method name: self_consistency, running accuracy: 90.0874635568513
Method name: p_true, running accuracy: 90.37900874635568
Method name: normilized_likelihood, running accuracy: 89.21282798833819
Method name: normilized_entropy, running accuracy: 88.92128279883383
Method name: topk_entropy, running accuracy: 88.92128279883383
Method name: window_entropy, running accuracy: 89.79591836734694
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 342/500 [29:30:57<12:29:50, 284.75s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=88.92%, self_consistency_acc=90.09%, p_true_acc=90.38%, normilized_likelihood_acc=89.21%, normilized_entropy_acc=88.92%, topk_entropy_acc=88.92%, window_entropy_acc=89.80%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▊   | 343/500 [29:30:57<12:58:11, 297.40s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=88.92%, self_consistency_acc=90.09%, p_true_acc=90.38%, normilized_likelihood_acc=89.21%, normilized_entropy_acc=88.92%, topk_entropy_acc=88.92%, window_entropy_acc=89.80%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 6.061432792888515
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 6.061432792888515
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 12.860613763332367
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 0.8125
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 9.8671875
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 6.302710771560669
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 6.019993960857391
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 5.305306047201157
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: Before the second stop, Henry had traveled 60 - 15 = 45 miles. This is after his...
    Score: 13.162386298179626
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 89.82558139534885
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.82558139534885
Method name: cer_prob_product_log_last, running accuracy: 88.95348837209302
Method name: self_consistency, running accuracy: 90.11627906976744
Method name: p_true, running accuracy: 90.40697674418605
Method name: normilized_likelihood, running accuracy: 89.24418604651163
Method name: normilized_entropy, running accuracy: 88.95348837209302
Method name: topk_entropy, running accuracy: 88.95348837209302
Method name: window_entropy, running accuracy: 89.82558139534885
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▊   | 343/500 [29:35:37<12:58:11, 297.40s/it, attention_weighted_confidence_acc=89.83%, cer_entropy_weighted_mean_all_acc=89.83%, cer_prob_product_log_last_acc=88.95%, self_consistency_acc=90.12%, p_true_acc=90.41%, normilized_likelihood_acc=89.24%, normilized_entropy_acc=88.95%, topk_entropy_acc=88.95%, window_entropy_acc=89.83%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 344/500 [29:35:37<12:39:36, 292.16s/it, attention_weighted_confidence_acc=89.83%, cer_entropy_weighted_mean_all_acc=89.83%, cer_prob_product_log_last_acc=88.95%, self_consistency_acc=90.12%, p_true_acc=90.41%, normilized_likelihood_acc=89.24%, normilized_entropy_acc=88.95%, topk_entropy_acc=88.95%, window_entropy_acc=89.83%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 8.090035819813435
    Answer: 18
    Ground truth:  18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 8.090035819813435
    Answer: 18
    Ground truth:  18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 15.99992036819458
    Answer: 18
    Ground truth:  18
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 1.0
    Answer: 18
    Ground truth:  18
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 11.43359375
    Answer: 18
    Ground truth:  18
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 2.1188872903585434
    Answer: 18
    Ground truth:  18
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 1.8846082240343094
    Answer: 18
    Ground truth:  18
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 1.880711704492569
    Answer: 18
    Ground truth:  18
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. There are 6 girls in the park.
2....
    Score: 5.379717230796814
    Answer: 18
    Ground truth:  18
Method name: attention_weighted_confidence, running accuracy: 89.85507246376811
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.85507246376811
Method name: cer_prob_product_log_last, running accuracy: 88.98550724637681
Method name: self_consistency, running accuracy: 90.14492753623189
Method name: p_true, running accuracy: 90.43478260869566
Method name: normilized_likelihood, running accuracy: 89.27536231884058
Method name: normilized_entropy, running accuracy: 88.98550724637681
Method name: topk_entropy, running accuracy: 88.98550724637681
Method name: window_entropy, running accuracy: 89.85507246376811
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 344/500 [29:38:39<12:39:36, 292.16s/it, attention_weighted_confidence_acc=89.86%, cer_entropy_weighted_mean_all_acc=89.86%, cer_prob_product_log_last_acc=88.99%, self_consistency_acc=90.14%, p_true_acc=90.43%, normilized_likelihood_acc=89.28%, normilized_entropy_acc=88.99%, topk_entropy_acc=88.99%, window_entropy_acc=89.86%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 345/500 [29:38:39<11:09:26, 259.14s/it, attention_weighted_confidence_acc=89.86%, cer_entropy_weighted_mean_all_acc=89.86%, cer_prob_product_log_last_acc=88.99%, self_consistency_acc=90.14%, p_true_acc=90.43%, normilized_likelihood_acc=89.28%, normilized_entropy_acc=88.99%, topk_entropy_acc=88.99%, window_entropy_acc=89.86%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 7.77831613075528
    Answer: 250
    Ground truth:  250
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 7.77831613075528
    Answer: 250
    Ground truth:  250
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 15.999926447868347
    Answer: 250
    Ground truth:  250
Method 4: self_consistency
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 1.0
    Answer: 250
    Ground truth:  250
Method 5: p_true
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 15.31640625
    Answer: 250
    Ground truth:  250
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 2.2365882396698
    Answer: 250
    Ground truth:  250
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 1.2046052813529968
    Answer: 250
    Ground truth:  250
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 1.1802089214324951
    Answer: 250
    Ground truth:  250
Method 9: window_entropy
  Batch 1:
    Text: To find out how many minutes Jason has left, we need to calculate the total minu...
    Score: 2.8267681002616882
    Answer: 250
    Ground truth:  250
Method name: attention_weighted_confidence, running accuracy: 89.88439306358381
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.88439306358381
Method name: cer_prob_product_log_last, running accuracy: 89.01734104046243
Method name: self_consistency, running accuracy: 90.17341040462428
Method name: p_true, running accuracy: 90.46242774566474
Method name: normilized_likelihood, running accuracy: 89.30635838150289
Method name: normilized_entropy, running accuracy: 89.01734104046243
Method name: topk_entropy, running accuracy: 89.01734104046243
Method name: window_entropy, running accuracy: 89.88439306358381
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 345/500 [29:42:42<11:09:26, 259.14s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=89.02%, self_consistency_acc=90.17%, p_true_acc=90.46%, normilized_likelihood_acc=89.31%, normilized_entropy_acc=89.02%, topk_entropy_acc=89.02%, window_entropy_acc=89.88%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 346/500 [29:42:42<10:52:52, 254.37s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=89.02%, self_consistency_acc=90.17%, p_true_acc=90.46%, normilized_likelihood_acc=89.31%, normilized_entropy_acc=89.02%, topk_entropy_acc=89.02%, window_entropy_acc=89.88%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 7.9022599380667415
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 7.9022599380667415
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 15.999809741973877
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 14.7265625
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 2.0458063036203384
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 1.7542505413293839
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 1.7438026815652847
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Mara started with 2 slices of cake on th...
    Score: 5.156275510787964
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 89.9135446685879
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.9135446685879
Method name: cer_prob_product_log_last, running accuracy: 89.04899135446685
Method name: self_consistency, running accuracy: 90.20172910662824
Method name: p_true, running accuracy: 90.48991354466858
Method name: normilized_likelihood, running accuracy: 89.33717579250721
Method name: normilized_entropy, running accuracy: 89.04899135446685
Method name: topk_entropy, running accuracy: 89.04899135446685
Method name: window_entropy, running accuracy: 89.9135446685879
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 346/500 [29:45:59<10:52:52, 254.37s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.20%, p_true_acc=90.49%, normilized_likelihood_acc=89.34%, normilized_entropy_acc=89.05%, topk_entropy_acc=89.05%, window_entropy_acc=89.91%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 347/500 [29:45:59<10:04:18, 236.99s/it, attention_weighted_confidence_acc=89.91%, cer_entropy_weighted_mean_all_acc=89.91%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.20%, p_true_acc=90.49%, normilized_likelihood_acc=89.34%, normilized_entropy_acc=89.05%, topk_entropy_acc=89.05%, window_entropy_acc=89.91%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 7.726020440034471
    Answer: 78
    Ground truth:  78
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 7.726020440034471
    Answer: 78
    Ground truth:  78
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 15.999082624912262
    Answer: 78
    Ground truth:  78
Method 4: self_consistency
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 1.0
    Answer: 78
    Ground truth:  78
Method 5: p_true
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 14.453125
    Answer: 78
    Ground truth:  78
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 2.1836243867874146
    Answer: 78
    Ground truth:  78
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 1.2041340470314026
    Answer: 78
    Ground truth:  78
Method 8: topk_entropy
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 1.1961495578289032
    Answer: 78
    Ground truth:  78
Method 9: window_entropy
  Batch 1:
    Text: To find the total time Peter exercised on Monday and Sunday, we need to add the ...
    Score: 2.4062830805778503
    Answer: 78
    Ground truth:  78
Method name: attention_weighted_confidence, running accuracy: 89.9425287356322
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.9425287356322
Method name: cer_prob_product_log_last, running accuracy: 89.08045977011494
Method name: self_consistency, running accuracy: 90.22988505747126
Method name: p_true, running accuracy: 90.51724137931035
Method name: normilized_likelihood, running accuracy: 89.36781609195403
Method name: normilized_entropy, running accuracy: 89.08045977011494
Method name: topk_entropy, running accuracy: 89.08045977011494
Method name: window_entropy, running accuracy: 89.9425287356322
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 347/500 [29:48:49<10:04:18, 236.99s/it, attention_weighted_confidence_acc=89.94%, cer_entropy_weighted_mean_all_acc=89.94%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=90.23%, p_true_acc=90.52%, normilized_likelihood_acc=89.37%, normilized_entropy_acc=89.08%, topk_entropy_acc=89.08%, window_entropy_acc=89.94%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|██████▉   | 348/500 [29:48:49<9:09:56, 217.08s/it, attention_weighted_confidence_acc=89.94%, cer_entropy_weighted_mean_all_acc=89.94%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=90.23%, p_true_acc=90.52%, normilized_likelihood_acc=89.37%, normilized_entropy_acc=89.08%, topk_entropy_acc=89.08%, window_entropy_acc=89.94%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 7.799148703315322
    Answer: 70
    Ground truth:  70
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 7.799148703315322
    Answer: 70
    Ground truth:  70
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 15.999853014945984
    Answer: 70
    Ground truth:  70
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 1.0
    Answer: 70
    Ground truth:  70
Method 5: p_true
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 14.421875
    Answer: 70
    Ground truth:  70
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 3.819074049592018
    Answer: 70
    Ground truth:  70
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 2.957164093852043
    Answer: 70
    Ground truth:  70
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 2.9377165734767914
    Answer: 70
    Ground truth:  70
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of the dog grooming with the discount, we first need to c...
    Score: 4.9112111032009125
    Answer: 70
    Ground truth:  70
Method name: attention_weighted_confidence, running accuracy: 89.97134670487105
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.97134670487105
Method name: cer_prob_product_log_last, running accuracy: 89.11174785100286
Method name: self_consistency, running accuracy: 90.25787965616045
Method name: p_true, running accuracy: 90.54441260744986
Method name: normilized_likelihood, running accuracy: 89.39828080229226
Method name: normilized_entropy, running accuracy: 89.11174785100286
Method name: topk_entropy, running accuracy: 89.11174785100286
Method name: window_entropy, running accuracy: 89.97134670487105
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|██████▉   | 348/500 [29:51:43<9:09:56, 217.08s/it, attention_weighted_confidence_acc=89.97%, cer_entropy_weighted_mean_all_acc=89.97%, cer_prob_product_log_last_acc=89.11%, self_consistency_acc=90.26%, p_true_acc=90.54%, normilized_likelihood_acc=89.40%, normilized_entropy_acc=89.11%, topk_entropy_acc=89.11%, window_entropy_acc=89.97%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|██████▉   | 349/500 [29:51:43<8:33:33, 204.06s/it, attention_weighted_confidence_acc=89.97%, cer_entropy_weighted_mean_all_acc=89.97%, cer_prob_product_log_last_acc=89.11%, self_consistency_acc=90.26%, p_true_acc=90.54%, normilized_likelihood_acc=89.40%, normilized_entropy_acc=89.11%, topk_entropy_acc=89.11%, window_entropy_acc=89.97%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 8.10406883738456
    Answer: 13
    Ground truth:  13
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 8.10406883738456
    Answer: 13
    Ground truth:  13
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 15.999776363372803
    Answer: 13
    Ground truth:  13
Method 4: self_consistency
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 1.0
    Answer: 13
    Ground truth:  13
Method 5: p_true
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 12.96484375
    Answer: 13
    Ground truth:  13
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 2.1664539724588394
    Answer: 13
    Ground truth:  13
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 1.188010573387146
    Answer: 13
    Ground truth:  13
Method 8: topk_entropy
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 1.1820857971906662
    Answer: 13
    Ground truth:  13
Method 9: window_entropy
  Batch 1:
    Text: To find the total time of the cassette, we need to calculate the length of the s...
    Score: 4.093805432319641
    Answer: 13
    Ground truth:  13
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 89.14285714285714
Method name: self_consistency, running accuracy: 90.28571428571428
Method name: p_true, running accuracy: 90.57142857142857
Method name: normilized_likelihood, running accuracy: 89.42857142857143
Method name: normilized_entropy, running accuracy: 89.14285714285714
Method name: topk_entropy, running accuracy: 89.14285714285714
Method name: window_entropy, running accuracy: 90.0
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|██████▉   | 349/500 [29:55:33<8:33:33, 204.06s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=89.14%, self_consistency_acc=90.29%, p_true_acc=90.57%, normilized_likelihood_acc=89.43%, normilized_entropy_acc=89.14%, topk_entropy_acc=89.14%, window_entropy_acc=90.00%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 350/500 [29:55:33<8:49:31, 211.81s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=89.14%, self_consistency_acc=90.29%, p_true_acc=90.57%, normilized_likelihood_acc=89.43%, normilized_entropy_acc=89.14%, topk_entropy_acc=89.14%, window_entropy_acc=90.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 8.49100139277751
    Answer: 90
    Ground truth:  90
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 8.49100139277751
    Answer: 90
    Ground truth:  90
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 15.995561301708221
    Answer: 90
    Ground truth:  90
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 1.0
    Answer: 90
    Ground truth:  90
Method 5: p_true
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 14.03515625
    Answer: 90
    Ground truth:  90
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 1.3908325284719467
    Answer: 90
    Ground truth:  90
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 0.6936317682266235
    Answer: 90
    Ground truth:  90
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 0.6956124156713486
    Answer: 90
    Ground truth:  90
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of sunflower seeds, we first need to determine the numb...
    Score: 2.7024746537208557
    Answer: 90
    Ground truth:  90
Method name: attention_weighted_confidence, running accuracy: 90.02849002849003
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.02849002849003
Method name: cer_prob_product_log_last, running accuracy: 89.17378917378917
Method name: self_consistency, running accuracy: 90.31339031339031
Method name: p_true, running accuracy: 90.5982905982906
Method name: normilized_likelihood, running accuracy: 89.45868945868945
Method name: normilized_entropy, running accuracy: 89.17378917378917
Method name: topk_entropy, running accuracy: 89.17378917378917
Method name: window_entropy, running accuracy: 90.02849002849003
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 350/500 [30:00:49<8:49:31, 211.81s/it, attention_weighted_confidence_acc=90.03%, cer_entropy_weighted_mean_all_acc=90.03%, cer_prob_product_log_last_acc=89.17%, self_consistency_acc=90.31%, p_true_acc=90.60%, normilized_likelihood_acc=89.46%, normilized_entropy_acc=89.17%, topk_entropy_acc=89.17%, window_entropy_acc=90.03%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 351/500 [30:00:49<10:03:30, 243.03s/it, attention_weighted_confidence_acc=90.03%, cer_entropy_weighted_mean_all_acc=90.03%, cer_prob_product_log_last_acc=89.17%, self_consistency_acc=90.31%, p_true_acc=90.60%, normilized_likelihood_acc=89.46%, normilized_entropy_acc=89.17%, topk_entropy_acc=89.17%, window_entropy_acc=90.03%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 8.77359797337566
    Answer: 142
    Ground truth:  142
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 8.77359797337566
    Answer: 142
    Ground truth:  142
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 15.99978256225586
    Answer: 142
    Ground truth:  142
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 1.0
    Answer: 142
    Ground truth:  142
Method 5: p_true
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 11.70703125
    Answer: 142
    Ground truth:  142
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 1.7802250385284424
    Answer: 142
    Ground truth:  142
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 1.1448896676301956
    Answer: 142
    Ground truth:  142
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 1.1317579001188278
    Answer: 142
    Ground truth:  142
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of fruits, we first need to calculate the total number ...
    Score: 5.4591149389743805
    Answer: 142
    Ground truth:  142
Method name: attention_weighted_confidence, running accuracy: 90.05681818181817
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.05681818181817
Method name: cer_prob_product_log_last, running accuracy: 89.20454545454545
Method name: self_consistency, running accuracy: 90.3409090909091
Method name: p_true, running accuracy: 90.625
Method name: normilized_likelihood, running accuracy: 89.48863636363636
Method name: normilized_entropy, running accuracy: 89.20454545454545
Method name: topk_entropy, running accuracy: 89.20454545454545
Method name: window_entropy, running accuracy: 90.05681818181817
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 351/500 [30:04:54<10:03:30, 243.03s/it, attention_weighted_confidence_acc=90.06%, cer_entropy_weighted_mean_all_acc=90.06%, cer_prob_product_log_last_acc=89.20%, self_consistency_acc=90.34%, p_true_acc=90.62%, normilized_likelihood_acc=89.49%, normilized_entropy_acc=89.20%, topk_entropy_acc=89.20%, window_entropy_acc=90.06%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 352/500 [30:04:54<10:00:38, 243.50s/it, attention_weighted_confidence_acc=90.06%, cer_entropy_weighted_mean_all_acc=90.06%, cer_prob_product_log_last_acc=89.20%, self_consistency_acc=90.34%, p_true_acc=90.62%, normilized_likelihood_acc=89.49%, normilized_entropy_acc=89.20%, topk_entropy_acc=89.20%, window_entropy_acc=90.06%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 8.225497336896888
    Answer: 14
    Ground truth:  14
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 8.225497336896888
    Answer: 14
    Ground truth:  14
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 15.994465053081512
    Answer: 14
    Ground truth:  14
Method 4: self_consistency
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 1.0
    Answer: 14
    Ground truth:  14
Method 5: p_true
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 10.384765625
    Answer: 14
    Ground truth:  14
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 1.405639335513115
    Answer: 14
    Ground truth:  14
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 0.5987448692321777
    Answer: 14
    Ground truth:  14
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 0.5957740396261215
    Answer: 14
    Ground truth:  14
Method 9: window_entropy
  Batch 1:
    Text: To find out how much they spent together, we need to calculate the cost for each...
    Score: 3.8957430720329285
    Answer: 14
    Ground truth:  14
Method name: attention_weighted_confidence, running accuracy: 90.08498583569406
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.08498583569406
Method name: cer_prob_product_log_last, running accuracy: 89.23512747875354
Method name: self_consistency, running accuracy: 90.36827195467421
Method name: p_true, running accuracy: 90.6515580736544
Method name: normilized_likelihood, running accuracy: 89.51841359773371
Method name: normilized_entropy, running accuracy: 89.23512747875354
Method name: topk_entropy, running accuracy: 89.23512747875354
Method name: window_entropy, running accuracy: 90.08498583569406
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 352/500 [30:08:26<10:00:38, 243.50s/it, attention_weighted_confidence_acc=90.08%, cer_entropy_weighted_mean_all_acc=90.08%, cer_prob_product_log_last_acc=89.24%, self_consistency_acc=90.37%, p_true_acc=90.65%, normilized_likelihood_acc=89.52%, normilized_entropy_acc=89.24%, topk_entropy_acc=89.24%, window_entropy_acc=90.08%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 353/500 [30:08:26<9:34:02, 234.31s/it, attention_weighted_confidence_acc=90.08%, cer_entropy_weighted_mean_all_acc=90.08%, cer_prob_product_log_last_acc=89.24%, self_consistency_acc=90.37%, p_true_acc=90.65%, normilized_likelihood_acc=89.52%, normilized_entropy_acc=89.24%, topk_entropy_acc=89.24%, window_entropy_acc=90.08%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 7.723219200239167
    Answer: 360
    Ground truth:  360
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 7.723219200239167
    Answer: 360
    Ground truth:  360
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 15.995400607585907
    Answer: 360
    Ground truth:  360
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 1.0
    Answer: 360
    Ground truth:  360
Method 5: p_true
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 14.67578125
    Answer: 360
    Ground truth:  360
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 3.969272881746292
    Answer: 360
    Ground truth:  360
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 1.2656114846467972
    Answer: 360
    Ground truth:  360
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 1.265595182776451
    Answer: 360
    Ground truth:  360
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount John paid, we need to follow these steps:

1. First, we...
    Score: 2.8323642313480377
    Answer: 360
    Ground truth:  360
Method name: attention_weighted_confidence, running accuracy: 90.11299435028248
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.11299435028248
Method name: cer_prob_product_log_last, running accuracy: 89.26553672316385
Method name: self_consistency, running accuracy: 90.3954802259887
Method name: p_true, running accuracy: 90.67796610169492
Method name: normilized_likelihood, running accuracy: 89.54802259887006
Method name: normilized_entropy, running accuracy: 89.26553672316385
Method name: topk_entropy, running accuracy: 89.26553672316385
Method name: window_entropy, running accuracy: 90.11299435028248
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 353/500 [30:11:13<9:34:02, 234.31s/it, attention_weighted_confidence_acc=90.11%, cer_entropy_weighted_mean_all_acc=90.11%, cer_prob_product_log_last_acc=89.27%, self_consistency_acc=90.40%, p_true_acc=90.68%, normilized_likelihood_acc=89.55%, normilized_entropy_acc=89.27%, topk_entropy_acc=89.27%, window_entropy_acc=90.11%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 354/500 [30:11:13<8:41:03, 214.13s/it, attention_weighted_confidence_acc=90.11%, cer_entropy_weighted_mean_all_acc=90.11%, cer_prob_product_log_last_acc=89.27%, self_consistency_acc=90.40%, p_true_acc=90.68%, normilized_likelihood_acc=89.55%, normilized_entropy_acc=89.27%, topk_entropy_acc=89.27%, window_entropy_acc=90.11%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 8.462820872853374
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 8.462820872853374
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 15.991008520126343
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 1.0
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 12.59375
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 1.3744791448116302
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 0.5898829847574234
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 0.5894756466150284
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find the amount of water Peter has left, we first need to calculate the total...
    Score: 2.528668999671936
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 90.14084507042254
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.14084507042254
Method name: cer_prob_product_log_last, running accuracy: 89.29577464788733
Method name: self_consistency, running accuracy: 90.4225352112676
Method name: p_true, running accuracy: 90.70422535211267
Method name: normilized_likelihood, running accuracy: 89.5774647887324
Method name: normilized_entropy, running accuracy: 89.29577464788733
Method name: topk_entropy, running accuracy: 89.29577464788733
Method name: window_entropy, running accuracy: 90.14084507042254
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 354/500 [30:16:42<8:41:03, 214.13s/it, attention_weighted_confidence_acc=90.14%, cer_entropy_weighted_mean_all_acc=90.14%, cer_prob_product_log_last_acc=89.30%, self_consistency_acc=90.42%, p_true_acc=90.70%, normilized_likelihood_acc=89.58%, normilized_entropy_acc=89.30%, topk_entropy_acc=89.30%, window_entropy_acc=90.14%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 355/500 [30:16:42<10:00:44, 248.58s/it, attention_weighted_confidence_acc=90.14%, cer_entropy_weighted_mean_all_acc=90.14%, cer_prob_product_log_last_acc=89.30%, self_consistency_acc=90.42%, p_true_acc=90.70%, normilized_likelihood_acc=89.58%, normilized_entropy_acc=89.30%, topk_entropy_acc=89.30%, window_entropy_acc=90.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 7.021900734831885
    Answer: 66
    Ground truth:  66
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 7.021900734831885
    Answer: 66
    Ground truth:  66
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 13.861656427383423
    Answer: 66
    Ground truth:  66
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 0.875
    Answer: 66
    Ground truth:  66
Method 5: p_true
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 9.88037109375
    Answer: 66
    Ground truth:  66
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 9.885451823472977
    Answer: 66
    Ground truth:  66
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 9.799887374043465
    Answer: 66
    Ground truth:  66
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 8.557006254792213
    Answer: 66
    Ground truth:  66
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of magazines Susan gets every year, we need to find the...
    Score: 17.04316222667694
    Answer: 66
    Ground truth:  66
Method name: attention_weighted_confidence, running accuracy: 90.1685393258427
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.1685393258427
Method name: cer_prob_product_log_last, running accuracy: 89.32584269662921
Method name: self_consistency, running accuracy: 90.4494382022472
Method name: p_true, running accuracy: 90.73033707865169
Method name: normilized_likelihood, running accuracy: 89.60674157303372
Method name: normilized_entropy, running accuracy: 89.32584269662921
Method name: topk_entropy, running accuracy: 89.32584269662921
Method name: window_entropy, running accuracy: 90.1685393258427
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 355/500 [30:20:41<10:00:44, 248.58s/it, attention_weighted_confidence_acc=90.17%, cer_entropy_weighted_mean_all_acc=90.17%, cer_prob_product_log_last_acc=89.33%, self_consistency_acc=90.45%, p_true_acc=90.73%, normilized_likelihood_acc=89.61%, normilized_entropy_acc=89.33%, topk_entropy_acc=89.33%, window_entropy_acc=90.17%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 356/500 [30:20:41<9:49:08, 245.48s/it, attention_weighted_confidence_acc=90.17%, cer_entropy_weighted_mean_all_acc=90.17%, cer_prob_product_log_last_acc=89.33%, self_consistency_acc=90.45%, p_true_acc=90.73%, normilized_likelihood_acc=89.61%, normilized_entropy_acc=89.33%, topk_entropy_acc=89.33%, window_entropy_acc=90.17%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 5.61638084632058
    Answer: 180
    Ground truth:  180
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 5.61638084632058
    Answer: 180
    Ground truth:  180
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 10.99993646144867
    Answer: 180
    Ground truth:  180
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 0.6875
    Answer: 180
    Ground truth:  180
Method 5: p_true
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 9.07421875
    Answer: 180
    Ground truth:  180
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 6.78596468269825
    Answer: 180
    Ground truth:  180
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 7.0005471259355545
    Answer: 180
    Ground truth:  180
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 6.296889930963516
    Answer: 180
    Ground truth:  180
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Tom's restaurant makes in a week, we need to calculate the ...
    Score: 15.367157995700836
    Answer: 180
    Ground truth:  180
Method name: attention_weighted_confidence, running accuracy: 90.19607843137256
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.19607843137256
Method name: cer_prob_product_log_last, running accuracy: 89.35574229691878
Method name: self_consistency, running accuracy: 90.47619047619048
Method name: p_true, running accuracy: 90.75630252100841
Method name: normilized_likelihood, running accuracy: 89.6358543417367
Method name: normilized_entropy, running accuracy: 89.35574229691878
Method name: topk_entropy, running accuracy: 89.35574229691878
Method name: window_entropy, running accuracy: 90.19607843137256
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 356/500 [30:24:44<9:49:08, 245.48s/it, attention_weighted_confidence_acc=90.20%, cer_entropy_weighted_mean_all_acc=90.20%, cer_prob_product_log_last_acc=89.36%, self_consistency_acc=90.48%, p_true_acc=90.76%, normilized_likelihood_acc=89.64%, normilized_entropy_acc=89.36%, topk_entropy_acc=89.36%, window_entropy_acc=90.20%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████▏  | 357/500 [30:24:44<9:43:35, 244.87s/it, attention_weighted_confidence_acc=90.20%, cer_entropy_weighted_mean_all_acc=90.20%, cer_prob_product_log_last_acc=89.36%, self_consistency_acc=90.48%, p_true_acc=90.76%, normilized_likelihood_acc=89.64%, normilized_entropy_acc=89.36%, topk_entropy_acc=89.36%, window_entropy_acc=90.20%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 7.568715055390131
    Answer: 54
    Ground truth:  54
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 7.568715055390131
    Answer: 54
    Ground truth:  54
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 14.999107480049133
    Answer: 54
    Ground truth:  54
Method 4: self_consistency
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 0.9375
    Answer: 54
    Ground truth:  54
Method 5: p_true
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 13.6171875
    Answer: 54
    Ground truth:  54
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 3.7865977585315704
    Answer: 54
    Ground truth:  54
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 3.07613468170166
    Answer: 54
    Ground truth:  54
Method 8: topk_entropy
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 2.992339611053467
    Answer: 54
    Ground truth:  54
Method 9: window_entropy
  Batch 1:
    Text: Let's break the problem down step by step.

1. The tow truck pulled ten cars eac...
    Score: 3.441601812839508
    Answer: 54
    Ground truth:  54
Method name: attention_weighted_confidence, running accuracy: 90.22346368715084
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.22346368715084
Method name: cer_prob_product_log_last, running accuracy: 89.3854748603352
Method name: self_consistency, running accuracy: 90.5027932960894
Method name: p_true, running accuracy: 90.78212290502793
Method name: normilized_likelihood, running accuracy: 89.66480446927375
Method name: normilized_entropy, running accuracy: 89.3854748603352
Method name: topk_entropy, running accuracy: 89.3854748603352
Method name: window_entropy, running accuracy: 90.22346368715084
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████▏  | 357/500 [30:29:55<9:43:35, 244.87s/it, attention_weighted_confidence_acc=90.22%, cer_entropy_weighted_mean_all_acc=90.22%, cer_prob_product_log_last_acc=89.39%, self_consistency_acc=90.50%, p_true_acc=90.78%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=89.39%, topk_entropy_acc=89.39%, window_entropy_acc=90.22%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 358/500 [30:29:55<10:26:05, 264.55s/it, attention_weighted_confidence_acc=90.22%, cer_entropy_weighted_mean_all_acc=90.22%, cer_prob_product_log_last_acc=89.39%, self_consistency_acc=90.50%, p_true_acc=90.78%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=89.39%, topk_entropy_acc=89.39%, window_entropy_acc=90.22%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 6.979465774922885
    Answer: 842
    Ground truth:  842
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 6.979465774922885
    Answer: 842
    Ground truth:  842
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 13.999850511550903
    Answer: 842
    Ground truth:  842
Method 4: self_consistency
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 0.875
    Answer: 842
    Ground truth:  842
Method 5: p_true
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 10.24609375
    Answer: 842
    Ground truth:  842
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 9.652717530727386
    Answer: 842
    Ground truth:  842
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 8.531939297914505
    Answer: 842
    Ground truth:  842
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 7.678576439619064
    Answer: 842
    Ground truth:  842
Method 9: window_entropy
  Batch 1:
    Text: To find out how many bottles of apple juice the teacher has left for herself, we...
    Score: 18.535574853420258
    Answer: 842
    Ground truth:  842
Method name: attention_weighted_confidence, running accuracy: 90.25069637883009
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.25069637883009
Method name: cer_prob_product_log_last, running accuracy: 89.41504178272982
Method name: self_consistency, running accuracy: 90.5292479108635
Method name: p_true, running accuracy: 90.80779944289694
Method name: normilized_likelihood, running accuracy: 89.69359331476323
Method name: normilized_entropy, running accuracy: 89.41504178272982
Method name: topk_entropy, running accuracy: 89.41504178272982
Method name: window_entropy, running accuracy: 90.25069637883009
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 358/500 [30:34:35<10:26:05, 264.55s/it, attention_weighted_confidence_acc=90.25%, cer_entropy_weighted_mean_all_acc=90.25%, cer_prob_product_log_last_acc=89.42%, self_consistency_acc=90.53%, p_true_acc=90.81%, normilized_likelihood_acc=89.69%, normilized_entropy_acc=89.42%, topk_entropy_acc=89.42%, window_entropy_acc=90.25%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 359/500 [30:34:35<10:32:55, 269.33s/it, attention_weighted_confidence_acc=90.25%, cer_entropy_weighted_mean_all_acc=90.25%, cer_prob_product_log_last_acc=89.42%, self_consistency_acc=90.53%, p_true_acc=90.81%, normilized_likelihood_acc=89.69%, normilized_entropy_acc=89.42%, topk_entropy_acc=89.42%, window_entropy_acc=90.25%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 8.07172787490941
    Answer: -10
    Ground truth:  -10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 8.07172787490941
    Answer: -10
    Ground truth:  -10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 7.959715709593237
    Answer: -10
    Ground truth:  -10
Method 4: self_consistency
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 1.0
    Answer: -10
    Ground truth:  -10
Method 5: p_true
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 4.1142578125
    Answer: -10
    Ground truth:  -10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 3.8062265515327454
    Answer: -10
    Ground truth:  -10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 4.669704914093018
    Answer: -10
    Ground truth:  -10
Method 8: topk_entropy
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 4.4643454402685165
    Answer: -10
    Ground truth:  -10
Method 9: window_entropy
  Batch 1:
    Text: To find the average highest temperature, we first need to find the sum of the hi...
    Score: 13.858127236366272
    Answer: -10
    Ground truth:  -10
Method name: attention_weighted_confidence, running accuracy: 90.27777777777779
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.27777777777779
Method name: cer_prob_product_log_last, running accuracy: 89.44444444444444
Method name: self_consistency, running accuracy: 90.55555555555556
Method name: p_true, running accuracy: 90.83333333333333
Method name: normilized_likelihood, running accuracy: 89.72222222222223
Method name: normilized_entropy, running accuracy: 89.44444444444444
Method name: topk_entropy, running accuracy: 89.44444444444444
Method name: window_entropy, running accuracy: 90.27777777777779
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 359/500 [30:37:28<10:32:55, 269.33s/it, attention_weighted_confidence_acc=90.28%, cer_entropy_weighted_mean_all_acc=90.28%, cer_prob_product_log_last_acc=89.44%, self_consistency_acc=90.56%, p_true_acc=90.83%, normilized_likelihood_acc=89.72%, normilized_entropy_acc=89.44%, topk_entropy_acc=89.44%, window_entropy_acc=90.28%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 360/500 [30:37:28<9:20:52, 240.37s/it, attention_weighted_confidence_acc=90.28%, cer_entropy_weighted_mean_all_acc=90.28%, cer_prob_product_log_last_acc=89.44%, self_consistency_acc=90.56%, p_true_acc=90.83%, normilized_likelihood_acc=89.72%, normilized_entropy_acc=89.44%, topk_entropy_acc=89.44%, window_entropy_acc=90.28%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 7.06040856824026
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 7.06040856824026
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 13.92325097322464
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 0.875
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 11.92578125
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 2.600297287106514
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 3.1870242059230804
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 3.0413798093795776
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  First, let's find the time taken by the dog to eat the sausages. Since ...
    Score: 12.804189383983612
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 90.30470914127424
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.30470914127424
Method name: cer_prob_product_log_last, running accuracy: 89.47368421052632
Method name: self_consistency, running accuracy: 90.58171745152355
Method name: p_true, running accuracy: 90.85872576177285
Method name: normilized_likelihood, running accuracy: 89.75069252077562
Method name: normilized_entropy, running accuracy: 89.47368421052632
Method name: topk_entropy, running accuracy: 89.47368421052632
Method name: window_entropy, running accuracy: 90.30470914127424
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 360/500 [30:42:39<9:20:52, 240.37s/it, attention_weighted_confidence_acc=90.30%, cer_entropy_weighted_mean_all_acc=90.30%, cer_prob_product_log_last_acc=89.47%, self_consistency_acc=90.58%, p_true_acc=90.86%, normilized_likelihood_acc=89.75%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=90.30%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 361/500 [30:42:39<10:06:12, 261.67s/it, attention_weighted_confidence_acc=90.30%, cer_entropy_weighted_mean_all_acc=90.30%, cer_prob_product_log_last_acc=89.47%, self_consistency_acc=90.58%, p_true_acc=90.86%, normilized_likelihood_acc=89.75%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=90.30%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 6.600455067513521
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 6.600455067513521
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 12.99321323633194
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 0.8125
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 11.125
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 5.067596569657326
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 4.92168927192688
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 4.524797022342682
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's break it down step by step.

1. Initially, the bus h...
    Score: 12.383556604385376
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 90.33149171270718
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.33149171270718
Method name: cer_prob_product_log_last, running accuracy: 89.50276243093923
Method name: self_consistency, running accuracy: 90.60773480662984
Method name: p_true, running accuracy: 90.88397790055248
Method name: normilized_likelihood, running accuracy: 89.77900552486187
Method name: normilized_entropy, running accuracy: 89.50276243093923
Method name: topk_entropy, running accuracy: 89.50276243093923
Method name: window_entropy, running accuracy: 90.33149171270718
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 361/500 [30:48:48<10:06:12, 261.67s/it, attention_weighted_confidence_acc=90.33%, cer_entropy_weighted_mean_all_acc=90.33%, cer_prob_product_log_last_acc=89.50%, self_consistency_acc=90.61%, p_true_acc=90.88%, normilized_likelihood_acc=89.78%, normilized_entropy_acc=89.50%, topk_entropy_acc=89.50%, window_entropy_acc=90.33%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 362/500 [30:48:48<11:15:36, 293.74s/it, attention_weighted_confidence_acc=90.33%, cer_entropy_weighted_mean_all_acc=90.33%, cer_prob_product_log_last_acc=89.50%, self_consistency_acc=90.61%, p_true_acc=90.88%, normilized_likelihood_acc=89.78%, normilized_entropy_acc=89.50%, topk_entropy_acc=89.50%, window_entropy_acc=90.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 6.789952622637493
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 6.789952622637493
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 13.853833377361298
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 0.875
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 11.421875
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 8.848314180970192
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 8.938441693782806
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 8.033919349312782
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find out how many times Peter can go to the movies, we need to determine how ...
    Score: 18.459143698215485
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 90.35812672176309
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.35812672176309
Method name: cer_prob_product_log_last, running accuracy: 89.53168044077135
Method name: self_consistency, running accuracy: 90.633608815427
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 89.80716253443526
Method name: normilized_entropy, running accuracy: 89.53168044077135
Method name: topk_entropy, running accuracy: 89.53168044077135
Method name: window_entropy, running accuracy: 90.35812672176309
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  72%|███████▏  | 362/500 [30:52:02<11:15:36, 293.74s/it, attention_weighted_confidence_acc=90.36%, cer_entropy_weighted_mean_all_acc=90.36%, cer_prob_product_log_last_acc=89.53%, self_consistency_acc=90.63%, p_true_acc=90.91%, normilized_likelihood_acc=89.81%, normilized_entropy_acc=89.53%, topk_entropy_acc=89.53%, window_entropy_acc=90.36%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 363/500 [30:52:02<10:02:25, 263.83s/it, attention_weighted_confidence_acc=90.36%, cer_entropy_weighted_mean_all_acc=90.36%, cer_prob_product_log_last_acc=89.53%, self_consistency_acc=90.63%, p_true_acc=90.91%, normilized_likelihood_acc=89.81%, normilized_entropy_acc=89.53%, topk_entropy_acc=89.53%, window_entropy_acc=90.36%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 5.088649109432691
    Answer: 17
    Ground truth:  17
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 5.088649109432691
    Answer: 17
    Ground truth:  17
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 9.955415546894073
    Answer: 17
    Ground truth:  17
Method 4: self_consistency
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 0.625
    Answer: 17
    Ground truth:  17
Method 5: p_true
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 5.125
    Answer: 17
    Ground truth:  17
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 1.8665554970502853
    Answer: 17
    Ground truth:  17
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 1.4336881041526794
    Answer: 17
    Ground truth:  17
Method 8: topk_entropy
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 1.3637883365154266
    Answer: 17
    Ground truth:  17
Method 9: window_entropy
  Batch 1:
    Text: To find the total weight, we first need to find the weight of Cindy's history bo...
    Score: 6.934519708156586
    Answer: 17
    Ground truth:  17
Method name: attention_weighted_confidence, running accuracy: 90.38461538461539
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.38461538461539
Method name: cer_prob_product_log_last, running accuracy: 89.56043956043956
Method name: self_consistency, running accuracy: 90.65934065934066
Method name: p_true, running accuracy: 90.93406593406593
Method name: normilized_likelihood, running accuracy: 89.83516483516483
Method name: normilized_entropy, running accuracy: 89.56043956043956
Method name: topk_entropy, running accuracy: 89.56043956043956
Method name: window_entropy, running accuracy: 90.38461538461539
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 363/500 [30:56:19<10:02:25, 263.83s/it, attention_weighted_confidence_acc=90.38%, cer_entropy_weighted_mean_all_acc=90.38%, cer_prob_product_log_last_acc=89.56%, self_consistency_acc=90.66%, p_true_acc=90.93%, normilized_likelihood_acc=89.84%, normilized_entropy_acc=89.56%, topk_entropy_acc=89.56%, window_entropy_acc=90.38%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 364/500 [30:56:19<9:53:26, 261.81s/it, attention_weighted_confidence_acc=90.38%, cer_entropy_weighted_mean_all_acc=90.38%, cer_prob_product_log_last_acc=89.56%, self_consistency_acc=90.66%, p_true_acc=90.93%, normilized_likelihood_acc=89.84%, normilized_entropy_acc=89.56%, topk_entropy_acc=89.56%, window_entropy_acc=90.38%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 4.550311592732488
    Answer: 1490
    Ground truth:  1490
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 4.550311592732488
    Answer: 1490
    Ground truth:  1490
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 4.499849170935683
    Answer: 1490
    Ground truth:  1490
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 0.5625
    Answer: 1490
    Ground truth:  1490
Method 5: p_true
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 5.51171875
    Answer: 1490
    Ground truth:  1490
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 9.65236496180296
    Answer: 1490
    Ground truth:  1490
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 9.40833568572998
    Answer: 1490
    Ground truth:  1490
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 8.013518646359444
    Answer: 1490
    Ground truth:  1490
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of porcupines after a year, we need to calculate the nu...
    Score: 13.901840567588806
    Answer: 1490
    Ground truth:  1490
Method name: attention_weighted_confidence, running accuracy: 90.41095890410958
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.41095890410958
Method name: cer_prob_product_log_last, running accuracy: 89.58904109589041
Method name: self_consistency, running accuracy: 90.68493150684932
Method name: p_true, running accuracy: 90.95890410958904
Method name: normilized_likelihood, running accuracy: 89.86301369863014
Method name: normilized_entropy, running accuracy: 89.58904109589041
Method name: topk_entropy, running accuracy: 89.58904109589041
Method name: window_entropy, running accuracy: 90.41095890410958
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 364/500 [31:04:53<9:53:26, 261.81s/it, attention_weighted_confidence_acc=90.41%, cer_entropy_weighted_mean_all_acc=90.41%, cer_prob_product_log_last_acc=89.59%, self_consistency_acc=90.68%, p_true_acc=90.96%, normilized_likelihood_acc=89.86%, normilized_entropy_acc=89.59%, topk_entropy_acc=89.59%, window_entropy_acc=90.41%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 365/500 [31:04:53<12:39:21, 337.49s/it, attention_weighted_confidence_acc=90.41%, cer_entropy_weighted_mean_all_acc=90.41%, cer_prob_product_log_last_acc=89.59%, self_consistency_acc=90.68%, p_true_acc=90.96%, normilized_likelihood_acc=89.86%, normilized_entropy_acc=89.59%, topk_entropy_acc=89.59%, window_entropy_acc=90.41%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 7.984110593452074
    Answer: 80
    Ground truth:  80
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 7.984110593452074
    Answer: 80
    Ground truth:  80
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 15.972245573997498
    Answer: 80
    Ground truth:  80
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 1.0
    Answer: 80
    Ground truth:  80
Method 5: p_true
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 15.01953125
    Answer: 80
    Ground truth:  80
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 2.252150744199753
    Answer: 80
    Ground truth:  80
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 1.2044982612133026
    Answer: 80
    Ground truth:  80
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 1.195765733718872
    Answer: 80
    Ground truth:  80
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Jeff gets, let's first represent the amount of money Brad g...
    Score: 2.4359187483787537
    Answer: 80
    Ground truth:  80
Method name: attention_weighted_confidence, running accuracy: 90.43715846994536
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.43715846994536
Method name: cer_prob_product_log_last, running accuracy: 89.61748633879782
Method name: self_consistency, running accuracy: 90.7103825136612
Method name: p_true, running accuracy: 90.98360655737704
Method name: normilized_likelihood, running accuracy: 89.89071038251366
Method name: normilized_entropy, running accuracy: 89.61748633879782
Method name: topk_entropy, running accuracy: 89.61748633879782
Method name: window_entropy, running accuracy: 90.43715846994536
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 365/500 [31:09:00<12:39:21, 337.49s/it, attention_weighted_confidence_acc=90.44%, cer_entropy_weighted_mean_all_acc=90.44%, cer_prob_product_log_last_acc=89.62%, self_consistency_acc=90.71%, p_true_acc=90.98%, normilized_likelihood_acc=89.89%, normilized_entropy_acc=89.62%, topk_entropy_acc=89.62%, window_entropy_acc=90.44%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 366/500 [31:09:00<11:32:49, 310.22s/it, attention_weighted_confidence_acc=90.44%, cer_entropy_weighted_mean_all_acc=90.44%, cer_prob_product_log_last_acc=89.62%, self_consistency_acc=90.71%, p_true_acc=90.98%, normilized_likelihood_acc=89.89%, normilized_entropy_acc=89.62%, topk_entropy_acc=89.62%, window_entropy_acc=90.44%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 8.700415179695394
    Answer: 130
    Ground truth:  130
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 8.700415179695394
    Answer: 130
    Ground truth:  130
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 15.999968409538269
    Answer: 130
    Ground truth:  130
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 1.0
    Answer: 130
    Ground truth:  130
Method 5: p_true
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 12.69140625
    Answer: 130
    Ground truth:  130
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 1.4514263421297073
    Answer: 130
    Ground truth:  130
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 1.1681909710168839
    Answer: 130
    Ground truth:  130
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 1.1645817309617996
    Answer: 130
    Ground truth:  130
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Lloyd earned for the first two weeks, we need to calculate ...
    Score: 2.319516748189926
    Answer: 130
    Ground truth:  130
Method name: attention_weighted_confidence, running accuracy: 90.46321525885558
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.46321525885558
Method name: cer_prob_product_log_last, running accuracy: 89.64577656675749
Method name: self_consistency, running accuracy: 90.73569482288828
Method name: p_true, running accuracy: 91.00817438692098
Method name: normilized_likelihood, running accuracy: 89.91825613079018
Method name: normilized_entropy, running accuracy: 89.64577656675749
Method name: topk_entropy, running accuracy: 89.64577656675749
Method name: window_entropy, running accuracy: 90.46321525885558
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 366/500 [31:12:42<11:32:49, 310.22s/it, attention_weighted_confidence_acc=90.46%, cer_entropy_weighted_mean_all_acc=90.46%, cer_prob_product_log_last_acc=89.65%, self_consistency_acc=90.74%, p_true_acc=91.01%, normilized_likelihood_acc=89.92%, normilized_entropy_acc=89.65%, topk_entropy_acc=89.65%, window_entropy_acc=90.46%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 367/500 [31:12:42<10:29:08, 283.82s/it, attention_weighted_confidence_acc=90.46%, cer_entropy_weighted_mean_all_acc=90.46%, cer_prob_product_log_last_acc=89.65%, self_consistency_acc=90.74%, p_true_acc=91.01%, normilized_likelihood_acc=89.92%, normilized_entropy_acc=89.65%, topk_entropy_acc=89.65%, window_entropy_acc=90.46%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 4.119512313735373
    Answer: 75.0
    Ground truth:  75
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 4.119512313735373
    Answer: 75.0
    Ground truth:  75
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Terry spends on yogurt over 30 days, we need to follow thes...
    Score: 6.999510824680328
    Answer: 75
    Ground truth:  75
Method 4: self_consistency
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 0.5
    Answer: 75.0
    Ground truth:  75
Method 5: p_true
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 7.078125
    Answer: 75.0
    Ground truth:  75
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 1.4328903406858444
    Answer: 75.0
    Ground truth:  75
Method 7: normilized_entropy
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 1.0935072153806686
    Answer: 75.0
    Ground truth:  75
Method 8: topk_entropy
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 1.0888968259096146
    Answer: 75.0
    Ground truth:  75
Method 9: window_entropy
  Batch 1:
    Text: To find the amount Terry spends on yogurt over 30 days, we'll break it down step...
    Score: 3.9096707105636597
    Answer: 75.0
    Ground truth:  75
Method name: attention_weighted_confidence, running accuracy: 90.48913043478261
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.48913043478261
Method name: cer_prob_product_log_last, running accuracy: 89.67391304347827
Method name: self_consistency, running accuracy: 90.76086956521739
Method name: p_true, running accuracy: 91.03260869565217
Method name: normilized_likelihood, running accuracy: 89.94565217391305
Method name: normilized_entropy, running accuracy: 89.67391304347827
Method name: topk_entropy, running accuracy: 89.67391304347827
Method name: window_entropy, running accuracy: 90.48913043478261
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  73%|███████▎  | 367/500 [31:17:16<10:29:08, 283.82s/it, attention_weighted_confidence_acc=90.49%, cer_entropy_weighted_mean_all_acc=90.49%, cer_prob_product_log_last_acc=89.67%, self_consistency_acc=90.76%, p_true_acc=91.03%, normilized_likelihood_acc=89.95%, normilized_entropy_acc=89.67%, topk_entropy_acc=89.67%, window_entropy_acc=90.49%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▎  | 368/500 [31:17:16<10:18:15, 281.02s/it, attention_weighted_confidence_acc=90.49%, cer_entropy_weighted_mean_all_acc=90.49%, cer_prob_product_log_last_acc=89.67%, self_consistency_acc=90.76%, p_true_acc=91.03%, normilized_likelihood_acc=89.95%, normilized_entropy_acc=89.67%, topk_entropy_acc=89.67%, window_entropy_acc=90.49%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 8.812532300117956
    Answer: 157
    Ground truth:  157
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 8.812532300117956
    Answer: 157
    Ground truth:  157
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 15.997036874294281
    Answer: 157
    Ground truth:  157
Method 4: self_consistency
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 1.0
    Answer: 157
    Ground truth:  157
Method 5: p_true
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 12.99609375
    Answer: 157
    Ground truth:  157
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 1.7698551192879677
    Answer: 157
    Ground truth:  157
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 1.750446580350399
    Answer: 157
    Ground truth:  157
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 1.7298249751329422
    Answer: 157
    Ground truth:  157
Method 9: window_entropy
  Batch 1:
    Text: To find out how much the store will receive after selling all the balls, we need...
    Score: 3.4174137115478516
    Answer: 157
    Ground truth:  157
Method name: attention_weighted_confidence, running accuracy: 90.5149051490515
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.5149051490515
Method name: cer_prob_product_log_last, running accuracy: 89.70189701897019
Method name: self_consistency, running accuracy: 90.78590785907859
Method name: p_true, running accuracy: 91.05691056910568
Method name: normilized_likelihood, running accuracy: 89.97289972899729
Method name: normilized_entropy, running accuracy: 89.70189701897019
Method name: topk_entropy, running accuracy: 89.70189701897019
Method name: window_entropy, running accuracy: 90.5149051490515
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▎  | 368/500 [31:21:56<10:18:15, 281.02s/it, attention_weighted_confidence_acc=90.51%, cer_entropy_weighted_mean_all_acc=90.51%, cer_prob_product_log_last_acc=89.70%, self_consistency_acc=90.79%, p_true_acc=91.06%, normilized_likelihood_acc=89.97%, normilized_entropy_acc=89.70%, topk_entropy_acc=89.70%, window_entropy_acc=90.51%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 369/500 [31:21:56<10:12:48, 280.67s/it, attention_weighted_confidence_acc=90.51%, cer_entropy_weighted_mean_all_acc=90.51%, cer_prob_product_log_last_acc=89.70%, self_consistency_acc=90.79%, p_true_acc=91.06%, normilized_likelihood_acc=89.97%, normilized_entropy_acc=89.70%, topk_entropy_acc=89.70%, window_entropy_acc=90.51%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 7.715572710633404
    Answer: 7.0
    Ground truth:  7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 7.715572710633404
    Answer: 7.0
    Ground truth:  7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the cost of the apple pie, let's follow the calculations step by step:

...
    Score: 5.4412917961599305e-05
    Answer: 12
    Ground truth:  7
Method 4: self_consistency
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 0.9375
    Answer: 7.0
    Ground truth:  7
Method 5: p_true
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 4.78173828125
    Answer: 7.0
    Ground truth:  7
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 9.490071922540665
    Answer: 7.0
    Ground truth:  7
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 7.528689667582512
    Answer: 7.0
    Ground truth:  7
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 6.4814000725746155
    Answer: 7.0
    Ground truth:  7
Method 9: window_entropy
  Batch 1:
    Text: To find out how much the apple pie cost, we first need to find the total amount ...
    Score: 22.238996505737305
    Answer: 7.0
    Ground truth:  7
Method name: attention_weighted_confidence, running accuracy: 90.54054054054053
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.54054054054053
Method name: cer_prob_product_log_last, running accuracy: 89.45945945945945
Method name: self_consistency, running accuracy: 90.81081081081082
Method name: p_true, running accuracy: 91.08108108108108
Method name: normilized_likelihood, running accuracy: 90.0
Method name: normilized_entropy, running accuracy: 89.72972972972974
Method name: topk_entropy, running accuracy: 89.72972972972974
Method name: window_entropy, running accuracy: 90.54054054054053
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 369/500 [31:25:59<10:12:48, 280.67s/it, attention_weighted_confidence_acc=90.54%, cer_entropy_weighted_mean_all_acc=90.54%, cer_prob_product_log_last_acc=89.46%, self_consistency_acc=90.81%, p_true_acc=91.08%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=89.73%, topk_entropy_acc=89.73%, window_entropy_acc=90.54%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 370/500 [31:25:59<9:43:38, 269.37s/it, attention_weighted_confidence_acc=90.54%, cer_entropy_weighted_mean_all_acc=90.54%, cer_prob_product_log_last_acc=89.46%, self_consistency_acc=90.81%, p_true_acc=91.08%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=89.73%, topk_entropy_acc=89.73%, window_entropy_acc=90.54%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 7.704329903132155
    Answer: 168
    Ground truth:  168
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 7.704329903132155
    Answer: 168
    Ground truth:  168
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 15.899942576885223
    Answer: 168
    Ground truth:  168
Method 4: self_consistency
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 1.0
    Answer: 168
    Ground truth:  168
Method 5: p_true
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 13.89453125
    Answer: 168
    Ground truth:  168
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 1.1635687351226807
    Answer: 168
    Ground truth:  168
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 1.1378792077302933
    Answer: 168
    Ground truth:  168
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 1.1347746849060059
    Answer: 168
    Ground truth:  168
Method 9: window_entropy
  Batch 1:
    Text: To calculate the total amount Alex charges for 2 weeks of tutoring, we need to f...
    Score: 3.212585985660553
    Answer: 168
    Ground truth:  168
Method name: attention_weighted_confidence, running accuracy: 90.56603773584906
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.56603773584906
Method name: cer_prob_product_log_last, running accuracy: 89.4878706199461
Method name: self_consistency, running accuracy: 90.83557951482479
Method name: p_true, running accuracy: 91.10512129380054
Method name: normilized_likelihood, running accuracy: 90.02695417789758
Method name: normilized_entropy, running accuracy: 89.75741239892183
Method name: topk_entropy, running accuracy: 89.75741239892183
Method name: window_entropy, running accuracy: 90.56603773584906
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 370/500 [31:28:34<9:43:38, 269.37s/it, attention_weighted_confidence_acc=90.57%, cer_entropy_weighted_mean_all_acc=90.57%, cer_prob_product_log_last_acc=89.49%, self_consistency_acc=90.84%, p_true_acc=91.11%, normilized_likelihood_acc=90.03%, normilized_entropy_acc=89.76%, topk_entropy_acc=89.76%, window_entropy_acc=90.57%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 371/500 [31:28:34<8:25:00, 234.89s/it, attention_weighted_confidence_acc=90.57%, cer_entropy_weighted_mean_all_acc=90.57%, cer_prob_product_log_last_acc=89.49%, self_consistency_acc=90.84%, p_true_acc=91.11%, normilized_likelihood_acc=90.03%, normilized_entropy_acc=89.76%, topk_entropy_acc=89.76%, window_entropy_acc=90.57%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 8.27774562043813
    Answer: 14
    Ground truth:  14
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 8.27774562043813
    Answer: 14
    Ground truth:  14
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 15.999842286109924
    Answer: 14
    Ground truth:  14
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 1.0
    Answer: 14
    Ground truth:  14
Method 5: p_true
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 15.59765625
    Answer: 14
    Ground truth:  14
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 3.4719370529055595
    Answer: 14
    Ground truth:  14
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 1.380171686410904
    Answer: 14
    Ground truth:  14
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 1.3682225942611694
    Answer: 14
    Ground truth:  14
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step.

1. Naomi caught 17 fish.
2. Darren caught 6 less than...
    Score: 7.813022792339325
    Answer: 14
    Ground truth:  14
Method name: attention_weighted_confidence, running accuracy: 90.59139784946237
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.59139784946237
Method name: cer_prob_product_log_last, running accuracy: 89.51612903225806
Method name: self_consistency, running accuracy: 90.86021505376344
Method name: p_true, running accuracy: 91.12903225806451
Method name: normilized_likelihood, running accuracy: 90.05376344086021
Method name: normilized_entropy, running accuracy: 89.78494623655914
Method name: topk_entropy, running accuracy: 89.78494623655914
Method name: window_entropy, running accuracy: 90.59139784946237
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 371/500 [31:31:07<8:25:00, 234.89s/it, attention_weighted_confidence_acc=90.59%, cer_entropy_weighted_mean_all_acc=90.59%, cer_prob_product_log_last_acc=89.52%, self_consistency_acc=90.86%, p_true_acc=91.13%, normilized_likelihood_acc=90.05%, normilized_entropy_acc=89.78%, topk_entropy_acc=89.78%, window_entropy_acc=90.59%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 372/500 [31:31:07<7:29:06, 210.52s/it, attention_weighted_confidence_acc=90.59%, cer_entropy_weighted_mean_all_acc=90.59%, cer_prob_product_log_last_acc=89.52%, self_consistency_acc=90.86%, p_true_acc=91.13%, normilized_likelihood_acc=90.05%, normilized_entropy_acc=89.78%, topk_entropy_acc=89.78%, window_entropy_acc=90.59%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 7.790328654788002
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 7.790328654788002
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 14.998687386512756
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 0.9375
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 14.140625
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 8.3567273914814
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 8.31918878853321
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 7.3153925240039825
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of eggs Chester needs for 60 days, we need to calculate...
    Score: 28.250267922878265
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 90.61662198391421
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.61662198391421
Method name: cer_prob_product_log_last, running accuracy: 89.54423592493298
Method name: self_consistency, running accuracy: 90.88471849865952
Method name: p_true, running accuracy: 91.15281501340483
Method name: normilized_likelihood, running accuracy: 90.0804289544236
Method name: normilized_entropy, running accuracy: 89.81233243967829
Method name: topk_entropy, running accuracy: 89.81233243967829
Method name: window_entropy, running accuracy: 90.61662198391421
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  74%|███████▍  | 372/500 [31:35:55<7:29:06, 210.52s/it, attention_weighted_confidence_acc=90.62%, cer_entropy_weighted_mean_all_acc=90.62%, cer_prob_product_log_last_acc=89.54%, self_consistency_acc=90.88%, p_true_acc=91.15%, normilized_likelihood_acc=90.08%, normilized_entropy_acc=89.81%, topk_entropy_acc=89.81%, window_entropy_acc=90.62%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▍  | 373/500 [31:35:55<8:14:27, 233.60s/it, attention_weighted_confidence_acc=90.62%, cer_entropy_weighted_mean_all_acc=90.62%, cer_prob_product_log_last_acc=89.54%, self_consistency_acc=90.88%, p_true_acc=91.15%, normilized_likelihood_acc=90.08%, normilized_entropy_acc=89.81%, topk_entropy_acc=89.81%, window_entropy_acc=90.62%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 7.973256144038027
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 7.973256144038027
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 15.99881774187088
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 1.0
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 12.7578125
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 1.2908928841352463
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 0.8921758234500885
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 0.8961363583803177
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

Step 1: Identify how m...
    Score: 2.287284731864929
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 90.64171122994652
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.64171122994652
Method name: cer_prob_product_log_last, running accuracy: 89.57219251336899
Method name: self_consistency, running accuracy: 90.9090909090909
Method name: p_true, running accuracy: 91.17647058823529
Method name: normilized_likelihood, running accuracy: 90.10695187165776
Method name: normilized_entropy, running accuracy: 89.83957219251337
Method name: topk_entropy, running accuracy: 89.83957219251337
Method name: window_entropy, running accuracy: 90.64171122994652
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▍  | 373/500 [31:38:52<8:14:27, 233.60s/it, attention_weighted_confidence_acc=90.64%, cer_entropy_weighted_mean_all_acc=90.64%, cer_prob_product_log_last_acc=89.57%, self_consistency_acc=90.91%, p_true_acc=91.18%, normilized_likelihood_acc=90.11%, normilized_entropy_acc=89.84%, topk_entropy_acc=89.84%, window_entropy_acc=90.64%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▍  | 374/500 [31:38:52<7:35:03, 216.70s/it, attention_weighted_confidence_acc=90.64%, cer_entropy_weighted_mean_all_acc=90.64%, cer_prob_product_log_last_acc=89.57%, self_consistency_acc=90.91%, p_true_acc=91.18%, normilized_likelihood_acc=90.11%, normilized_entropy_acc=89.84%, topk_entropy_acc=89.84%, window_entropy_acc=90.64%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 7.433064703254742
    Answer: 140000
    Ground truth:  140000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 7.433064703254742
    Answer: 140000
    Ground truth:  140000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 2.196671379453666
    Answer: 140000
    Ground truth:  140000
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 0.9375
    Answer: 140000
    Ground truth:  140000
Method 5: p_true
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 14.28125
    Answer: 140000
    Ground truth:  140000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 1.5921024084091187
    Answer: 140000
    Ground truth:  140000
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 1.3576049357652664
    Answer: 140000
    Ground truth:  140000
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 1.355319768190384
    Answer: 140000
    Ground truth:  140000
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money the company receives in two weeks, we need to calcula...
    Score: 2.6240345239639282
    Answer: 140000
    Ground truth:  140000
Method name: attention_weighted_confidence, running accuracy: 90.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 89.60000000000001
Method name: self_consistency, running accuracy: 90.93333333333334
Method name: p_true, running accuracy: 91.2
Method name: normilized_likelihood, running accuracy: 90.13333333333333
Method name: normilized_entropy, running accuracy: 89.86666666666666
Method name: topk_entropy, running accuracy: 89.86666666666666
Method name: window_entropy, running accuracy: 90.66666666666666
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▍  | 374/500 [31:42:24<7:35:03, 216.70s/it, attention_weighted_confidence_acc=90.67%, cer_entropy_weighted_mean_all_acc=90.67%, cer_prob_product_log_last_acc=89.60%, self_consistency_acc=90.93%, p_true_acc=91.20%, normilized_likelihood_acc=90.13%, normilized_entropy_acc=89.87%, topk_entropy_acc=89.87%, window_entropy_acc=90.67%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▌  | 375/500 [31:42:24<7:28:45, 215.41s/it, attention_weighted_confidence_acc=90.67%, cer_entropy_weighted_mean_all_acc=90.67%, cer_prob_product_log_last_acc=89.60%, self_consistency_acc=90.93%, p_true_acc=91.20%, normilized_likelihood_acc=90.13%, normilized_entropy_acc=89.87%, topk_entropy_acc=89.87%, window_entropy_acc=90.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 7.168105293898887
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 7.168105293898887
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 14.999297320842743
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 0.9375
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 14.16015625
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 2.3047350645065308
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 1.1448018550872803
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 1.1352570950984955
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Initially, let's assume the number o...
    Score: 7.209494769573212
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 90.69148936170212
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.69148936170212
Method name: cer_prob_product_log_last, running accuracy: 89.62765957446808
Method name: self_consistency, running accuracy: 90.95744680851064
Method name: p_true, running accuracy: 91.22340425531915
Method name: normilized_likelihood, running accuracy: 90.1595744680851
Method name: normilized_entropy, running accuracy: 89.8936170212766
Method name: topk_entropy, running accuracy: 89.8936170212766
Method name: window_entropy, running accuracy: 90.69148936170212
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▌  | 375/500 [31:46:06<7:28:45, 215.41s/it, attention_weighted_confidence_acc=90.69%, cer_entropy_weighted_mean_all_acc=90.69%, cer_prob_product_log_last_acc=89.63%, self_consistency_acc=90.96%, p_true_acc=91.22%, normilized_likelihood_acc=90.16%, normilized_entropy_acc=89.89%, topk_entropy_acc=89.89%, window_entropy_acc=90.69%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▌  | 376/500 [31:46:06<7:29:16, 217.39s/it, attention_weighted_confidence_acc=90.69%, cer_entropy_weighted_mean_all_acc=90.69%, cer_prob_product_log_last_acc=89.63%, self_consistency_acc=90.96%, p_true_acc=91.22%, normilized_likelihood_acc=90.16%, normilized_entropy_acc=89.89%, topk_entropy_acc=89.89%, window_entropy_acc=90.69%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 4.056085085533576
    Answer: 34
    Ground truth:  34
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 4.056085085533576
    Answer: 34
    Ground truth:  34
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 7.969462811946869
    Answer: 34
    Ground truth:  34
Method 4: self_consistency
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 0.5
    Answer: 34
    Ground truth:  34
Method 5: p_true
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 6.55078125
    Answer: 34
    Ground truth:  34
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 3.921114668250084
    Answer: 34
    Ground truth:  34
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 3.7410614639520645
    Answer: 34
    Ground truth:  34
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 3.2513752579689026
    Answer: 34
    Ground truth:  34
Method 9: window_entropy
  Batch 1:
    Text: To find the number of balloons Sally finally carried into the school, we will so...
    Score: 9.719032287597656
    Answer: 34
    Ground truth:  34
Method name: attention_weighted_confidence, running accuracy: 90.71618037135278
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.71618037135278
Method name: cer_prob_product_log_last, running accuracy: 89.65517241379311
Method name: self_consistency, running accuracy: 90.98143236074272
Method name: p_true, running accuracy: 91.24668435013263
Method name: normilized_likelihood, running accuracy: 90.18567639257294
Method name: normilized_entropy, running accuracy: 89.92042440318302
Method name: topk_entropy, running accuracy: 89.92042440318302
Method name: window_entropy, running accuracy: 90.71618037135278
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▌  | 376/500 [31:53:07<7:29:16, 217.39s/it, attention_weighted_confidence_acc=90.72%, cer_entropy_weighted_mean_all_acc=90.72%, cer_prob_product_log_last_acc=89.66%, self_consistency_acc=90.98%, p_true_acc=91.25%, normilized_likelihood_acc=90.19%, normilized_entropy_acc=89.92%, topk_entropy_acc=89.92%, window_entropy_acc=90.72%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▌  | 377/500 [31:53:07<9:30:26, 278.26s/it, attention_weighted_confidence_acc=90.72%, cer_entropy_weighted_mean_all_acc=90.72%, cer_prob_product_log_last_acc=89.66%, self_consistency_acc=90.98%, p_true_acc=91.25%, normilized_likelihood_acc=90.19%, normilized_entropy_acc=89.92%, topk_entropy_acc=89.92%, window_entropy_acc=90.72%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 1.4498293396170536
    Answer: 1047.2
    Ground truth:  826
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 1.4498293396170536
    Answer: 1047.2
    Ground truth:  826
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the cost to fill the pool, we need to calculate the volume of the pool f...
    Score: 0.9490065574645996
    Answer: 823
    Ground truth:  826
Method 4: self_consistency
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 0.1875
    Answer: 1047.2
    Ground truth:  826
Method 5: p_true
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 1.953125
    Answer: 1047.2
    Ground truth:  826
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 1.2651642560958862
    Answer: 1047.2
    Ground truth:  826
Method 7: normilized_entropy
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 1.3372963964939117
    Answer: 1047.2
    Ground truth:  826
Method 8: topk_entropy
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 1.185859590768814
    Answer: 1047.2
    Ground truth:  826
Method 9: window_entropy
  Batch 1:
    Text: To find the volume of the pool, we need to multiply the width, length, and depth...
    Score: 2.1745351552963257
    Answer: 1047.2
    Ground truth:  826
Method name: attention_weighted_confidence, running accuracy: 90.47619047619048
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.47619047619048
Method name: cer_prob_product_log_last, running accuracy: 89.68253968253968
Method name: self_consistency, running accuracy: 90.74074074074075
Method name: p_true, running accuracy: 91.005291005291
Method name: normilized_likelihood, running accuracy: 89.94708994708994
Method name: normilized_entropy, running accuracy: 89.68253968253968
Method name: topk_entropy, running accuracy: 89.68253968253968
Method name: window_entropy, running accuracy: 90.47619047619048
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  75%|███████▌  | 377/500 [31:59:13<9:30:26, 278.26s/it, attention_weighted_confidence_acc=90.48%, cer_entropy_weighted_mean_all_acc=90.48%, cer_prob_product_log_last_acc=89.68%, self_consistency_acc=90.74%, p_true_acc=91.01%, normilized_likelihood_acc=89.95%, normilized_entropy_acc=89.68%, topk_entropy_acc=89.68%, window_entropy_acc=90.48%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 378/500 [31:59:13<10:19:26, 304.64s/it, attention_weighted_confidence_acc=90.48%, cer_entropy_weighted_mean_all_acc=90.48%, cer_prob_product_log_last_acc=89.68%, self_consistency_acc=90.74%, p_true_acc=91.01%, normilized_likelihood_acc=89.95%, normilized_entropy_acc=89.68%, topk_entropy_acc=89.68%, window_entropy_acc=90.48%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 2.895449471850116
    Answer: 5
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 2.895449471850116
    Answer: 5
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 4.287652745842934
    Answer: 5
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 0.375
    Answer: 5
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 4.61865234375
    Answer: 5
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 8.70483085513115
    Answer: 5
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 8.404772907495499
    Answer: 5
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 6.94825679063797
    Answer: 5
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: Let's denote the amount of citrus zest as Z, the amount of fragrance as F, the a...
    Score: 6.625400245189667
    Answer: 5
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 90.23746701846966
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.23746701846966
Method name: cer_prob_product_log_last, running accuracy: 89.44591029023746
Method name: self_consistency, running accuracy: 90.50131926121372
Method name: p_true, running accuracy: 90.76517150395779
Method name: normilized_likelihood, running accuracy: 89.70976253298153
Method name: normilized_entropy, running accuracy: 89.44591029023746
Method name: topk_entropy, running accuracy: 89.44591029023746
Method name: window_entropy, running accuracy: 90.23746701846966
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 378/500 [32:08:14<10:19:26, 304.64s/it, attention_weighted_confidence_acc=90.24%, cer_entropy_weighted_mean_all_acc=90.24%, cer_prob_product_log_last_acc=89.45%, self_consistency_acc=90.50%, p_true_acc=90.77%, normilized_likelihood_acc=89.71%, normilized_entropy_acc=89.45%, topk_entropy_acc=89.45%, window_entropy_acc=90.24%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 379/500 [32:08:14<12:37:18, 375.53s/it, attention_weighted_confidence_acc=90.24%, cer_entropy_weighted_mean_all_acc=90.24%, cer_prob_product_log_last_acc=89.45%, self_consistency_acc=90.50%, p_true_acc=90.77%, normilized_likelihood_acc=89.71%, normilized_entropy_acc=89.45%, topk_entropy_acc=89.45%, window_entropy_acc=90.24%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 7.871852045011788
    Answer: 13
    Ground truth:  13
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 7.871852045011788
    Answer: 13
    Ground truth:  13
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 15.999971151351929
    Answer: 13
    Ground truth:  13
Method 4: self_consistency
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 1.0
    Answer: 13
    Ground truth:  13
Method 5: p_true
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 14.90625
    Answer: 13
    Ground truth:  13
Method 6: normilized_likelihood
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 2.838794067502022
    Answer: 13
    Ground truth:  13
Method 7: normilized_entropy
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 1.370339259505272
    Answer: 13
    Ground truth:  13
Method 8: topk_entropy
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 1.3628408908843994
    Answer: 13
    Ground truth:  13
Method 9: window_entropy
  Batch 1:
    Text: Jasmine starts with 15 stickers for participating in class. 

Next, she lost 7 s...
    Score: 3.280298173427582
    Answer: 13
    Ground truth:  13
Method name: attention_weighted_confidence, running accuracy: 90.26315789473685
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.26315789473685
Method name: cer_prob_product_log_last, running accuracy: 89.47368421052632
Method name: self_consistency, running accuracy: 90.52631578947368
Method name: p_true, running accuracy: 90.78947368421053
Method name: normilized_likelihood, running accuracy: 89.73684210526316
Method name: normilized_entropy, running accuracy: 89.47368421052632
Method name: topk_entropy, running accuracy: 89.47368421052632
Method name: window_entropy, running accuracy: 90.26315789473685
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 379/500 [32:10:53<12:37:18, 375.53s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=89.47%, self_consistency_acc=90.53%, p_true_acc=90.79%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=90.26%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 380/500 [32:10:53<10:21:04, 310.54s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=89.47%, self_consistency_acc=90.53%, p_true_acc=90.79%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=90.26%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 8.434568395563167
    Answer: 132.0
    Ground truth:  132
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 8.434568395563167
    Answer: 132.0
    Ground truth:  132
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 0.0
    Answer: 132.0
    Ground truth:  132
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 1.0
    Answer: 132.0
    Ground truth:  132
Method 5: p_true
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 12.390625
    Answer: 132.0
    Ground truth:  132
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 1.761087492108345
    Answer: 132.0
    Ground truth:  132
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 2.0791933238506317
    Answer: 132.0
    Ground truth:  132
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 2.0649396181106567
    Answer: 132.0
    Ground truth:  132
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of Charlotte's order, let's break it down step by step.

...
    Score: 3.0899943709373474
    Answer: 132.0
    Ground truth:  132
Method name: attention_weighted_confidence, running accuracy: 90.28871391076116
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.28871391076116
Method name: cer_prob_product_log_last, running accuracy: 89.501312335958
Method name: self_consistency, running accuracy: 90.5511811023622
Method name: p_true, running accuracy: 90.81364829396325
Method name: normilized_likelihood, running accuracy: 89.76377952755905
Method name: normilized_entropy, running accuracy: 89.501312335958
Method name: topk_entropy, running accuracy: 89.501312335958
Method name: window_entropy, running accuracy: 90.28871391076116
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 380/500 [32:15:37<10:21:04, 310.54s/it, attention_weighted_confidence_acc=90.29%, cer_entropy_weighted_mean_all_acc=90.29%, cer_prob_product_log_last_acc=89.50%, self_consistency_acc=90.55%, p_true_acc=90.81%, normilized_likelihood_acc=89.76%, normilized_entropy_acc=89.50%, topk_entropy_acc=89.50%, window_entropy_acc=90.29%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 381/500 [32:15:37<10:00:34, 302.81s/it, attention_weighted_confidence_acc=90.29%, cer_entropy_weighted_mean_all_acc=90.29%, cer_prob_product_log_last_acc=89.50%, self_consistency_acc=90.55%, p_true_acc=90.81%, normilized_likelihood_acc=89.76%, normilized_entropy_acc=89.50%, topk_entropy_acc=89.50%, window_entropy_acc=90.29%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 4.389390912536048
    Answer: 7
    Ground truth:  7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 4.389390912536048
    Answer: 7
    Ground truth:  7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 8.992986381053925
    Answer: 7
    Ground truth:  7
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 0.5625
    Answer: 7
    Ground truth:  7
Method 5: p_true
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 7.21875
    Answer: 7
    Ground truth:  7
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 6.075845122337341
    Answer: 7
    Ground truth:  7
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 5.78437614440918
    Answer: 7
    Ground truth:  7
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 5.0577841103076935
    Answer: 7
    Ground truth:  7
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of balls, we need to figure out how many balls each boy...
    Score: 11.449846267700195
    Answer: 7
    Ground truth:  7
Method name: attention_weighted_confidence, running accuracy: 90.31413612565446
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.31413612565446
Method name: cer_prob_product_log_last, running accuracy: 89.52879581151832
Method name: self_consistency, running accuracy: 90.57591623036649
Method name: p_true, running accuracy: 90.83769633507853
Method name: normilized_likelihood, running accuracy: 89.79057591623037
Method name: normilized_entropy, running accuracy: 89.52879581151832
Method name: topk_entropy, running accuracy: 89.52879581151832
Method name: window_entropy, running accuracy: 90.31413612565446
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▌  | 381/500 [32:21:28<10:00:34, 302.81s/it, attention_weighted_confidence_acc=90.31%, cer_entropy_weighted_mean_all_acc=90.31%, cer_prob_product_log_last_acc=89.53%, self_consistency_acc=90.58%, p_true_acc=90.84%, normilized_likelihood_acc=89.79%, normilized_entropy_acc=89.53%, topk_entropy_acc=89.53%, window_entropy_acc=90.31%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▋  | 382/500 [32:21:28<10:23:49, 317.20s/it, attention_weighted_confidence_acc=90.31%, cer_entropy_weighted_mean_all_acc=90.31%, cer_prob_product_log_last_acc=89.53%, self_consistency_acc=90.58%, p_true_acc=90.84%, normilized_likelihood_acc=89.79%, normilized_entropy_acc=89.53%, topk_entropy_acc=89.53%, window_entropy_acc=90.31%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 8.692116193252737
    Answer: 29
    Ground truth:  29
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 8.692116193252737
    Answer: 29
    Ground truth:  29
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 15.46733745932579
    Answer: 29
    Ground truth:  29
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 1.0
    Answer: 29
    Ground truth:  29
Method 5: p_true
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 15.73046875
    Answer: 29
    Ground truth:  29
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 1.4483741894364357
    Answer: 29
    Ground truth:  29
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 1.4050057977437973
    Answer: 29
    Ground truth:  29
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 1.3978222161531448
    Answer: 29
    Ground truth:  29
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount of water removed from the air, we need to calculate the...
    Score: 7.420318245887756
    Answer: 29
    Ground truth:  29
Method name: attention_weighted_confidence, running accuracy: 90.33942558746736
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.33942558746736
Method name: cer_prob_product_log_last, running accuracy: 89.55613577023499
Method name: self_consistency, running accuracy: 90.60052219321149
Method name: p_true, running accuracy: 90.86161879895562
Method name: normilized_likelihood, running accuracy: 89.81723237597912
Method name: normilized_entropy, running accuracy: 89.55613577023499
Method name: topk_entropy, running accuracy: 89.55613577023499
Method name: window_entropy, running accuracy: 90.33942558746736
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  76%|███████▋  | 382/500 [32:27:01<10:23:49, 317.20s/it, attention_weighted_confidence_acc=90.34%, cer_entropy_weighted_mean_all_acc=90.34%, cer_prob_product_log_last_acc=89.56%, self_consistency_acc=90.60%, p_true_acc=90.86%, normilized_likelihood_acc=89.82%, normilized_entropy_acc=89.56%, topk_entropy_acc=89.56%, window_entropy_acc=90.34%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 383/500 [32:27:01<10:27:40, 321.89s/it, attention_weighted_confidence_acc=90.34%, cer_entropy_weighted_mean_all_acc=90.34%, cer_prob_product_log_last_acc=89.56%, self_consistency_acc=90.60%, p_true_acc=90.86%, normilized_likelihood_acc=89.82%, normilized_entropy_acc=89.56%, topk_entropy_acc=89.56%, window_entropy_acc=90.34%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 8.450319866579465
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 8.450319866579465
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 15.999864101409912
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 15.62890625
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 1.740597665309906
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 0.9325223118066788
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 0.925842210650444
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the total time it will take Bill to dig the well, we need to calculate t...
    Score: 3.4247339963912964
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 90.36458333333334
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.36458333333334
Method name: cer_prob_product_log_last, running accuracy: 89.58333333333334
Method name: self_consistency, running accuracy: 90.625
Method name: p_true, running accuracy: 90.88541666666666
Method name: normilized_likelihood, running accuracy: 89.84375
Method name: normilized_entropy, running accuracy: 89.58333333333334
Method name: topk_entropy, running accuracy: 89.58333333333334
Method name: window_entropy, running accuracy: 90.36458333333334
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 383/500 [32:31:55<10:27:40, 321.89s/it, attention_weighted_confidence_acc=90.36%, cer_entropy_weighted_mean_all_acc=90.36%, cer_prob_product_log_last_acc=89.58%, self_consistency_acc=90.62%, p_true_acc=90.89%, normilized_likelihood_acc=89.84%, normilized_entropy_acc=89.58%, topk_entropy_acc=89.58%, window_entropy_acc=90.36%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 384/500 [32:31:55<10:05:51, 313.38s/it, attention_weighted_confidence_acc=90.36%, cer_entropy_weighted_mean_all_acc=90.36%, cer_prob_product_log_last_acc=89.58%, self_consistency_acc=90.62%, p_true_acc=90.89%, normilized_likelihood_acc=89.84%, normilized_entropy_acc=89.58%, topk_entropy_acc=89.58%, window_entropy_acc=90.36%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 8.311449721709455
    Answer: 38
    Ground truth:  38
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 8.311449721709455
    Answer: 38
    Ground truth:  38
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 15.993894457817078
    Answer: 38
    Ground truth:  38
Method 4: self_consistency
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 1.0
    Answer: 38
    Ground truth:  38
Method 5: p_true
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 14.6875
    Answer: 38
    Ground truth:  38
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 5.604983642697334
    Answer: 38
    Ground truth:  38
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 3.849761962890625
    Answer: 38
    Ground truth:  38
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 3.640484794974327
    Answer: 38
    Ground truth:  38
Method 9: window_entropy
  Batch 1:
    Text: To calculate the total cost, let's break down the expenses step by step:

1. The...
    Score: 18.320905685424805
    Answer: 38
    Ground truth:  38
Method name: attention_weighted_confidence, running accuracy: 90.38961038961038
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.38961038961038
Method name: cer_prob_product_log_last, running accuracy: 89.6103896103896
Method name: self_consistency, running accuracy: 90.64935064935065
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 89.87012987012987
Method name: normilized_entropy, running accuracy: 89.6103896103896
Method name: topk_entropy, running accuracy: 89.6103896103896
Method name: window_entropy, running accuracy: 90.38961038961038
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 384/500 [32:36:08<10:05:51, 313.38s/it, attention_weighted_confidence_acc=90.39%, cer_entropy_weighted_mean_all_acc=90.39%, cer_prob_product_log_last_acc=89.61%, self_consistency_acc=90.65%, p_true_acc=90.91%, normilized_likelihood_acc=89.87%, normilized_entropy_acc=89.61%, topk_entropy_acc=89.61%, window_entropy_acc=90.39%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 385/500 [32:36:08<9:26:02, 295.33s/it, attention_weighted_confidence_acc=90.39%, cer_entropy_weighted_mean_all_acc=90.39%, cer_prob_product_log_last_acc=89.61%, self_consistency_acc=90.65%, p_true_acc=90.91%, normilized_likelihood_acc=89.87%, normilized_entropy_acc=89.61%, topk_entropy_acc=89.61%, window_entropy_acc=90.39%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 6.598664359143284
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 6.598664359143284
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 12.985356628894806
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 0.8125
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 12.546875
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 17.616828575730324
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 17.139549374580383
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 14.417676985263824
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. The train travels 7...
    Score: 21.172384321689606
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 90.41450777202073
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.41450777202073
Method name: cer_prob_product_log_last, running accuracy: 89.63730569948186
Method name: self_consistency, running accuracy: 90.67357512953367
Method name: p_true, running accuracy: 90.93264248704664
Method name: normilized_likelihood, running accuracy: 89.89637305699482
Method name: normilized_entropy, running accuracy: 89.63730569948186
Method name: topk_entropy, running accuracy: 89.63730569948186
Method name: window_entropy, running accuracy: 90.41450777202073
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 385/500 [32:43:09<9:26:02, 295.33s/it, attention_weighted_confidence_acc=90.41%, cer_entropy_weighted_mean_all_acc=90.41%, cer_prob_product_log_last_acc=89.64%, self_consistency_acc=90.67%, p_true_acc=90.93%, normilized_likelihood_acc=89.90%, normilized_entropy_acc=89.64%, topk_entropy_acc=89.64%, window_entropy_acc=90.41%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 386/500 [32:43:09<10:33:01, 333.17s/it, attention_weighted_confidence_acc=90.41%, cer_entropy_weighted_mean_all_acc=90.41%, cer_prob_product_log_last_acc=89.64%, self_consistency_acc=90.67%, p_true_acc=90.93%, normilized_likelihood_acc=89.90%, normilized_entropy_acc=89.64%, topk_entropy_acc=89.64%, window_entropy_acc=90.41%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 1.3733904134811725
    Answer: 600
    Ground truth:  500
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 1.3733904134811725
    Answer: 600
    Ground truth:  500
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 2.999605119228363
    Answer: 600
    Ground truth:  500
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 0.1875
    Answer: 600
    Ground truth:  500
Method 5: p_true
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 2.3203125
    Answer: 600
    Ground truth:  500
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 2.2278985381126404
    Answer: 600
    Ground truth:  500
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 2.137446641921997
    Answer: 600
    Ground truth:  500
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 1.7746197879314423
    Answer: 600
    Ground truth:  500
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Carmen spent on the desk, we need to calculate the to...
    Score: 3.273207426071167
    Answer: 600
    Ground truth:  500
Method name: attention_weighted_confidence, running accuracy: 90.18087855297158
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.18087855297158
Method name: cer_prob_product_log_last, running accuracy: 89.40568475452196
Method name: self_consistency, running accuracy: 90.43927648578811
Method name: p_true, running accuracy: 90.69767441860465
Method name: normilized_likelihood, running accuracy: 89.66408268733849
Method name: normilized_entropy, running accuracy: 89.40568475452196
Method name: topk_entropy, running accuracy: 89.40568475452196
Method name: window_entropy, running accuracy: 90.18087855297158
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 386/500 [32:49:38<10:33:01, 333.17s/it, attention_weighted_confidence_acc=90.18%, cer_entropy_weighted_mean_all_acc=90.18%, cer_prob_product_log_last_acc=89.41%, self_consistency_acc=90.44%, p_true_acc=90.70%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=89.41%, topk_entropy_acc=89.41%, window_entropy_acc=90.18%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 387/500 [32:49:38<10:58:41, 349.75s/it, attention_weighted_confidence_acc=90.18%, cer_entropy_weighted_mean_all_acc=90.18%, cer_prob_product_log_last_acc=89.41%, self_consistency_acc=90.44%, p_true_acc=90.70%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=89.41%, topk_entropy_acc=89.41%, window_entropy_acc=90.18%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 7.881085715019537
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 7.881085715019537
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 14.170217871665955
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 0.9375
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 13.443359375
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 2.386475771665573
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 1.2633789479732513
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 1.218598112463951
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Crista has 20 plants in total. 
2...
    Score: 6.012145161628723
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 90.20618556701031
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.20618556701031
Method name: cer_prob_product_log_last, running accuracy: 89.43298969072166
Method name: self_consistency, running accuracy: 90.4639175257732
Method name: p_true, running accuracy: 90.72164948453609
Method name: normilized_likelihood, running accuracy: 89.69072164948454
Method name: normilized_entropy, running accuracy: 89.43298969072166
Method name: topk_entropy, running accuracy: 89.43298969072166
Method name: window_entropy, running accuracy: 90.20618556701031
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  77%|███████▋  | 387/500 [32:54:54<10:58:41, 349.75s/it, attention_weighted_confidence_acc=90.21%, cer_entropy_weighted_mean_all_acc=90.21%, cer_prob_product_log_last_acc=89.43%, self_consistency_acc=90.46%, p_true_acc=90.72%, normilized_likelihood_acc=89.69%, normilized_entropy_acc=89.43%, topk_entropy_acc=89.43%, window_entropy_acc=90.21%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 388/500 [32:54:54<10:33:58, 339.63s/it, attention_weighted_confidence_acc=90.21%, cer_entropy_weighted_mean_all_acc=90.21%, cer_prob_product_log_last_acc=89.43%, self_consistency_acc=90.46%, p_true_acc=90.72%, normilized_likelihood_acc=89.69%, normilized_entropy_acc=89.43%, topk_entropy_acc=89.43%, window_entropy_acc=90.21%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 7.644681515956494
    Answer: 18
    Ground truth:  18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 7.644681515956494
    Answer: 18
    Ground truth:  18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 15.999601125717163
    Answer: 18
    Ground truth:  18
Method 4: self_consistency
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 1.0
    Answer: 18
    Ground truth:  18
Method 5: p_true
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 15.0234375
    Answer: 18
    Ground truth:  18
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 3.275674819946289
    Answer: 18
    Ground truth:  18
Method 7: normilized_entropy
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 2.7468262761831284
    Answer: 18
    Ground truth:  18
Method 8: topk_entropy
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 2.7248382717370987
    Answer: 18
    Ground truth:  18
Method 9: window_entropy
  Batch 1:
    Text: To find the floor Bill is on, we first need to determine the equation based on t...
    Score: 11.06545740365982
    Answer: 18
    Ground truth:  18
Method name: attention_weighted_confidence, running accuracy: 90.23136246786633
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.23136246786633
Method name: cer_prob_product_log_last, running accuracy: 89.46015424164524
Method name: self_consistency, running accuracy: 90.48843187660668
Method name: p_true, running accuracy: 90.74550128534705
Method name: normilized_likelihood, running accuracy: 89.7172236503856
Method name: normilized_entropy, running accuracy: 89.46015424164524
Method name: topk_entropy, running accuracy: 89.46015424164524
Method name: window_entropy, running accuracy: 90.23136246786633
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 388/500 [32:57:41<10:33:58, 339.63s/it, attention_weighted_confidence_acc=90.23%, cer_entropy_weighted_mean_all_acc=90.23%, cer_prob_product_log_last_acc=89.46%, self_consistency_acc=90.49%, p_true_acc=90.75%, normilized_likelihood_acc=89.72%, normilized_entropy_acc=89.46%, topk_entropy_acc=89.46%, window_entropy_acc=90.23%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 389/500 [32:57:41<8:52:27, 287.82s/it, attention_weighted_confidence_acc=90.23%, cer_entropy_weighted_mean_all_acc=90.23%, cer_prob_product_log_last_acc=89.46%, self_consistency_acc=90.49%, p_true_acc=90.75%, normilized_likelihood_acc=89.72%, normilized_entropy_acc=89.46%, topk_entropy_acc=89.46%, window_entropy_acc=90.23%] Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 8.18481936782124
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 8.18481936782124
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 15.914751708507538
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 15.19140625
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 3.3270725905895233
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 2.6710689663887024
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 2.6541922837495804
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To determine the age of Brandon's iPhone, let's follow the steps:

1. Suzy's iPh...
    Score: 10.81888461112976
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 90.25641025641026
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.25641025641026
Method name: cer_prob_product_log_last, running accuracy: 89.48717948717949
Method name: self_consistency, running accuracy: 90.51282051282051
Method name: p_true, running accuracy: 90.76923076923077
Method name: normilized_likelihood, running accuracy: 89.74358974358975
Method name: normilized_entropy, running accuracy: 89.48717948717949
Method name: topk_entropy, running accuracy: 89.48717948717949
Method name: window_entropy, running accuracy: 90.25641025641026
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 389/500 [33:00:49<8:52:27, 287.82s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=89.49%, self_consistency_acc=90.51%, p_true_acc=90.77%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=89.49%, topk_entropy_acc=89.49%, window_entropy_acc=90.26%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 390/500 [33:00:49<7:52:58, 257.99s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=89.49%, self_consistency_acc=90.51%, p_true_acc=90.77%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=89.49%, topk_entropy_acc=89.49%, window_entropy_acc=90.26%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 7.574117417215543
    Answer: 600
    Ground truth:  1000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 7.574117417215543
    Answer: 600
    Ground truth:  1000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 15.950316429138184
    Answer: 600
    Ground truth:  1000
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 1.0
    Answer: 600
    Ground truth:  1000
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 13.66015625
    Answer: 600
    Ground truth:  1000
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 3.0621956437826157
    Answer: 600
    Ground truth:  1000
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 3.5958031862974167
    Answer: 600
    Ground truth:  1000
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 3.316364973783493
    Answer: 600
    Ground truth:  1000
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. The total cost for supplies is $400. 
2. T...
    Score: 17.24048712849617
    Answer: 600
    Ground truth:  1000
Method name: attention_weighted_confidence, running accuracy: 90.02557544757033
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.02557544757033
Method name: cer_prob_product_log_last, running accuracy: 89.25831202046037
Method name: self_consistency, running accuracy: 90.28132992327366
Method name: p_true, running accuracy: 90.53708439897699
Method name: normilized_likelihood, running accuracy: 89.51406649616368
Method name: normilized_entropy, running accuracy: 89.25831202046037
Method name: topk_entropy, running accuracy: 89.25831202046037
Method name: window_entropy, running accuracy: 90.02557544757033
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 390/500 [33:03:56<7:52:58, 257.99s/it, attention_weighted_confidence_acc=90.03%, cer_entropy_weighted_mean_all_acc=90.03%, cer_prob_product_log_last_acc=89.26%, self_consistency_acc=90.28%, p_true_acc=90.54%, normilized_likelihood_acc=89.51%, normilized_entropy_acc=89.26%, topk_entropy_acc=89.26%, window_entropy_acc=90.03%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 391/500 [33:03:56<7:09:49, 236.60s/it, attention_weighted_confidence_acc=90.03%, cer_entropy_weighted_mean_all_acc=90.03%, cer_prob_product_log_last_acc=89.26%, self_consistency_acc=90.28%, p_true_acc=90.54%, normilized_likelihood_acc=89.51%, normilized_entropy_acc=89.26%, topk_entropy_acc=89.26%, window_entropy_acc=90.03%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 4.810581512572883
    Answer: 1600
    Ground truth:  1.600
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 4.810581512572883
    Answer: 1600
    Ground truth:  1.600
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 4.961274680534197
    Answer: 1600
    Ground truth:  1.600
Method 4: self_consistency
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 0.625
    Answer: 1600
    Ground truth:  1.600
Method 5: p_true
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 8.5546875
    Answer: 1600
    Ground truth:  1.600
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 4.487149149179459
    Answer: 1600
    Ground truth:  1.600
Method 7: normilized_entropy
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 4.696764051914215
    Answer: 1600
    Ground truth:  1.600
Method 8: topk_entropy
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 4.070739299058914
    Answer: 1600
    Ground truth:  1.600
Method 9: window_entropy
  Batch 1:
    Text: To find the most popsicle sticks Frederick can make, we need to determine which ...
    Score: 10.177060902118683
    Answer: 1600
    Ground truth:  1.600
Method name: attention_weighted_confidence, running accuracy: 89.79591836734694
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.79591836734694
Method name: cer_prob_product_log_last, running accuracy: 89.03061224489795
Method name: self_consistency, running accuracy: 90.05102040816327
Method name: p_true, running accuracy: 90.3061224489796
Method name: normilized_likelihood, running accuracy: 89.28571428571429
Method name: normilized_entropy, running accuracy: 89.03061224489795
Method name: topk_entropy, running accuracy: 89.03061224489795
Method name: window_entropy, running accuracy: 89.79591836734694
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 391/500 [33:11:39<7:09:49, 236.60s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=89.03%, self_consistency_acc=90.05%, p_true_acc=90.31%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=89.03%, topk_entropy_acc=89.03%, window_entropy_acc=89.80%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 392/500 [33:11:39<9:08:25, 304.68s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=89.03%, self_consistency_acc=90.05%, p_true_acc=90.31%, normilized_likelihood_acc=89.29%, normilized_entropy_acc=89.03%, topk_entropy_acc=89.03%, window_entropy_acc=89.80%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 8.266172625930311
    Answer: 10800
    Ground truth:  10.800
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 8.266172625930311
    Answer: 10800
    Ground truth:  10.800
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 5.345928955976575
    Answer: 10800
    Ground truth:  10.800
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 1.0
    Answer: 10800
    Ground truth:  10.800
Method 5: p_true
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 14.53125
    Answer: 10800
    Ground truth:  10.800
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 1.8208159059286118
    Answer: 10800
    Ground truth:  10.800
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 1.869510680437088
    Answer: 10800
    Ground truth:  10.800
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 1.7969807237386703
    Answer: 10800
    Ground truth:  10.800
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of everything, let's break it down step by step.

1. The ...
    Score: 14.265847891569138
    Answer: 10800
    Ground truth:  10.800
Method name: attention_weighted_confidence, running accuracy: 89.56743002544529
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.56743002544529
Method name: cer_prob_product_log_last, running accuracy: 88.80407124681933
Method name: self_consistency, running accuracy: 89.82188295165395
Method name: p_true, running accuracy: 90.07633587786259
Method name: normilized_likelihood, running accuracy: 89.05852417302799
Method name: normilized_entropy, running accuracy: 88.80407124681933
Method name: topk_entropy, running accuracy: 88.80407124681933
Method name: window_entropy, running accuracy: 89.56743002544529
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  78%|███████▊  | 392/500 [33:17:22<9:08:25, 304.68s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.80%, self_consistency_acc=89.82%, p_true_acc=90.08%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.80%, topk_entropy_acc=88.80%, window_entropy_acc=89.57%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▊  | 393/500 [33:17:22<9:23:32, 316.00s/it, attention_weighted_confidence_acc=89.57%, cer_entropy_weighted_mean_all_acc=89.57%, cer_prob_product_log_last_acc=88.80%, self_consistency_acc=89.82%, p_true_acc=90.08%, normilized_likelihood_acc=89.06%, normilized_entropy_acc=88.80%, topk_entropy_acc=88.80%, window_entropy_acc=89.57%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 7.887174274148508
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 7.887174274148508
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 15.787790834903717
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 1.0
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 15.390625
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 2.5382441580295563
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 1.8188455998897552
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 1.7997462153434753
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To find the number of years until Adam is tall enough to ride the roller coaster...
    Score: 5.256369769573212
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.59390862944161
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.59390862944161
Method name: cer_prob_product_log_last, running accuracy: 88.83248730964468
Method name: self_consistency, running accuracy: 89.84771573604061
Method name: p_true, running accuracy: 90.1015228426396
Method name: normilized_likelihood, running accuracy: 89.08629441624365
Method name: normilized_entropy, running accuracy: 88.83248730964468
Method name: topk_entropy, running accuracy: 88.83248730964468
Method name: window_entropy, running accuracy: 89.59390862944161
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▊  | 393/500 [33:21:23<9:23:32, 316.00s/it, attention_weighted_confidence_acc=89.59%, cer_entropy_weighted_mean_all_acc=89.59%, cer_prob_product_log_last_acc=88.83%, self_consistency_acc=89.85%, p_true_acc=90.10%, normilized_likelihood_acc=89.09%, normilized_entropy_acc=88.83%, topk_entropy_acc=88.83%, window_entropy_acc=89.59%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 394/500 [33:21:23<8:38:54, 293.72s/it, attention_weighted_confidence_acc=89.59%, cer_entropy_weighted_mean_all_acc=89.59%, cer_prob_product_log_last_acc=88.83%, self_consistency_acc=89.85%, p_true_acc=90.10%, normilized_likelihood_acc=89.09%, normilized_entropy_acc=88.83%, topk_entropy_acc=88.83%, window_entropy_acc=89.59%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 8.837981009348942
    Answer: 27
    Ground truth:  27
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 8.837981009348942
    Answer: 27
    Ground truth:  27
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 15.689824044704437
    Answer: 27
    Ground truth:  27
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 1.0
    Answer: 27
    Ground truth:  27
Method 5: p_true
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 14.5
    Answer: 27
    Ground truth:  27
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 2.5279250741004944
    Answer: 27
    Ground truth:  27
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 2.149846374988556
    Answer: 27
    Ground truth:  27
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 2.136902466416359
    Answer: 27
    Ground truth:  27
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the information to solve the problem step by step.

1. Cindy ha...
    Score: 9.51594653725624
    Answer: 27
    Ground truth:  27
Method name: attention_weighted_confidence, running accuracy: 89.62025316455696
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.62025316455696
Method name: cer_prob_product_log_last, running accuracy: 88.86075949367088
Method name: self_consistency, running accuracy: 89.87341772151899
Method name: p_true, running accuracy: 90.12658227848101
Method name: normilized_likelihood, running accuracy: 89.1139240506329
Method name: normilized_entropy, running accuracy: 88.86075949367088
Method name: topk_entropy, running accuracy: 88.86075949367088
Method name: window_entropy, running accuracy: 89.62025316455696
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 394/500 [33:25:42<8:38:54, 293.72s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=88.86%, self_consistency_acc=89.87%, p_true_acc=90.13%, normilized_likelihood_acc=89.11%, normilized_entropy_acc=88.86%, topk_entropy_acc=88.86%, window_entropy_acc=89.62%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 395/500 [33:25:42<8:15:36, 283.20s/it, attention_weighted_confidence_acc=89.62%, cer_entropy_weighted_mean_all_acc=89.62%, cer_prob_product_log_last_acc=88.86%, self_consistency_acc=89.87%, p_true_acc=90.13%, normilized_likelihood_acc=89.11%, normilized_entropy_acc=88.86%, topk_entropy_acc=88.86%, window_entropy_acc=89.62%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 6.810744480520755
    Answer: 31
    Ground truth:  31
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 6.810744480520755
    Answer: 31
    Ground truth:  31
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 11.3373943567276
    Answer: 31
    Ground truth:  31
Method 4: self_consistency
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 0.75
    Answer: 31
    Ground truth:  31
Method 5: p_true
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 7.68359375
    Answer: 31
    Ground truth:  31
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 0.657679907977581
    Answer: 31
    Ground truth:  31
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 0.42588870972394943
    Answer: 31
    Ground truth:  31
Method 8: topk_entropy
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 0.4252610579133034
    Answer: 31
    Ground truth:  31
Method 9: window_entropy
  Batch 1:
    Text: To find the total dollar amount in the jar, we need to calculate the value of ea...
    Score: 1.6076384484767914
    Answer: 31
    Ground truth:  31
Method name: attention_weighted_confidence, running accuracy: 89.64646464646465
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.64646464646465
Method name: cer_prob_product_log_last, running accuracy: 88.88888888888889
Method name: self_consistency, running accuracy: 89.8989898989899
Method name: p_true, running accuracy: 90.15151515151516
Method name: normilized_likelihood, running accuracy: 89.14141414141415
Method name: normilized_entropy, running accuracy: 88.88888888888889
Method name: topk_entropy, running accuracy: 88.88888888888889
Method name: window_entropy, running accuracy: 89.64646464646465
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 395/500 [33:31:06<8:15:36, 283.20s/it, attention_weighted_confidence_acc=89.65%, cer_entropy_weighted_mean_all_acc=89.65%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=89.90%, p_true_acc=90.15%, normilized_likelihood_acc=89.14%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=89.65%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 396/500 [33:31:06<8:32:03, 295.41s/it, attention_weighted_confidence_acc=89.65%, cer_entropy_weighted_mean_all_acc=89.65%, cer_prob_product_log_last_acc=88.89%, self_consistency_acc=89.90%, p_true_acc=90.15%, normilized_likelihood_acc=89.14%, normilized_entropy_acc=88.89%, topk_entropy_acc=88.89%, window_entropy_acc=89.65%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 7.450848192140076
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 7.450848192140076
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 13.998103260993958
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 0.875
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 13.015625
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 2.958649292588234
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 2.3165528923273087
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 2.270030051469803
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: Let's denote the length of the yellow rope as 'y', the blue rope as 'b', and the...
    Score: 12.400098770856857
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 89.67254408060454
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.67254408060454
Method name: cer_prob_product_log_last, running accuracy: 88.9168765743073
Method name: self_consistency, running accuracy: 89.92443324937027
Method name: p_true, running accuracy: 90.17632241813602
Method name: normilized_likelihood, running accuracy: 89.16876574307305
Method name: normilized_entropy, running accuracy: 88.9168765743073
Method name: topk_entropy, running accuracy: 88.9168765743073
Method name: window_entropy, running accuracy: 89.67254408060454
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 396/500 [33:38:08<8:32:03, 295.41s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.92%, self_consistency_acc=89.92%, p_true_acc=90.18%, normilized_likelihood_acc=89.17%, normilized_entropy_acc=88.92%, topk_entropy_acc=88.92%, window_entropy_acc=89.67%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 397/500 [33:38:08<9:32:11, 333.32s/it, attention_weighted_confidence_acc=89.67%, cer_entropy_weighted_mean_all_acc=89.67%, cer_prob_product_log_last_acc=88.92%, self_consistency_acc=89.92%, p_true_acc=90.18%, normilized_likelihood_acc=89.17%, normilized_entropy_acc=88.92%, topk_entropy_acc=88.92%, window_entropy_acc=89.67%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 4.011918063014035
    Answer: 520
    Ground truth:  520
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 4.011918063014035
    Answer: 520
    Ground truth:  520
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 7.999486804008484
    Answer: 520
    Ground truth:  520
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 0.5
    Answer: 520
    Ground truth:  520
Method 5: p_true
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 6.15234375
    Answer: 520
    Ground truth:  520
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Bud spends on making macaroni and cheese in one year,...
    Score: 2.0479236990213394
    Answer: 520.0
    Ground truth:  520
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 2.165183112025261
    Answer: 520
    Ground truth:  520
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 2.060159519314766
    Answer: 520
    Ground truth:  520
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Bud spends on making macaroni and cheese in a year, we need...
    Score: 9.475804090499878
    Answer: 520
    Ground truth:  520
Method name: attention_weighted_confidence, running accuracy: 89.69849246231156
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.69849246231156
Method name: cer_prob_product_log_last, running accuracy: 88.94472361809045
Method name: self_consistency, running accuracy: 89.9497487437186
Method name: p_true, running accuracy: 90.20100502512562
Method name: normilized_likelihood, running accuracy: 89.19597989949749
Method name: normilized_entropy, running accuracy: 88.94472361809045
Method name: topk_entropy, running accuracy: 88.94472361809045
Method name: window_entropy, running accuracy: 89.69849246231156
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  79%|███████▉  | 397/500 [33:43:03<9:32:11, 333.32s/it, attention_weighted_confidence_acc=89.70%, cer_entropy_weighted_mean_all_acc=89.70%, cer_prob_product_log_last_acc=88.94%, self_consistency_acc=89.95%, p_true_acc=90.20%, normilized_likelihood_acc=89.20%, normilized_entropy_acc=88.94%, topk_entropy_acc=88.94%, window_entropy_acc=89.70%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|███████▉  | 398/500 [33:43:03<9:07:02, 321.79s/it, attention_weighted_confidence_acc=89.70%, cer_entropy_weighted_mean_all_acc=89.70%, cer_prob_product_log_last_acc=88.94%, self_consistency_acc=89.95%, p_true_acc=90.20%, normilized_likelihood_acc=89.20%, normilized_entropy_acc=88.94%, topk_entropy_acc=88.94%, window_entropy_acc=89.70%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 8.309240192965808
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 8.309240192965808
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 15.444012820720673
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 1.0
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 15.71484375
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 2.164265662431717
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 1.2132486253976822
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 1.2083530128002167
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To find the total time, we need to calculate the time it takes to dig the small ...
    Score: 4.472345679998398
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 89.72431077694235
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.72431077694235
Method name: cer_prob_product_log_last, running accuracy: 88.97243107769424
Method name: self_consistency, running accuracy: 89.97493734335839
Method name: p_true, running accuracy: 90.22556390977444
Method name: normilized_likelihood, running accuracy: 89.22305764411027
Method name: normilized_entropy, running accuracy: 88.97243107769424
Method name: topk_entropy, running accuracy: 88.97243107769424
Method name: window_entropy, running accuracy: 89.72431077694235
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|███████▉  | 398/500 [33:47:46<9:07:02, 321.79s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=89.97%, p_true_acc=90.23%, normilized_likelihood_acc=89.22%, normilized_entropy_acc=88.97%, topk_entropy_acc=88.97%, window_entropy_acc=89.72%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|███████▉  | 399/500 [33:47:46<8:42:15, 310.25s/it, attention_weighted_confidence_acc=89.72%, cer_entropy_weighted_mean_all_acc=89.72%, cer_prob_product_log_last_acc=88.97%, self_consistency_acc=89.97%, p_true_acc=90.23%, normilized_likelihood_acc=89.22%, normilized_entropy_acc=88.97%, topk_entropy_acc=88.97%, window_entropy_acc=89.72%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 8.49799638441125
    Answer: 1375
    Ground truth:  1375
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 8.49799638441125
    Answer: 1375
    Ground truth:  1375
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 7.660370026547916
    Answer: 1375
    Ground truth:  1375
Method 4: self_consistency
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 1.0
    Answer: 1375
    Ground truth:  1375
Method 5: p_true
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 13.9140625
    Answer: 1375
    Ground truth:  1375
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 1.4210715666413307
    Answer: 1375
    Ground truth:  1375
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 0.897785909473896
    Answer: 1375
    Ground truth:  1375
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 0.8967047408223152
    Answer: 1375
    Ground truth:  1375
Method 9: window_entropy
  Batch 1:
    Text: To find out how much the real estate agent spent on buying all the ads, we need ...
    Score: 3.5270808339118958
    Answer: 1375
    Ground truth:  1375
Method name: attention_weighted_confidence, running accuracy: 89.75
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.75
Method name: cer_prob_product_log_last, running accuracy: 89.0
Method name: self_consistency, running accuracy: 90.0
Method name: p_true, running accuracy: 90.25
Method name: normilized_likelihood, running accuracy: 89.25
Method name: normilized_entropy, running accuracy: 89.0
Method name: topk_entropy, running accuracy: 89.0
Method name: window_entropy, running accuracy: 89.75
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|███████▉  | 399/500 [33:51:49<8:42:15, 310.25s/it, attention_weighted_confidence_acc=89.75%, cer_entropy_weighted_mean_all_acc=89.75%, cer_prob_product_log_last_acc=89.00%, self_consistency_acc=90.00%, p_true_acc=90.25%, normilized_likelihood_acc=89.25%, normilized_entropy_acc=89.00%, topk_entropy_acc=89.00%, window_entropy_acc=89.75%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|████████  | 400/500 [33:51:49<8:03:21, 290.01s/it, attention_weighted_confidence_acc=89.75%, cer_entropy_weighted_mean_all_acc=89.75%, cer_prob_product_log_last_acc=89.00%, self_consistency_acc=90.00%, p_true_acc=90.25%, normilized_likelihood_acc=89.25%, normilized_entropy_acc=89.00%, topk_entropy_acc=89.00%, window_entropy_acc=89.75%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 7.3314063917650385
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 7.3314063917650385
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 15.98682987689972
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 1.0
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 13.41015625
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 7.226075455546379
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 7.518934488296509
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 6.895137369632721
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find out how long it will take to fully charge the phone, we need to calculat...
    Score: 19.232486486434937
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 89.77556109725685
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.77556109725685
Method name: cer_prob_product_log_last, running accuracy: 89.02743142144638
Method name: self_consistency, running accuracy: 90.02493765586036
Method name: p_true, running accuracy: 90.27431421446383
Method name: normilized_likelihood, running accuracy: 89.27680798004988
Method name: normilized_entropy, running accuracy: 89.02743142144638
Method name: topk_entropy, running accuracy: 89.02743142144638
Method name: window_entropy, running accuracy: 89.77556109725685
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|████████  | 400/500 [33:55:57<8:03:21, 290.01s/it, attention_weighted_confidence_acc=89.78%, cer_entropy_weighted_mean_all_acc=89.78%, cer_prob_product_log_last_acc=89.03%, self_consistency_acc=90.02%, p_true_acc=90.27%, normilized_likelihood_acc=89.28%, normilized_entropy_acc=89.03%, topk_entropy_acc=89.03%, window_entropy_acc=89.78%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|████████  | 401/500 [33:55:57<7:38:03, 277.61s/it, attention_weighted_confidence_acc=89.78%, cer_entropy_weighted_mean_all_acc=89.78%, cer_prob_product_log_last_acc=89.03%, self_consistency_acc=90.02%, p_true_acc=90.27%, normilized_likelihood_acc=89.28%, normilized_entropy_acc=89.03%, topk_entropy_acc=89.03%, window_entropy_acc=89.78%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 8.612489833435534
    Answer: 1200
    Ground truth:  1200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 8.612489833435534
    Answer: 1200
    Ground truth:  1200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 7.721436218206101
    Answer: 1200
    Ground truth:  1200
Method 4: self_consistency
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 1.0
    Answer: 1200
    Ground truth:  1200
Method 5: p_true
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 13.1953125
    Answer: 1200
    Ground truth:  1200
Method 6: normilized_likelihood
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 1.0831152871251106
    Answer: 1200
    Ground truth:  1200
Method 7: normilized_entropy
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 0.936652384698391
    Answer: 1200
    Ground truth:  1200
Method 8: topk_entropy
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 0.9335026443004608
    Answer: 1200
    Ground truth:  1200
Method 9: window_entropy
  Batch 1:
    Text: In each room, there are:
- 2 sheets
- 1 comforter
- Twice as many pillowcases as...
    Score: 3.7135944068431854
    Answer: 1200
    Ground truth:  1200
Method name: attention_weighted_confidence, running accuracy: 89.80099502487562
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.80099502487562
Method name: cer_prob_product_log_last, running accuracy: 89.05472636815921
Method name: self_consistency, running accuracy: 90.04975124378109
Method name: p_true, running accuracy: 90.29850746268657
Method name: normilized_likelihood, running accuracy: 89.30348258706468
Method name: normilized_entropy, running accuracy: 89.05472636815921
Method name: topk_entropy, running accuracy: 89.05472636815921
Method name: window_entropy, running accuracy: 89.80099502487562
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|████████  | 401/500 [34:00:53<7:38:03, 277.61s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.05%, p_true_acc=90.30%, normilized_likelihood_acc=89.30%, normilized_entropy_acc=89.05%, topk_entropy_acc=89.05%, window_entropy_acc=89.80%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|████████  | 402/500 [34:00:53<7:42:08, 282.95s/it, attention_weighted_confidence_acc=89.80%, cer_entropy_weighted_mean_all_acc=89.80%, cer_prob_product_log_last_acc=89.05%, self_consistency_acc=90.05%, p_true_acc=90.30%, normilized_likelihood_acc=89.30%, normilized_entropy_acc=89.05%, topk_entropy_acc=89.05%, window_entropy_acc=89.80%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 8.391999349064454
    Answer: 88
    Ground truth:  88
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 8.391999349064454
    Answer: 88
    Ground truth:  88
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 15.999566614627838
    Answer: 88
    Ground truth:  88
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 1.0
    Answer: 88
    Ground truth:  88
Method 5: p_true
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 12.0703125
    Answer: 88
    Ground truth:  88
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 1.3976589143276215
    Answer: 88
    Ground truth:  88
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 0.7672096937894821
    Answer: 88
    Ground truth:  88
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 0.7636753022670746
    Answer: 88
    Ground truth:  88
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of messages James delivers, we need to add the messages...
    Score: 2.247925251722336
    Answer: 88
    Ground truth:  88
Method name: attention_weighted_confidence, running accuracy: 89.82630272952854
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.82630272952854
Method name: cer_prob_product_log_last, running accuracy: 89.0818858560794
Method name: self_consistency, running accuracy: 90.07444168734492
Method name: p_true, running accuracy: 90.32258064516128
Method name: normilized_likelihood, running accuracy: 89.33002481389578
Method name: normilized_entropy, running accuracy: 89.0818858560794
Method name: topk_entropy, running accuracy: 89.0818858560794
Method name: window_entropy, running accuracy: 89.82630272952854
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  80%|████████  | 402/500 [34:04:32<7:42:08, 282.95s/it, attention_weighted_confidence_acc=89.83%, cer_entropy_weighted_mean_all_acc=89.83%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=90.07%, p_true_acc=90.32%, normilized_likelihood_acc=89.33%, normilized_entropy_acc=89.08%, topk_entropy_acc=89.08%, window_entropy_acc=89.83%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 403/500 [34:04:32<7:06:39, 263.91s/it, attention_weighted_confidence_acc=89.83%, cer_entropy_weighted_mean_all_acc=89.83%, cer_prob_product_log_last_acc=89.08%, self_consistency_acc=90.07%, p_true_acc=90.32%, normilized_likelihood_acc=89.33%, normilized_entropy_acc=89.08%, topk_entropy_acc=89.08%, window_entropy_acc=89.83%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 7.862451298927525
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 7.862451298927525
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 15.999969005584717
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 14.201171875
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 10.003101974725723
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 8.266415253281593
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 7.158535659313202
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find out how many plant pots April has left over, we first need to determine ...
    Score: 21.534599751234055
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 89.85148514851485
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.85148514851485
Method name: cer_prob_product_log_last, running accuracy: 89.10891089108911
Method name: self_consistency, running accuracy: 90.0990099009901
Method name: p_true, running accuracy: 90.34653465346535
Method name: normilized_likelihood, running accuracy: 89.35643564356435
Method name: normilized_entropy, running accuracy: 89.10891089108911
Method name: topk_entropy, running accuracy: 89.10891089108911
Method name: window_entropy, running accuracy: 89.85148514851485
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 403/500 [34:08:14<7:06:39, 263.91s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=89.11%, self_consistency_acc=90.10%, p_true_acc=90.35%, normilized_likelihood_acc=89.36%, normilized_entropy_acc=89.11%, topk_entropy_acc=89.11%, window_entropy_acc=89.85%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 404/500 [34:08:14<6:41:59, 251.25s/it, attention_weighted_confidence_acc=89.85%, cer_entropy_weighted_mean_all_acc=89.85%, cer_prob_product_log_last_acc=89.11%, self_consistency_acc=90.10%, p_true_acc=90.35%, normilized_likelihood_acc=89.36%, normilized_entropy_acc=89.11%, topk_entropy_acc=89.11%, window_entropy_acc=89.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 8.440792970414405
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 8.440792970414405
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 15.999995946884155
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 1.0
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 14.94140625
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 3.599089279770851
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 2.022850438952446
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 2.0130922347307205
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Mike will have left over, we need to calculate the di...
    Score: 8.022385895252228
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 89.87654320987654
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.87654320987654
Method name: cer_prob_product_log_last, running accuracy: 89.13580246913581
Method name: self_consistency, running accuracy: 90.12345679012346
Method name: p_true, running accuracy: 90.37037037037037
Method name: normilized_likelihood, running accuracy: 89.38271604938272
Method name: normilized_entropy, running accuracy: 89.13580246913581
Method name: topk_entropy, running accuracy: 89.13580246913581
Method name: window_entropy, running accuracy: 89.87654320987654
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 404/500 [34:12:02<6:41:59, 251.25s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=89.14%, self_consistency_acc=90.12%, p_true_acc=90.37%, normilized_likelihood_acc=89.38%, normilized_entropy_acc=89.14%, topk_entropy_acc=89.14%, window_entropy_acc=89.88%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 405/500 [34:12:02<6:26:38, 244.19s/it, attention_weighted_confidence_acc=89.88%, cer_entropy_weighted_mean_all_acc=89.88%, cer_prob_product_log_last_acc=89.14%, self_consistency_acc=90.12%, p_true_acc=90.37%, normilized_likelihood_acc=89.38%, normilized_entropy_acc=89.14%, topk_entropy_acc=89.14%, window_entropy_acc=89.88%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 7.906872258892299
    Answer: 85
    Ground truth:  85
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 7.906872258892299
    Answer: 85
    Ground truth:  85
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 15.999911665916443
    Answer: 85
    Ground truth:  85
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 1.0
    Answer: 85
    Ground truth:  85
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 12.28515625
    Answer: 85
    Ground truth:  85
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 3.04230397939682
    Answer: 85
    Ground truth:  85
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 1.2219747453927994
    Answer: 85
    Ground truth:  85
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 1.2205833047628403
    Answer: 85
    Ground truth:  85
Method 9: window_entropy
  Batch 1:
    Text: Let's break this down step by step.

1. Jayden initially had $70.
2. Ava gave hi...
    Score: 3.199344366788864
    Answer: 85
    Ground truth:  85
Method name: attention_weighted_confidence, running accuracy: 89.90147783251231
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.90147783251231
Method name: cer_prob_product_log_last, running accuracy: 89.16256157635468
Method name: self_consistency, running accuracy: 90.14778325123153
Method name: p_true, running accuracy: 90.39408866995073
Method name: normilized_likelihood, running accuracy: 89.4088669950739
Method name: normilized_entropy, running accuracy: 89.16256157635468
Method name: topk_entropy, running accuracy: 89.16256157635468
Method name: window_entropy, running accuracy: 89.90147783251231
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 405/500 [34:15:13<6:26:38, 244.19s/it, attention_weighted_confidence_acc=89.90%, cer_entropy_weighted_mean_all_acc=89.90%, cer_prob_product_log_last_acc=89.16%, self_consistency_acc=90.15%, p_true_acc=90.39%, normilized_likelihood_acc=89.41%, normilized_entropy_acc=89.16%, topk_entropy_acc=89.16%, window_entropy_acc=89.90%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 406/500 [34:15:13<5:57:54, 228.45s/it, attention_weighted_confidence_acc=89.90%, cer_entropy_weighted_mean_all_acc=89.90%, cer_prob_product_log_last_acc=89.16%, self_consistency_acc=90.15%, p_true_acc=90.39%, normilized_likelihood_acc=89.41%, normilized_entropy_acc=89.16%, topk_entropy_acc=89.16%, window_entropy_acc=89.90%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 8.208856244948137
    Answer: 96
    Ground truth:  96
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 8.208856244948137
    Answer: 96
    Ground truth:  96
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 15.99885892868042
    Answer: 96
    Ground truth:  96
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 1.0
    Answer: 96
    Ground truth:  96
Method 5: p_true
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 15.2265625
    Answer: 96
    Ground truth:  96
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 2.543168857693672
    Answer: 96
    Ground truth:  96
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 2.0558785498142242
    Answer: 96
    Ground truth:  96
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 2.0387299954891205
    Answer: 96
    Ground truth:  96
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount John makes, let's break down the problem into steps.

1...
    Score: 4.942957162857056
    Answer: 96
    Ground truth:  96
Method name: attention_weighted_confidence, running accuracy: 89.92628992628993
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.92628992628993
Method name: cer_prob_product_log_last, running accuracy: 89.1891891891892
Method name: self_consistency, running accuracy: 90.17199017199017
Method name: p_true, running accuracy: 90.41769041769042
Method name: normilized_likelihood, running accuracy: 89.43488943488943
Method name: normilized_entropy, running accuracy: 89.1891891891892
Method name: topk_entropy, running accuracy: 89.1891891891892
Method name: window_entropy, running accuracy: 89.92628992628993
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████  | 406/500 [34:19:42<5:57:54, 228.45s/it, attention_weighted_confidence_acc=89.93%, cer_entropy_weighted_mean_all_acc=89.93%, cer_prob_product_log_last_acc=89.19%, self_consistency_acc=90.17%, p_true_acc=90.42%, normilized_likelihood_acc=89.43%, normilized_entropy_acc=89.19%, topk_entropy_acc=89.19%, window_entropy_acc=89.93%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████▏ | 407/500 [34:19:42<6:12:43, 240.46s/it, attention_weighted_confidence_acc=89.93%, cer_entropy_weighted_mean_all_acc=89.93%, cer_prob_product_log_last_acc=89.19%, self_consistency_acc=90.17%, p_true_acc=90.42%, normilized_likelihood_acc=89.43%, normilized_entropy_acc=89.19%, topk_entropy_acc=89.19%, window_entropy_acc=89.93%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 7.917526283487932
    Answer: 42
    Ground truth:  42
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 7.917526283487932
    Answer: 42
    Ground truth:  42
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 14.99977719783783
    Answer: 42
    Ground truth:  42
Method 4: self_consistency
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 0.9375
    Answer: 42
    Ground truth:  42
Method 5: p_true
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 12.71484375
    Answer: 42
    Ground truth:  42
Method 6: normilized_likelihood
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 1.3091895133256912
    Answer: 42
    Ground truth:  42
Method 7: normilized_entropy
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 1.308387964963913
    Answer: 42
    Ground truth:  42
Method 8: topk_entropy
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 1.2669810205698013
    Answer: 42
    Ground truth:  42
Method 9: window_entropy
  Batch 1:
    Text: First, let's find out how much money Patrick earned in the first four hours. 

I...
    Score: 7.884873867034912
    Answer: 42
    Ground truth:  42
Method name: attention_weighted_confidence, running accuracy: 89.95098039215686
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.95098039215686
Method name: cer_prob_product_log_last, running accuracy: 89.2156862745098
Method name: self_consistency, running accuracy: 90.19607843137256
Method name: p_true, running accuracy: 90.44117647058823
Method name: normilized_likelihood, running accuracy: 89.4607843137255
Method name: normilized_entropy, running accuracy: 89.2156862745098
Method name: topk_entropy, running accuracy: 89.2156862745098
Method name: window_entropy, running accuracy: 89.95098039215686
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  81%|████████▏ | 407/500 [34:25:03<6:12:43, 240.46s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=89.22%, self_consistency_acc=90.20%, p_true_acc=90.44%, normilized_likelihood_acc=89.46%, normilized_entropy_acc=89.22%, topk_entropy_acc=89.22%, window_entropy_acc=89.95%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 408/500 [34:25:03<6:45:57, 264.75s/it, attention_weighted_confidence_acc=89.95%, cer_entropy_weighted_mean_all_acc=89.95%, cer_prob_product_log_last_acc=89.22%, self_consistency_acc=90.20%, p_true_acc=90.44%, normilized_likelihood_acc=89.46%, normilized_entropy_acc=89.22%, topk_entropy_acc=89.22%, window_entropy_acc=89.95%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 8.079287644884483
    Answer: 50
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 8.079287644884483
    Answer: 50
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 15.994862020015717
    Answer: 50
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 1.0
    Answer: 50
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 13.015625
    Answer: 50
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 2.7445848286151886
    Answer: 50
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 1.846543699502945
    Answer: 50
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 1.787670612335205
    Answer: 50
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

Step 1: First, we need to find ...
    Score: 9.65589290857315
    Answer: 50
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 89.97555012224939
Method name: cer_entropy_weighted_mean_all, running accuracy: 89.97555012224939
Method name: cer_prob_product_log_last, running accuracy: 89.24205378973105
Method name: self_consistency, running accuracy: 90.2200488997555
Method name: p_true, running accuracy: 90.4645476772616
Method name: normilized_likelihood, running accuracy: 89.48655256723717
Method name: normilized_entropy, running accuracy: 89.24205378973105
Method name: topk_entropy, running accuracy: 89.24205378973105
Method name: window_entropy, running accuracy: 89.97555012224939
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 408/500 [34:29:30<6:45:57, 264.75s/it, attention_weighted_confidence_acc=89.98%, cer_entropy_weighted_mean_all_acc=89.98%, cer_prob_product_log_last_acc=89.24%, self_consistency_acc=90.22%, p_true_acc=90.46%, normilized_likelihood_acc=89.49%, normilized_entropy_acc=89.24%, topk_entropy_acc=89.24%, window_entropy_acc=89.98%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 409/500 [34:29:30<6:42:30, 265.39s/it, attention_weighted_confidence_acc=89.98%, cer_entropy_weighted_mean_all_acc=89.98%, cer_prob_product_log_last_acc=89.24%, self_consistency_acc=90.22%, p_true_acc=90.46%, normilized_likelihood_acc=89.49%, normilized_entropy_acc=89.24%, topk_entropy_acc=89.24%, window_entropy_acc=89.98%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 7.597951981916332
    Answer: 145
    Ground truth:  145
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 7.597951981916332
    Answer: 145
    Ground truth:  145
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 14.997810244560242
    Answer: 145
    Ground truth:  145
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 0.9375
    Answer: 145
    Ground truth:  145
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 14.07421875
    Answer: 145
    Ground truth:  145
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 3.457826644182205
    Answer: 145
    Ground truth:  145
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 3.727904498577118
    Answer: 145
    Ground truth:  145
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 3.260239228606224
    Answer: 145
    Ground truth:  145
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we will break it down into steps.

Step 1: Find Betty's p...
    Score: 18.344712913036346
    Answer: 145
    Ground truth:  145
Method name: attention_weighted_confidence, running accuracy: 90.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0
Method name: cer_prob_product_log_last, running accuracy: 89.26829268292683
Method name: self_consistency, running accuracy: 90.2439024390244
Method name: p_true, running accuracy: 90.48780487804878
Method name: normilized_likelihood, running accuracy: 89.51219512195122
Method name: normilized_entropy, running accuracy: 89.26829268292683
Method name: topk_entropy, running accuracy: 89.26829268292683
Method name: window_entropy, running accuracy: 90.0
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 409/500 [34:35:33<6:42:30, 265.39s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=89.27%, self_consistency_acc=90.24%, p_true_acc=90.49%, normilized_likelihood_acc=89.51%, normilized_entropy_acc=89.27%, topk_entropy_acc=89.27%, window_entropy_acc=90.00%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 410/500 [34:35:33<7:21:40, 294.45s/it, attention_weighted_confidence_acc=90.00%, cer_entropy_weighted_mean_all_acc=90.00%, cer_prob_product_log_last_acc=89.27%, self_consistency_acc=90.24%, p_true_acc=90.49%, normilized_likelihood_acc=89.51%, normilized_entropy_acc=89.27%, topk_entropy_acc=89.27%, window_entropy_acc=90.00%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 5.339116936562336
    Answer: 54
    Ground truth:  54
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 5.339116936562336
    Answer: 54
    Ground truth:  54
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 11.979674696922302
    Answer: 54
    Ground truth:  54
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 0.75
    Answer: 54
    Ground truth:  54
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 9.267578125
    Answer: 54
    Ground truth:  54
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 17.92881751060486
    Answer: 54
    Ground truth:  54
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 17.66070368885994
    Answer: 54
    Ground truth:  54
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 14.609202861785889
    Answer: 54
    Ground truth:  54
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. Since Elvis travels...
    Score: 13.95880526304245
    Answer: 54
    Ground truth:  54
Method name: attention_weighted_confidence, running accuracy: 90.02433090024331
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.02433090024331
Method name: cer_prob_product_log_last, running accuracy: 89.29440389294405
Method name: self_consistency, running accuracy: 90.2676399026764
Method name: p_true, running accuracy: 90.51094890510949
Method name: normilized_likelihood, running accuracy: 89.53771289537713
Method name: normilized_entropy, running accuracy: 89.29440389294405
Method name: topk_entropy, running accuracy: 89.29440389294405
Method name: window_entropy, running accuracy: 90.02433090024331
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 410/500 [34:42:00<7:21:40, 294.45s/it, attention_weighted_confidence_acc=90.02%, cer_entropy_weighted_mean_all_acc=90.02%, cer_prob_product_log_last_acc=89.29%, self_consistency_acc=90.27%, p_true_acc=90.51%, normilized_likelihood_acc=89.54%, normilized_entropy_acc=89.29%, topk_entropy_acc=89.29%, window_entropy_acc=90.02%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 411/500 [34:42:00<7:58:10, 322.36s/it, attention_weighted_confidence_acc=90.02%, cer_entropy_weighted_mean_all_acc=90.02%, cer_prob_product_log_last_acc=89.29%, self_consistency_acc=90.27%, p_true_acc=90.51%, normilized_likelihood_acc=89.54%, normilized_entropy_acc=89.29%, topk_entropy_acc=89.29%, window_entropy_acc=90.02%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 7.962118540011043
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 7.962118540011043
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 15.975381553173065
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 15.484375
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 9.850565314292908
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 9.711715951561928
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 8.87548442184925
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's calculate the number of white shirts with collars an...
    Score: 22.107835948467255
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 90.0485436893204
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.0485436893204
Method name: cer_prob_product_log_last, running accuracy: 89.32038834951457
Method name: self_consistency, running accuracy: 90.29126213592234
Method name: p_true, running accuracy: 90.53398058252428
Method name: normilized_likelihood, running accuracy: 89.56310679611651
Method name: normilized_entropy, running accuracy: 89.32038834951457
Method name: topk_entropy, running accuracy: 89.32038834951457
Method name: window_entropy, running accuracy: 90.0485436893204
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 411/500 [34:48:16<7:58:10, 322.36s/it, attention_weighted_confidence_acc=90.05%, cer_entropy_weighted_mean_all_acc=90.05%, cer_prob_product_log_last_acc=89.32%, self_consistency_acc=90.29%, p_true_acc=90.53%, normilized_likelihood_acc=89.56%, normilized_entropy_acc=89.32%, topk_entropy_acc=89.32%, window_entropy_acc=90.05%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 412/500 [34:48:16<8:16:29, 338.52s/it, attention_weighted_confidence_acc=90.05%, cer_entropy_weighted_mean_all_acc=90.05%, cer_prob_product_log_last_acc=89.32%, self_consistency_acc=90.29%, p_true_acc=90.53%, normilized_likelihood_acc=89.56%, normilized_entropy_acc=89.32%, topk_entropy_acc=89.32%, window_entropy_acc=90.05%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 8.486013563743409
    Answer: 4500
    Ground truth:  4500
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 8.486013563743409
    Answer: 4500
    Ground truth:  4500
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 7.9999817013795465
    Answer: 4500
    Ground truth:  4500
Method 4: self_consistency
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 1.0
    Answer: 4500
    Ground truth:  4500
Method 5: p_true
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 12.9453125
    Answer: 4500
    Ground truth:  4500
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 1.7960263267159462
    Answer: 4500
    Ground truth:  4500
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 0.5917526334524155
    Answer: 4500
    Ground truth:  4500
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 0.590572401881218
    Answer: 4500
    Ground truth:  4500
Method 9: window_entropy
  Batch 1:
    Text: To calculate the total amount of money John spent, let's break it down step by s...
    Score: 1.0413821935653687
    Answer: 4500
    Ground truth:  4500
Method name: attention_weighted_confidence, running accuracy: 90.07263922518159
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.07263922518159
Method name: cer_prob_product_log_last, running accuracy: 89.34624697336562
Method name: self_consistency, running accuracy: 90.31476997578693
Method name: p_true, running accuracy: 90.55690072639226
Method name: normilized_likelihood, running accuracy: 89.58837772397095
Method name: normilized_entropy, running accuracy: 89.34624697336562
Method name: topk_entropy, running accuracy: 89.34624697336562
Method name: window_entropy, running accuracy: 90.07263922518159
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  82%|████████▏ | 412/500 [34:50:57<8:16:29, 338.52s/it, attention_weighted_confidence_acc=90.07%, cer_entropy_weighted_mean_all_acc=90.07%, cer_prob_product_log_last_acc=89.35%, self_consistency_acc=90.31%, p_true_acc=90.56%, normilized_likelihood_acc=89.59%, normilized_entropy_acc=89.35%, topk_entropy_acc=89.35%, window_entropy_acc=90.07%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 413/500 [34:50:57<6:53:24, 285.10s/it, attention_weighted_confidence_acc=90.07%, cer_entropy_weighted_mean_all_acc=90.07%, cer_prob_product_log_last_acc=89.35%, self_consistency_acc=90.31%, p_true_acc=90.56%, normilized_likelihood_acc=89.59%, normilized_entropy_acc=89.35%, topk_entropy_acc=89.35%, window_entropy_acc=90.07%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 8.018940323656583
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 8.018940323656583
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 15.999913811683655
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 14.33984375
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 2.082986071705818
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 1.5464724600315094
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 1.523703008890152
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the height of the beanstalk after 3 weeks, we need to consider the growt...
    Score: 3.4855284988880157
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 90.09661835748793
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.09661835748793
Method name: cer_prob_product_log_last, running accuracy: 89.3719806763285
Method name: self_consistency, running accuracy: 90.33816425120773
Method name: p_true, running accuracy: 90.57971014492753
Method name: normilized_likelihood, running accuracy: 89.61352657004831
Method name: normilized_entropy, running accuracy: 89.3719806763285
Method name: topk_entropy, running accuracy: 89.3719806763285
Method name: window_entropy, running accuracy: 90.09661835748793
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 413/500 [34:52:51<6:53:24, 285.10s/it, attention_weighted_confidence_acc=90.10%, cer_entropy_weighted_mean_all_acc=90.10%, cer_prob_product_log_last_acc=89.37%, self_consistency_acc=90.34%, p_true_acc=90.58%, normilized_likelihood_acc=89.61%, normilized_entropy_acc=89.37%, topk_entropy_acc=89.37%, window_entropy_acc=90.10%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 414/500 [34:52:51<5:35:15, 233.90s/it, attention_weighted_confidence_acc=90.10%, cer_entropy_weighted_mean_all_acc=90.10%, cer_prob_product_log_last_acc=89.37%, self_consistency_acc=90.34%, p_true_acc=90.58%, normilized_likelihood_acc=89.61%, normilized_entropy_acc=89.37%, topk_entropy_acc=89.37%, window_entropy_acc=90.10%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 7.692566983318102
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 7.692566983318102
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 13.545705795288086
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 0.9375
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 14.265625
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 4.301403805613518
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 4.717123925685883
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 4.505418360233307
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. Matt wants to eat five cookies in total....
    Score: 16.200072169303894
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 90.12048192771084
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.12048192771084
Method name: cer_prob_product_log_last, running accuracy: 89.39759036144578
Method name: self_consistency, running accuracy: 90.36144578313254
Method name: p_true, running accuracy: 90.60240963855422
Method name: normilized_likelihood, running accuracy: 89.63855421686748
Method name: normilized_entropy, running accuracy: 89.39759036144578
Method name: topk_entropy, running accuracy: 89.39759036144578
Method name: window_entropy, running accuracy: 90.12048192771084
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 414/500 [34:56:05<5:35:15, 233.90s/it, attention_weighted_confidence_acc=90.12%, cer_entropy_weighted_mean_all_acc=90.12%, cer_prob_product_log_last_acc=89.40%, self_consistency_acc=90.36%, p_true_acc=90.60%, normilized_likelihood_acc=89.64%, normilized_entropy_acc=89.40%, topk_entropy_acc=89.40%, window_entropy_acc=90.12%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 415/500 [34:56:05<5:14:22, 221.91s/it, attention_weighted_confidence_acc=90.12%, cer_entropy_weighted_mean_all_acc=90.12%, cer_prob_product_log_last_acc=89.40%, self_consistency_acc=90.36%, p_true_acc=90.60%, normilized_likelihood_acc=89.64%, normilized_entropy_acc=89.40%, topk_entropy_acc=89.40%, window_entropy_acc=90.12%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 7.593001174713938
    Answer: 72
    Ground truth:  72
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 7.593001174713938
    Answer: 72
    Ground truth:  72
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 15.754090249538422
    Answer: 72
    Ground truth:  72
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 1.0
    Answer: 72
    Ground truth:  72
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 14.330810546875
    Answer: 72
    Ground truth:  72
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 16.35648100078106
    Answer: 72
    Ground truth:  72
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 15.671570271253586
    Answer: 72
    Ground truth:  72
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 13.648802012205124
    Answer: 72
    Ground truth:  72
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Matthew has a collection of 12 un...
    Score: 21.404876232147217
    Answer: 72
    Ground truth:  72
Method name: attention_weighted_confidence, running accuracy: 90.14423076923077
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.14423076923077
Method name: cer_prob_product_log_last, running accuracy: 89.42307692307693
Method name: self_consistency, running accuracy: 90.38461538461539
Method name: p_true, running accuracy: 90.625
Method name: normilized_likelihood, running accuracy: 89.66346153846155
Method name: normilized_entropy, running accuracy: 89.42307692307693
Method name: topk_entropy, running accuracy: 89.42307692307693
Method name: window_entropy, running accuracy: 90.14423076923077
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 415/500 [34:59:41<5:14:22, 221.91s/it, attention_weighted_confidence_acc=90.14%, cer_entropy_weighted_mean_all_acc=90.14%, cer_prob_product_log_last_acc=89.42%, self_consistency_acc=90.38%, p_true_acc=90.62%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=89.42%, topk_entropy_acc=89.42%, window_entropy_acc=90.14%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 416/500 [34:59:41<5:08:01, 220.02s/it, attention_weighted_confidence_acc=90.14%, cer_entropy_weighted_mean_all_acc=90.14%, cer_prob_product_log_last_acc=89.42%, self_consistency_acc=90.38%, p_true_acc=90.62%, normilized_likelihood_acc=89.66%, normilized_entropy_acc=89.42%, topk_entropy_acc=89.42%, window_entropy_acc=90.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 6.5485101934777745
    Answer: 21
    Ground truth:  21
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 6.5485101934777745
    Answer: 21
    Ground truth:  21
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 13.996735394001007
    Answer: 21
    Ground truth:  21
Method 4: self_consistency
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 0.875
    Answer: 21
    Ground truth:  21
Method 5: p_true
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 10.80859375
    Answer: 21
    Ground truth:  21
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 10.232402622699738
    Answer: 21
    Ground truth:  21
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 10.96330001950264
    Answer: 21
    Ground truth:  21
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 9.802672773599625
    Answer: 21
    Ground truth:  21
Method 9: window_entropy
  Batch 1:
    Text: To find out how much more money Mitchell made than Liam, we first need to find o...
    Score: 18.51593554019928
    Answer: 21
    Ground truth:  21
Method name: attention_weighted_confidence, running accuracy: 90.16786570743405
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.16786570743405
Method name: cer_prob_product_log_last, running accuracy: 89.44844124700239
Method name: self_consistency, running accuracy: 90.40767386091127
Method name: p_true, running accuracy: 90.64748201438849
Method name: normilized_likelihood, running accuracy: 89.68824940047962
Method name: normilized_entropy, running accuracy: 89.44844124700239
Method name: topk_entropy, running accuracy: 89.44844124700239
Method name: window_entropy, running accuracy: 90.16786570743405
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 416/500 [35:02:19<5:08:01, 220.02s/it, attention_weighted_confidence_acc=90.17%, cer_entropy_weighted_mean_all_acc=90.17%, cer_prob_product_log_last_acc=89.45%, self_consistency_acc=90.41%, p_true_acc=90.65%, normilized_likelihood_acc=89.69%, normilized_entropy_acc=89.45%, topk_entropy_acc=89.45%, window_entropy_acc=90.17%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 417/500 [35:02:19<4:38:55, 201.64s/it, attention_weighted_confidence_acc=90.17%, cer_entropy_weighted_mean_all_acc=90.17%, cer_prob_product_log_last_acc=89.45%, self_consistency_acc=90.41%, p_true_acc=90.65%, normilized_likelihood_acc=89.69%, normilized_entropy_acc=89.45%, topk_entropy_acc=89.45%, window_entropy_acc=90.17%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 3.5539470845467984
    Answer: 64
    Ground truth:  64
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 3.5539470845467984
    Answer: 64
    Ground truth:  64
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 7.043921649456024
    Answer: 64
    Ground truth:  64
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 0.5
    Answer: 64
    Ground truth:  64
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 7.65234375
    Answer: 64
    Ground truth:  64
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 7.219661980867386
    Answer: 64
    Ground truth:  64
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 6.975431352853775
    Answer: 64
    Ground truth:  64
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 6.10859015583992
    Answer: 64
    Ground truth:  64
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

Let's denote the number of stickers ...
    Score: 8.389420926570892
    Answer: 64
    Ground truth:  64
Method name: attention_weighted_confidence, running accuracy: 90.19138755980862
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.19138755980862
Method name: cer_prob_product_log_last, running accuracy: 89.47368421052632
Method name: self_consistency, running accuracy: 90.43062200956938
Method name: p_true, running accuracy: 90.66985645933015
Method name: normilized_likelihood, running accuracy: 89.71291866028707
Method name: normilized_entropy, running accuracy: 89.47368421052632
Method name: topk_entropy, running accuracy: 89.47368421052632
Method name: window_entropy, running accuracy: 90.19138755980862
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  83%|████████▎ | 417/500 [35:06:30<4:38:55, 201.64s/it, attention_weighted_confidence_acc=90.19%, cer_entropy_weighted_mean_all_acc=90.19%, cer_prob_product_log_last_acc=89.47%, self_consistency_acc=90.43%, p_true_acc=90.67%, normilized_likelihood_acc=89.71%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=90.19%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▎ | 418/500 [35:06:30<4:55:41, 216.36s/it, attention_weighted_confidence_acc=90.19%, cer_entropy_weighted_mean_all_acc=90.19%, cer_prob_product_log_last_acc=89.47%, self_consistency_acc=90.43%, p_true_acc=90.67%, normilized_likelihood_acc=89.71%, normilized_entropy_acc=89.47%, topk_entropy_acc=89.47%, window_entropy_acc=90.19%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 8.35959961941127
    Answer: 175
    Ground truth:  175
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 8.35959961941127
    Answer: 175
    Ground truth:  175
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 15.999730944633484
    Answer: 175
    Ground truth:  175
Method 4: self_consistency
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 1.0
    Answer: 175
    Ground truth:  175
Method 5: p_true
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 15.515625
    Answer: 175
    Ground truth:  175
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 2.744380474090576
    Answer: 175
    Ground truth:  175
Method 7: normilized_entropy
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 2.676329478621483
    Answer: 175
    Ground truth:  175
Method 8: topk_entropy
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 2.6446857303380966
    Answer: 175
    Ground truth:  175
Method 9: window_entropy
  Batch 1:
    Text: To find out the total number of goals scored by the three teenagers, we need to ...
    Score: 4.925419628620148
    Answer: 175
    Ground truth:  175
Method name: attention_weighted_confidence, running accuracy: 90.21479713603819
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.21479713603819
Method name: cer_prob_product_log_last, running accuracy: 89.49880668257757
Method name: self_consistency, running accuracy: 90.45346062052506
Method name: p_true, running accuracy: 90.69212410501193
Method name: normilized_likelihood, running accuracy: 89.73747016706443
Method name: normilized_entropy, running accuracy: 89.49880668257757
Method name: topk_entropy, running accuracy: 89.49880668257757
Method name: window_entropy, running accuracy: 90.21479713603819
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▎ | 418/500 [35:09:49<4:55:41, 216.36s/it, attention_weighted_confidence_acc=90.21%, cer_entropy_weighted_mean_all_acc=90.21%, cer_prob_product_log_last_acc=89.50%, self_consistency_acc=90.45%, p_true_acc=90.69%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=89.50%, topk_entropy_acc=89.50%, window_entropy_acc=90.21%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 419/500 [35:09:49<4:44:55, 211.05s/it, attention_weighted_confidence_acc=90.21%, cer_entropy_weighted_mean_all_acc=90.21%, cer_prob_product_log_last_acc=89.50%, self_consistency_acc=90.45%, p_true_acc=90.69%, normilized_likelihood_acc=89.74%, normilized_entropy_acc=89.50%, topk_entropy_acc=89.50%, window_entropy_acc=90.21%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 8.284062581551158
    Answer: 2600
    Ground truth:  2600
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 8.284062581551158
    Answer: 2600
    Ground truth:  2600
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 7.962035405471248
    Answer: 2600
    Ground truth:  2600
Method 4: self_consistency
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 1.0
    Answer: 2600
    Ground truth:  2600
Method 5: p_true
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 14.73828125
    Answer: 2600
    Ground truth:  2600
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 2.4117274284362793
    Answer: 2600
    Ground truth:  2600
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 1.8149503469467163
    Answer: 2600
    Ground truth:  2600
Method 8: topk_entropy
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 1.8100677728652954
    Answer: 2600
    Ground truth:  2600
Method 9: window_entropy
  Batch 1:
    Text: To find the total weight of Joe's bag of candy, we need to calculate the weight ...
    Score: 4.932505249977112
    Answer: 2600
    Ground truth:  2600
Method name: attention_weighted_confidence, running accuracy: 90.23809523809524
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.23809523809524
Method name: cer_prob_product_log_last, running accuracy: 89.52380952380953
Method name: self_consistency, running accuracy: 90.47619047619048
Method name: p_true, running accuracy: 90.71428571428571
Method name: normilized_likelihood, running accuracy: 89.76190476190476
Method name: normilized_entropy, running accuracy: 89.52380952380953
Method name: topk_entropy, running accuracy: 89.52380952380953
Method name: window_entropy, running accuracy: 90.23809523809524
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 419/500 [35:13:35<4:44:55, 211.05s/it, attention_weighted_confidence_acc=90.24%, cer_entropy_weighted_mean_all_acc=90.24%, cer_prob_product_log_last_acc=89.52%, self_consistency_acc=90.48%, p_true_acc=90.71%, normilized_likelihood_acc=89.76%, normilized_entropy_acc=89.52%, topk_entropy_acc=89.52%, window_entropy_acc=90.24%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 420/500 [35:13:35<4:47:32, 215.66s/it, attention_weighted_confidence_acc=90.24%, cer_entropy_weighted_mean_all_acc=90.24%, cer_prob_product_log_last_acc=89.52%, self_consistency_acc=90.48%, p_true_acc=90.71%, normilized_likelihood_acc=89.76%, normilized_entropy_acc=89.52%, topk_entropy_acc=89.52%, window_entropy_acc=90.24%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 6.332997212804022
    Answer: 120
    Ground truth:  120
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 6.332997212804022
    Answer: 120
    Ground truth:  120
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 11.999494671821594
    Answer: 120
    Ground truth:  120
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 0.75
    Answer: 120
    Ground truth:  120
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 10.734375
    Answer: 120
    Ground truth:  120
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 5.7848741710186005
    Answer: 120
    Ground truth:  120
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 6.131801500916481
    Answer: 120
    Ground truth:  120
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 5.566740646958351
    Answer: 120
    Ground truth:  120
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step.

1. Aurelia has $120.

...
    Score: 13.597732961177826
    Answer: 120
    Ground truth:  120
Method name: attention_weighted_confidence, running accuracy: 90.26128266033254
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.26128266033254
Method name: cer_prob_product_log_last, running accuracy: 89.54869358669833
Method name: self_consistency, running accuracy: 90.49881235154395
Method name: p_true, running accuracy: 90.73634204275535
Method name: normilized_likelihood, running accuracy: 89.78622327790974
Method name: normilized_entropy, running accuracy: 89.54869358669833
Method name: topk_entropy, running accuracy: 89.54869358669833
Method name: window_entropy, running accuracy: 90.26128266033254
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 420/500 [35:17:15<4:47:32, 215.66s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=89.55%, self_consistency_acc=90.50%, p_true_acc=90.74%, normilized_likelihood_acc=89.79%, normilized_entropy_acc=89.55%, topk_entropy_acc=89.55%, window_entropy_acc=90.26%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 421/500 [35:17:15<4:45:39, 216.96s/it, attention_weighted_confidence_acc=90.26%, cer_entropy_weighted_mean_all_acc=90.26%, cer_prob_product_log_last_acc=89.55%, self_consistency_acc=90.50%, p_true_acc=90.74%, normilized_likelihood_acc=89.79%, normilized_entropy_acc=89.55%, topk_entropy_acc=89.55%, window_entropy_acc=90.26%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 7.673118651441501
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 7.673118651441501
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 15.995869636535645
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 1.0
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 12.7578125
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 2.9226164370775223
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 2.3406947404146194
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 2.3299954682588577
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: To find the change, we first need to find the total amount of money Thea gave th...
    Score: 5.402079850435257
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 90.28436018957346
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.28436018957346
Method name: cer_prob_product_log_last, running accuracy: 89.57345971563981
Method name: self_consistency, running accuracy: 90.52132701421802
Method name: p_true, running accuracy: 90.75829383886256
Method name: normilized_likelihood, running accuracy: 89.81042654028435
Method name: normilized_entropy, running accuracy: 89.57345971563981
Method name: topk_entropy, running accuracy: 89.57345971563981
Method name: window_entropy, running accuracy: 90.28436018957346
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 421/500 [35:19:03<4:45:39, 216.96s/it, attention_weighted_confidence_acc=90.28%, cer_entropy_weighted_mean_all_acc=90.28%, cer_prob_product_log_last_acc=89.57%, self_consistency_acc=90.52%, p_true_acc=90.76%, normilized_likelihood_acc=89.81%, normilized_entropy_acc=89.57%, topk_entropy_acc=89.57%, window_entropy_acc=90.28%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 422/500 [35:19:03<3:59:21, 184.12s/it, attention_weighted_confidence_acc=90.28%, cer_entropy_weighted_mean_all_acc=90.28%, cer_prob_product_log_last_acc=89.57%, self_consistency_acc=90.52%, p_true_acc=90.76%, normilized_likelihood_acc=89.81%, normilized_entropy_acc=89.57%, topk_entropy_acc=89.57%, window_entropy_acc=90.28%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 8.41500883664924
    Answer: 96
    Ground truth:  96
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 8.41500883664924
    Answer: 96
    Ground truth:  96
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 14.192610766738653
    Answer: 96
    Ground truth:  96
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 1.0
    Answer: 96
    Ground truth:  96
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 15.40234375
    Answer: 96
    Ground truth:  96
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 2.3071309328079224
    Answer: 96
    Ground truth:  96
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 2.2802804857492447
    Answer: 96
    Ground truth:  96
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 2.2490875720977783
    Answer: 96
    Ground truth:  96
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Let x be the number of strawberry...
    Score: 9.347170323133469
    Answer: 96
    Ground truth:  96
Method name: attention_weighted_confidence, running accuracy: 90.30732860520094
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.30732860520094
Method name: cer_prob_product_log_last, running accuracy: 89.59810874704492
Method name: self_consistency, running accuracy: 90.54373522458629
Method name: p_true, running accuracy: 90.78014184397163
Method name: normilized_likelihood, running accuracy: 89.83451536643025
Method name: normilized_entropy, running accuracy: 89.59810874704492
Method name: topk_entropy, running accuracy: 89.59810874704492
Method name: window_entropy, running accuracy: 90.30732860520094
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  84%|████████▍ | 422/500 [35:22:52<3:59:21, 184.12s/it, attention_weighted_confidence_acc=90.31%, cer_entropy_weighted_mean_all_acc=90.31%, cer_prob_product_log_last_acc=89.60%, self_consistency_acc=90.54%, p_true_acc=90.78%, normilized_likelihood_acc=89.83%, normilized_entropy_acc=89.60%, topk_entropy_acc=89.60%, window_entropy_acc=90.31%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▍ | 423/500 [35:22:52<4:13:49, 197.78s/it, attention_weighted_confidence_acc=90.31%, cer_entropy_weighted_mean_all_acc=90.31%, cer_prob_product_log_last_acc=89.60%, self_consistency_acc=90.54%, p_true_acc=90.78%, normilized_likelihood_acc=89.83%, normilized_entropy_acc=89.60%, topk_entropy_acc=89.60%, window_entropy_acc=90.31%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 7.690713507308093
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 7.690713507308093
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 14.993996322154999
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 0.9375
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 9.66796875
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 2.414031535387039
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 2.4488246887922287
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 2.440683349967003
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of everything, we need to calculate the cost of non-food ...
    Score: 4.02711421251297
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 90.33018867924528
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.33018867924528
Method name: cer_prob_product_log_last, running accuracy: 89.62264150943396
Method name: self_consistency, running accuracy: 90.56603773584906
Method name: p_true, running accuracy: 90.80188679245283
Method name: normilized_likelihood, running accuracy: 89.85849056603774
Method name: normilized_entropy, running accuracy: 89.62264150943396
Method name: topk_entropy, running accuracy: 89.62264150943396
Method name: window_entropy, running accuracy: 90.33018867924528
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▍ | 423/500 [35:26:12<4:13:49, 197.78s/it, attention_weighted_confidence_acc=90.33%, cer_entropy_weighted_mean_all_acc=90.33%, cer_prob_product_log_last_acc=89.62%, self_consistency_acc=90.57%, p_true_acc=90.80%, normilized_likelihood_acc=89.86%, normilized_entropy_acc=89.62%, topk_entropy_acc=89.62%, window_entropy_acc=90.33%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▍ | 424/500 [35:26:12<4:11:02, 198.20s/it, attention_weighted_confidence_acc=90.33%, cer_entropy_weighted_mean_all_acc=90.33%, cer_prob_product_log_last_acc=89.62%, self_consistency_acc=90.57%, p_true_acc=90.80%, normilized_likelihood_acc=89.86%, normilized_entropy_acc=89.62%, topk_entropy_acc=89.62%, window_entropy_acc=90.33%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 6.735734482642371
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 6.735734482642371
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 13.999051868915558
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 0.875
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 13.5078125
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 1.9114008247852325
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 1.023020088672638
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 0.9865831434726715
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Calculate the amount Mark needs to save to buy the bike. 
Mark already h...
    Score: 2.753119468688965
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 90.3529411764706
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.3529411764706
Method name: cer_prob_product_log_last, running accuracy: 89.64705882352942
Method name: self_consistency, running accuracy: 90.58823529411765
Method name: p_true, running accuracy: 90.8235294117647
Method name: normilized_likelihood, running accuracy: 89.88235294117646
Method name: normilized_entropy, running accuracy: 89.64705882352942
Method name: topk_entropy, running accuracy: 89.64705882352942
Method name: window_entropy, running accuracy: 90.3529411764706
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▍ | 424/500 [35:28:28<4:11:02, 198.20s/it, attention_weighted_confidence_acc=90.35%, cer_entropy_weighted_mean_all_acc=90.35%, cer_prob_product_log_last_acc=89.65%, self_consistency_acc=90.59%, p_true_acc=90.82%, normilized_likelihood_acc=89.88%, normilized_entropy_acc=89.65%, topk_entropy_acc=89.65%, window_entropy_acc=90.35%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▌ | 425/500 [35:28:28<3:44:36, 179.69s/it, attention_weighted_confidence_acc=90.35%, cer_entropy_weighted_mean_all_acc=90.35%, cer_prob_product_log_last_acc=89.65%, self_consistency_acc=90.59%, p_true_acc=90.82%, normilized_likelihood_acc=89.88%, normilized_entropy_acc=89.65%, topk_entropy_acc=89.65%, window_entropy_acc=90.35%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 6.185507432745699
    Answer: 17
    Ground truth:  17
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 6.185507432745699
    Answer: 17
    Ground truth:  17
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 11.993419885635376
    Answer: 17
    Ground truth:  17
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 0.75
    Answer: 17
    Ground truth:  17
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 11.3046875
    Answer: 17
    Ground truth:  17
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 14.88015565276146
    Answer: 17
    Ground truth:  17
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 14.089005053043365
    Answer: 17
    Ground truth:  17
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 12.12871339917183
    Answer: 17
    Ground truth:  17
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Total number of students in one c...
    Score: 18.595712065696716
    Answer: 17
    Ground truth:  17
Method name: attention_weighted_confidence, running accuracy: 90.3755868544601
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.3755868544601
Method name: cer_prob_product_log_last, running accuracy: 89.67136150234741
Method name: self_consistency, running accuracy: 90.61032863849765
Method name: p_true, running accuracy: 90.84507042253522
Method name: normilized_likelihood, running accuracy: 89.90610328638498
Method name: normilized_entropy, running accuracy: 89.67136150234741
Method name: topk_entropy, running accuracy: 89.67136150234741
Method name: window_entropy, running accuracy: 90.3755868544601
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▌ | 425/500 [35:34:03<3:44:36, 179.69s/it, attention_weighted_confidence_acc=90.38%, cer_entropy_weighted_mean_all_acc=90.38%, cer_prob_product_log_last_acc=89.67%, self_consistency_acc=90.61%, p_true_acc=90.85%, normilized_likelihood_acc=89.91%, normilized_entropy_acc=89.67%, topk_entropy_acc=89.67%, window_entropy_acc=90.38%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▌ | 426/500 [35:34:03<4:39:05, 226.29s/it, attention_weighted_confidence_acc=90.38%, cer_entropy_weighted_mean_all_acc=90.38%, cer_prob_product_log_last_acc=89.67%, self_consistency_acc=90.61%, p_true_acc=90.85%, normilized_likelihood_acc=89.91%, normilized_entropy_acc=89.67%, topk_entropy_acc=89.67%, window_entropy_acc=90.38%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 8.735621292566314
    Answer: 123
    Ground truth:  123
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 8.735621292566314
    Answer: 123
    Ground truth:  123
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 15.64360249042511
    Answer: 123
    Ground truth:  123
Method 4: self_consistency
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 1.0
    Answer: 123
    Ground truth:  123
Method 5: p_true
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 15.015625
    Answer: 123
    Ground truth:  123
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 2.0303206220269203
    Answer: 123
    Ground truth:  123
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 1.1674869507551193
    Answer: 123
    Ground truth:  123
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 1.1593447625637054
    Answer: 123
    Ground truth:  123
Method 9: window_entropy
  Batch 1:
    Text: To find the number of girls and nongendered children, we need to follow the give...
    Score: 4.718399465084076
    Answer: 123
    Ground truth:  123
Method name: attention_weighted_confidence, running accuracy: 90.39812646370024
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.39812646370024
Method name: cer_prob_product_log_last, running accuracy: 89.69555035128806
Method name: self_consistency, running accuracy: 90.63231850117096
Method name: p_true, running accuracy: 90.86651053864169
Method name: normilized_likelihood, running accuracy: 89.92974238875878
Method name: normilized_entropy, running accuracy: 89.69555035128806
Method name: topk_entropy, running accuracy: 89.69555035128806
Method name: window_entropy, running accuracy: 90.39812646370024
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▌ | 426/500 [35:37:03<4:39:05, 226.29s/it, attention_weighted_confidence_acc=90.40%, cer_entropy_weighted_mean_all_acc=90.40%, cer_prob_product_log_last_acc=89.70%, self_consistency_acc=90.63%, p_true_acc=90.87%, normilized_likelihood_acc=89.93%, normilized_entropy_acc=89.70%, topk_entropy_acc=89.70%, window_entropy_acc=90.40%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▌ | 427/500 [35:37:03<4:18:15, 212.27s/it, attention_weighted_confidence_acc=90.40%, cer_entropy_weighted_mean_all_acc=90.40%, cer_prob_product_log_last_acc=89.70%, self_consistency_acc=90.63%, p_true_acc=90.87%, normilized_likelihood_acc=89.93%, normilized_entropy_acc=89.70%, topk_entropy_acc=89.70%, window_entropy_acc=90.40%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 7.7784599462369535
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 7.7784599462369535
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 15.998549461364746
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 1.0
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 14.41796875
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 2.910562351346016
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 1.6301045566797256
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 1.6032195389270782
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To find out how much candy each person would have after sharing equally, we need...
    Score: 5.448122501373291
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 90.42056074766354
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.42056074766354
Method name: cer_prob_product_log_last, running accuracy: 89.7196261682243
Method name: self_consistency, running accuracy: 90.65420560747664
Method name: p_true, running accuracy: 90.88785046728972
Method name: normilized_likelihood, running accuracy: 89.95327102803739
Method name: normilized_entropy, running accuracy: 89.7196261682243
Method name: topk_entropy, running accuracy: 89.7196261682243
Method name: window_entropy, running accuracy: 90.42056074766354
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  85%|████████▌ | 427/500 [35:39:22<4:18:15, 212.27s/it, attention_weighted_confidence_acc=90.42%, cer_entropy_weighted_mean_all_acc=90.42%, cer_prob_product_log_last_acc=89.72%, self_consistency_acc=90.65%, p_true_acc=90.89%, normilized_likelihood_acc=89.95%, normilized_entropy_acc=89.72%, topk_entropy_acc=89.72%, window_entropy_acc=90.42%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 428/500 [35:39:22<3:48:39, 190.54s/it, attention_weighted_confidence_acc=90.42%, cer_entropy_weighted_mean_all_acc=90.42%, cer_prob_product_log_last_acc=89.72%, self_consistency_acc=90.65%, p_true_acc=90.89%, normilized_likelihood_acc=89.95%, normilized_entropy_acc=89.72%, topk_entropy_acc=89.72%, window_entropy_acc=90.42%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 7.8828175496770125
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 7.8828175496770125
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 15.999890804290771
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 1.0
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 13.7734375
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 1.4877859801054
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 1.5746913850307465
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 1.575609415769577
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. Annika brought $50 to the town fair.
2. Sh...
    Score: 2.278898775577545
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 90.44289044289044
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.44289044289044
Method name: cer_prob_product_log_last, running accuracy: 89.74358974358975
Method name: self_consistency, running accuracy: 90.67599067599068
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 89.97668997668997
Method name: normilized_entropy, running accuracy: 89.74358974358975
Method name: topk_entropy, running accuracy: 89.74358974358975
Method name: window_entropy, running accuracy: 90.44289044289044
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 428/500 [35:41:22<3:48:39, 190.54s/it, attention_weighted_confidence_acc=90.44%, cer_entropy_weighted_mean_all_acc=90.44%, cer_prob_product_log_last_acc=89.74%, self_consistency_acc=90.68%, p_true_acc=90.91%, normilized_likelihood_acc=89.98%, normilized_entropy_acc=89.74%, topk_entropy_acc=89.74%, window_entropy_acc=90.44%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 429/500 [35:41:22<3:20:09, 169.14s/it, attention_weighted_confidence_acc=90.44%, cer_entropy_weighted_mean_all_acc=90.44%, cer_prob_product_log_last_acc=89.74%, self_consistency_acc=90.68%, p_true_acc=90.91%, normilized_likelihood_acc=89.98%, normilized_entropy_acc=89.74%, topk_entropy_acc=89.74%, window_entropy_acc=90.44%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 7.592971996691222
    Answer: 168
    Ground truth:  168
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 7.592971996691222
    Answer: 168
    Ground truth:  168
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 14.977500557899475
    Answer: 168
    Ground truth:  168
Method 4: self_consistency
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 0.9375
    Answer: 168
    Ground truth:  168
Method 5: p_true
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 13.625
    Answer: 168
    Ground truth:  168
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 3.8701306581497192
    Answer: 168
    Ground truth:  168
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 2.7107649594545364
    Answer: 168
    Ground truth:  168
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 2.53460131585598
    Answer: 168
    Ground truth:  168
Method 9: window_entropy
  Batch 1:
    Text: To find out how many shells team Gogetters found, we first need to calculate the...
    Score: 12.478172361850739
    Answer: 168
    Ground truth:  168
Method name: attention_weighted_confidence, running accuracy: 90.46511627906978
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.46511627906978
Method name: cer_prob_product_log_last, running accuracy: 89.76744186046511
Method name: self_consistency, running accuracy: 90.69767441860465
Method name: p_true, running accuracy: 90.93023255813954
Method name: normilized_likelihood, running accuracy: 90.0
Method name: normilized_entropy, running accuracy: 89.76744186046511
Method name: topk_entropy, running accuracy: 89.76744186046511
Method name: window_entropy, running accuracy: 90.46511627906978
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 429/500 [35:44:51<3:20:09, 169.14s/it, attention_weighted_confidence_acc=90.47%, cer_entropy_weighted_mean_all_acc=90.47%, cer_prob_product_log_last_acc=89.77%, self_consistency_acc=90.70%, p_true_acc=90.93%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=89.77%, topk_entropy_acc=89.77%, window_entropy_acc=90.47%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 430/500 [35:44:51<3:31:27, 181.25s/it, attention_weighted_confidence_acc=90.47%, cer_entropy_weighted_mean_all_acc=90.47%, cer_prob_product_log_last_acc=89.77%, self_consistency_acc=90.70%, p_true_acc=90.93%, normilized_likelihood_acc=90.00%, normilized_entropy_acc=89.77%, topk_entropy_acc=89.77%, window_entropy_acc=90.47%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 8.28930829322987
    Answer: 188
    Ground truth:  188
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 8.28930829322987
    Answer: 188
    Ground truth:  188
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 15.984510123729706
    Answer: 188
    Ground truth:  188
Method 4: self_consistency
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 1.0
    Answer: 188
    Ground truth:  188
Method 5: p_true
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 13.0390625
    Answer: 188
    Ground truth:  188
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 2.997878819704056
    Answer: 188
    Ground truth:  188
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 1.2505407929420471
    Answer: 188
    Ground truth:  188
Method 8: topk_entropy
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 1.245856836438179
    Answer: 188
    Ground truth:  188
Method 9: window_entropy
  Batch 1:
    Text: To find the total thickness of the four books, let's break down the information ...
    Score: 1.7958832383155823
    Answer: 188
    Ground truth:  188
Method name: attention_weighted_confidence, running accuracy: 90.48723897911833
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.48723897911833
Method name: cer_prob_product_log_last, running accuracy: 89.79118329466357
Method name: self_consistency, running accuracy: 90.71925754060325
Method name: p_true, running accuracy: 90.95127610208816
Method name: normilized_likelihood, running accuracy: 90.02320185614849
Method name: normilized_entropy, running accuracy: 89.79118329466357
Method name: topk_entropy, running accuracy: 89.79118329466357
Method name: window_entropy, running accuracy: 90.48723897911833
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 430/500 [35:47:22<3:31:27, 181.25s/it, attention_weighted_confidence_acc=90.49%, cer_entropy_weighted_mean_all_acc=90.49%, cer_prob_product_log_last_acc=89.79%, self_consistency_acc=90.72%, p_true_acc=90.95%, normilized_likelihood_acc=90.02%, normilized_entropy_acc=89.79%, topk_entropy_acc=89.79%, window_entropy_acc=90.49%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 431/500 [35:47:22<3:17:54, 172.10s/it, attention_weighted_confidence_acc=90.49%, cer_entropy_weighted_mean_all_acc=90.49%, cer_prob_product_log_last_acc=89.79%, self_consistency_acc=90.72%, p_true_acc=90.95%, normilized_likelihood_acc=90.02%, normilized_entropy_acc=89.79%, topk_entropy_acc=89.79%, window_entropy_acc=90.49%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 5.634630811785788
    Answer: 2
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 5.634630811785788
    Answer: 2
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 10.972359657287598
    Answer: 2
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 0.6875
    Answer: 2
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 7.74609375
    Answer: 2
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 3.7582242488861084
    Answer: 2
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 2.822140112519264
    Answer: 2
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 2.7559337317943573
    Answer: 2
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find the cost price of each watermelon, we need to first calculate the total ...
    Score: 10.792295634746552
    Answer: 2
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 90.50925925925925
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.50925925925925
Method name: cer_prob_product_log_last, running accuracy: 89.81481481481481
Method name: self_consistency, running accuracy: 90.74074074074075
Method name: p_true, running accuracy: 90.97222222222221
Method name: normilized_likelihood, running accuracy: 90.04629629629629
Method name: normilized_entropy, running accuracy: 89.81481481481481
Method name: topk_entropy, running accuracy: 89.81481481481481
Method name: window_entropy, running accuracy: 90.50925925925925
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▌ | 431/500 [35:50:54<3:17:54, 172.10s/it, attention_weighted_confidence_acc=90.51%, cer_entropy_weighted_mean_all_acc=90.51%, cer_prob_product_log_last_acc=89.81%, self_consistency_acc=90.74%, p_true_acc=90.97%, normilized_likelihood_acc=90.05%, normilized_entropy_acc=89.81%, topk_entropy_acc=89.81%, window_entropy_acc=90.51%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▋ | 432/500 [35:50:54<3:28:40, 184.12s/it, attention_weighted_confidence_acc=90.51%, cer_entropy_weighted_mean_all_acc=90.51%, cer_prob_product_log_last_acc=89.81%, self_consistency_acc=90.74%, p_true_acc=90.97%, normilized_likelihood_acc=90.05%, normilized_entropy_acc=89.81%, topk_entropy_acc=89.81%, window_entropy_acc=90.51%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 1.9056089994861147
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 1.9056089994861147
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 3.9931076169013977
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 0.25
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 3.51953125
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 3.3137753009796143
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 3.0102608799934387
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 2.584209144115448
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate how ma...
    Score: 5.464801371097565
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 90.5311778290993
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.5311778290993
Method name: cer_prob_product_log_last, running accuracy: 89.83833718244803
Method name: self_consistency, running accuracy: 90.76212471131639
Method name: p_true, running accuracy: 90.99307159353349
Method name: normilized_likelihood, running accuracy: 90.06928406466513
Method name: normilized_entropy, running accuracy: 89.83833718244803
Method name: topk_entropy, running accuracy: 89.83833718244803
Method name: window_entropy, running accuracy: 90.5311778290993
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  86%|████████▋ | 432/500 [35:57:29<3:28:40, 184.12s/it, attention_weighted_confidence_acc=90.53%, cer_entropy_weighted_mean_all_acc=90.53%, cer_prob_product_log_last_acc=89.84%, self_consistency_acc=90.76%, p_true_acc=90.99%, normilized_likelihood_acc=90.07%, normilized_entropy_acc=89.84%, topk_entropy_acc=89.84%, window_entropy_acc=90.53%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 433/500 [35:57:29<4:36:13, 247.36s/it, attention_weighted_confidence_acc=90.53%, cer_entropy_weighted_mean_all_acc=90.53%, cer_prob_product_log_last_acc=89.84%, self_consistency_acc=90.76%, p_true_acc=90.99%, normilized_likelihood_acc=90.07%, normilized_entropy_acc=89.84%, topk_entropy_acc=89.84%, window_entropy_acc=90.53%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 7.510476432823725
    Answer: 9
    Ground truth:  9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 7.510476432823725
    Answer: 9
    Ground truth:  9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 14.99948513507843
    Answer: 9
    Ground truth:  9
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 0.9375
    Answer: 9
    Ground truth:  9
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 12.35546875
    Answer: 9
    Ground truth:  9
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 3.505729764699936
    Answer: 9
    Ground truth:  9
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 2.697186380624771
    Answer: 9
    Ground truth:  9
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 2.654732406139374
    Answer: 9
    Ground truth:  9
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we'll break it down into steps.

Step 1: Calculate the nu...
    Score: 2.8231775760650635
    Answer: 9
    Ground truth:  9
Method name: attention_weighted_confidence, running accuracy: 90.55299539170507
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.55299539170507
Method name: cer_prob_product_log_last, running accuracy: 89.86175115207374
Method name: self_consistency, running accuracy: 90.78341013824884
Method name: p_true, running accuracy: 91.01382488479263
Method name: normilized_likelihood, running accuracy: 90.09216589861751
Method name: normilized_entropy, running accuracy: 89.86175115207374
Method name: topk_entropy, running accuracy: 89.86175115207374
Method name: window_entropy, running accuracy: 90.55299539170507
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 433/500 [36:00:45<4:36:13, 247.36s/it, attention_weighted_confidence_acc=90.55%, cer_entropy_weighted_mean_all_acc=90.55%, cer_prob_product_log_last_acc=89.86%, self_consistency_acc=90.78%, p_true_acc=91.01%, normilized_likelihood_acc=90.09%, normilized_entropy_acc=89.86%, topk_entropy_acc=89.86%, window_entropy_acc=90.55%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 434/500 [36:00:45<4:15:05, 231.91s/it, attention_weighted_confidence_acc=90.55%, cer_entropy_weighted_mean_all_acc=90.55%, cer_prob_product_log_last_acc=89.86%, self_consistency_acc=90.78%, p_true_acc=91.01%, normilized_likelihood_acc=90.09%, normilized_entropy_acc=89.86%, topk_entropy_acc=89.86%, window_entropy_acc=90.55%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 7.6980710746246555
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 7.6980710746246555
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 15.995876550674438
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 13.4765625
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 3.087170660495758
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 1.7457347214221954
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 1.7227526605129242
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To find the average price Deepa paid per ticket, we need to determine the total ...
    Score: 4.8043166399002075
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 90.57471264367815
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.57471264367815
Method name: cer_prob_product_log_last, running accuracy: 89.88505747126436
Method name: self_consistency, running accuracy: 90.80459770114942
Method name: p_true, running accuracy: 91.0344827586207
Method name: normilized_likelihood, running accuracy: 90.11494252873563
Method name: normilized_entropy, running accuracy: 89.88505747126436
Method name: topk_entropy, running accuracy: 89.88505747126436
Method name: window_entropy, running accuracy: 90.57471264367815
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 434/500 [36:03:07<4:15:05, 231.91s/it, attention_weighted_confidence_acc=90.57%, cer_entropy_weighted_mean_all_acc=90.57%, cer_prob_product_log_last_acc=89.89%, self_consistency_acc=90.80%, p_true_acc=91.03%, normilized_likelihood_acc=90.11%, normilized_entropy_acc=89.89%, topk_entropy_acc=89.89%, window_entropy_acc=90.57%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 435/500 [36:03:07<3:42:12, 205.11s/it, attention_weighted_confidence_acc=90.57%, cer_entropy_weighted_mean_all_acc=90.57%, cer_prob_product_log_last_acc=89.89%, self_consistency_acc=90.80%, p_true_acc=91.03%, normilized_likelihood_acc=90.11%, normilized_entropy_acc=89.89%, topk_entropy_acc=89.89%, window_entropy_acc=90.57%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 7.9332934941584465
    Answer: 50
    Ground truth:  50
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 7.9332934941584465
    Answer: 50
    Ground truth:  50
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 14.994099915027618
    Answer: 50
    Ground truth:  50
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 0.9375
    Answer: 50
    Ground truth:  50
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 11.515625
    Answer: 50
    Ground truth:  50
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 8.319279968738556
    Answer: 50
    Ground truth:  50
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 7.449377968907356
    Answer: 50
    Ground truth:  50
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 6.957713484764099
    Answer: 50
    Ground truth:  50
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that one re...
    Score: 15.913346707820892
    Answer: 50
    Ground truth:  50
Method name: attention_weighted_confidence, running accuracy: 90.59633027522935
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.59633027522935
Method name: cer_prob_product_log_last, running accuracy: 89.90825688073394
Method name: self_consistency, running accuracy: 90.82568807339449
Method name: p_true, running accuracy: 91.05504587155964
Method name: normilized_likelihood, running accuracy: 90.13761467889908
Method name: normilized_entropy, running accuracy: 89.90825688073394
Method name: topk_entropy, running accuracy: 89.90825688073394
Method name: window_entropy, running accuracy: 90.59633027522935
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 435/500 [36:06:22<3:42:12, 205.11s/it, attention_weighted_confidence_acc=90.60%, cer_entropy_weighted_mean_all_acc=90.60%, cer_prob_product_log_last_acc=89.91%, self_consistency_acc=90.83%, p_true_acc=91.06%, normilized_likelihood_acc=90.14%, normilized_entropy_acc=89.91%, topk_entropy_acc=89.91%, window_entropy_acc=90.60%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 436/500 [36:06:22<3:35:19, 201.87s/it, attention_weighted_confidence_acc=90.60%, cer_entropy_weighted_mean_all_acc=90.60%, cer_prob_product_log_last_acc=89.91%, self_consistency_acc=90.83%, p_true_acc=91.06%, normilized_likelihood_acc=90.14%, normilized_entropy_acc=89.91%, topk_entropy_acc=89.91%, window_entropy_acc=90.60%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 3.6557601798111277
    Answer: 480
    Ground truth:  480
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 3.6557601798111277
    Answer: 480
    Ground truth:  480
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 6.999869346618652
    Answer: 480
    Ground truth:  480
Method 4: self_consistency
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 0.4375
    Answer: 480
    Ground truth:  480
Method 5: p_true
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 5.88671875
    Answer: 480
    Ground truth:  480
Method 6: normilized_likelihood
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 7.372456714510918
    Answer: 480
    Ground truth:  480
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 7.478170543909073
    Answer: 480
    Ground truth:  480
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 6.392957657575607
    Answer: 480
    Ground truth:  480
Method 9: window_entropy
  Batch 1:
    Text: To calculate the expected earnings of the dance studio in a month, we need to fi...
    Score: 10.25954407453537
    Answer: 480
    Ground truth:  480
Method name: attention_weighted_confidence, running accuracy: 90.61784897025171
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.61784897025171
Method name: cer_prob_product_log_last, running accuracy: 89.93135011441647
Method name: self_consistency, running accuracy: 90.8466819221968
Method name: p_true, running accuracy: 91.07551487414187
Method name: normilized_likelihood, running accuracy: 90.16018306636155
Method name: normilized_entropy, running accuracy: 89.93135011441647
Method name: topk_entropy, running accuracy: 89.93135011441647
Method name: window_entropy, running accuracy: 90.61784897025171
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 436/500 [36:10:38<3:35:19, 201.87s/it, attention_weighted_confidence_acc=90.62%, cer_entropy_weighted_mean_all_acc=90.62%, cer_prob_product_log_last_acc=89.93%, self_consistency_acc=90.85%, p_true_acc=91.08%, normilized_likelihood_acc=90.16%, normilized_entropy_acc=89.93%, topk_entropy_acc=89.93%, window_entropy_acc=90.62%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 437/500 [36:10:38<3:49:14, 218.33s/it, attention_weighted_confidence_acc=90.62%, cer_entropy_weighted_mean_all_acc=90.62%, cer_prob_product_log_last_acc=89.93%, self_consistency_acc=90.85%, p_true_acc=91.08%, normilized_likelihood_acc=90.16%, normilized_entropy_acc=89.93%, topk_entropy_acc=89.93%, window_entropy_acc=90.62%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 7.64146497773688
    Answer: 48
    Ground truth:  48
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 7.64146497773688
    Answer: 48
    Ground truth:  48
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 15.460077047348022
    Answer: 48
    Ground truth:  48
Method 4: self_consistency
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 1.0
    Answer: 48
    Ground truth:  48
Method 5: p_true
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 14.71484375
    Answer: 48
    Ground truth:  48
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 2.322696164250374
    Answer: 48
    Ground truth:  48
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 1.2851561307907104
    Answer: 48
    Ground truth:  48
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 1.2705552577972412
    Answer: 48
    Ground truth:  48
Method 9: window_entropy
  Batch 1:
    Text: To find the number of pieces of rolls fed to the chickens, we need to follow the...
    Score: 2.5124216079711914
    Answer: 48
    Ground truth:  48
Method name: attention_weighted_confidence, running accuracy: 90.6392694063927
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.6392694063927
Method name: cer_prob_product_log_last, running accuracy: 89.95433789954338
Method name: self_consistency, running accuracy: 90.8675799086758
Method name: p_true, running accuracy: 91.0958904109589
Method name: normilized_likelihood, running accuracy: 90.18264840182648
Method name: normilized_entropy, running accuracy: 89.95433789954338
Method name: topk_entropy, running accuracy: 89.95433789954338
Method name: window_entropy, running accuracy: 90.6392694063927
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  87%|████████▋ | 437/500 [36:12:53<3:49:14, 218.33s/it, attention_weighted_confidence_acc=90.64%, cer_entropy_weighted_mean_all_acc=90.64%, cer_prob_product_log_last_acc=89.95%, self_consistency_acc=90.87%, p_true_acc=91.10%, normilized_likelihood_acc=90.18%, normilized_entropy_acc=89.95%, topk_entropy_acc=89.95%, window_entropy_acc=90.64%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 438/500 [36:12:53<3:19:31, 193.09s/it, attention_weighted_confidence_acc=90.64%, cer_entropy_weighted_mean_all_acc=90.64%, cer_prob_product_log_last_acc=89.95%, self_consistency_acc=90.87%, p_true_acc=91.10%, normilized_likelihood_acc=90.18%, normilized_entropy_acc=89.95%, topk_entropy_acc=89.95%, window_entropy_acc=90.64%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 6.954688549406697
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 6.954688549406697
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 13.422308802604675
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 0.875
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 12.26953125
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 8.432234853506088
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 7.632985785603523
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 6.816676065325737
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: To find the total time John spent on his trip, we need to calculate the time tak...
    Score: 12.927508413791656
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 90.66059225512528
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.66059225512528
Method name: cer_prob_product_log_last, running accuracy: 89.97722095671982
Method name: self_consistency, running accuracy: 90.8883826879271
Method name: p_true, running accuracy: 91.11617312072893
Method name: normilized_likelihood, running accuracy: 90.20501138952164
Method name: normilized_entropy, running accuracy: 89.97722095671982
Method name: topk_entropy, running accuracy: 89.97722095671982
Method name: window_entropy, running accuracy: 90.66059225512528
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 438/500 [36:16:45<3:19:31, 193.09s/it, attention_weighted_confidence_acc=90.66%, cer_entropy_weighted_mean_all_acc=90.66%, cer_prob_product_log_last_acc=89.98%, self_consistency_acc=90.89%, p_true_acc=91.12%, normilized_likelihood_acc=90.21%, normilized_entropy_acc=89.98%, topk_entropy_acc=89.98%, window_entropy_acc=90.66%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 439/500 [36:16:45<3:28:19, 204.91s/it, attention_weighted_confidence_acc=90.66%, cer_entropy_weighted_mean_all_acc=90.66%, cer_prob_product_log_last_acc=89.98%, self_consistency_acc=90.89%, p_true_acc=91.12%, normilized_likelihood_acc=90.21%, normilized_entropy_acc=89.98%, topk_entropy_acc=89.98%, window_entropy_acc=90.66%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 2.1555867567818945
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 2.1555867567818945
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 4.481535494327545
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 0.3125
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 3.38818359375
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 5.360145777463913
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 5.319304049015045
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 4.418448448181152
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. The time elapsed from Bill's first sightin...
    Score: 5.698852956295013
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 90.68181818181819
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.68181818181819
Method name: cer_prob_product_log_last, running accuracy: 90.0
Method name: self_consistency, running accuracy: 90.9090909090909
Method name: p_true, running accuracy: 91.13636363636364
Method name: normilized_likelihood, running accuracy: 90.22727272727272
Method name: normilized_entropy, running accuracy: 90.0
Method name: topk_entropy, running accuracy: 90.0
Method name: window_entropy, running accuracy: 90.68181818181819
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 439/500 [36:23:25<3:28:19, 204.91s/it, attention_weighted_confidence_acc=90.68%, cer_entropy_weighted_mean_all_acc=90.68%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.91%, p_true_acc=91.14%, normilized_likelihood_acc=90.23%, normilized_entropy_acc=90.00%, topk_entropy_acc=90.00%, window_entropy_acc=90.68%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 440/500 [36:23:25<4:23:22, 263.37s/it, attention_weighted_confidence_acc=90.68%, cer_entropy_weighted_mean_all_acc=90.68%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.91%, p_true_acc=91.14%, normilized_likelihood_acc=90.23%, normilized_entropy_acc=90.00%, topk_entropy_acc=90.00%, window_entropy_acc=90.68%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 3.6603296655038697
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 3.6603296655038697
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 5.936200127005577
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 0.5
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 6.67578125
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 7.352525398135185
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 7.120693951845169
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 5.957986414432526
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. George spent $5 and...
    Score: 8.884940564632416
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 90.702947845805
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.702947845805
Method name: cer_prob_product_log_last, running accuracy: 90.02267573696146
Method name: self_consistency, running accuracy: 90.9297052154195
Method name: p_true, running accuracy: 91.15646258503402
Method name: normilized_likelihood, running accuracy: 90.24943310657596
Method name: normilized_entropy, running accuracy: 90.02267573696146
Method name: topk_entropy, running accuracy: 90.02267573696146
Method name: window_entropy, running accuracy: 90.702947845805
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 440/500 [36:30:14<4:23:22, 263.37s/it, attention_weighted_confidence_acc=90.70%, cer_entropy_weighted_mean_all_acc=90.70%, cer_prob_product_log_last_acc=90.02%, self_consistency_acc=90.93%, p_true_acc=91.16%, normilized_likelihood_acc=90.25%, normilized_entropy_acc=90.02%, topk_entropy_acc=90.02%, window_entropy_acc=90.70%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 441/500 [36:30:14<5:01:56, 307.06s/it, attention_weighted_confidence_acc=90.70%, cer_entropy_weighted_mean_all_acc=90.70%, cer_prob_product_log_last_acc=90.02%, self_consistency_acc=90.93%, p_true_acc=91.16%, normilized_likelihood_acc=90.25%, normilized_entropy_acc=90.02%, topk_entropy_acc=90.02%, window_entropy_acc=90.70%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 8.497726379373837
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 8.497726379373837
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 15.999287247657776
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 15.16015625
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 1.5201464891433716
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 1.2997357100248337
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 1.2958655208349228
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To find out how many more trees are in Harry's yard than Ferdinand's yard, we ne...
    Score: 2.2670565843582153
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 90.72398190045249
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.72398190045249
Method name: cer_prob_product_log_last, running accuracy: 90.04524886877829
Method name: self_consistency, running accuracy: 90.9502262443439
Method name: p_true, running accuracy: 91.17647058823529
Method name: normilized_likelihood, running accuracy: 90.27149321266968
Method name: normilized_entropy, running accuracy: 90.04524886877829
Method name: topk_entropy, running accuracy: 90.04524886877829
Method name: window_entropy, running accuracy: 90.72398190045249
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 441/500 [36:33:10<5:01:56, 307.06s/it, attention_weighted_confidence_acc=90.72%, cer_entropy_weighted_mean_all_acc=90.72%, cer_prob_product_log_last_acc=90.05%, self_consistency_acc=90.95%, p_true_acc=91.18%, normilized_likelihood_acc=90.27%, normilized_entropy_acc=90.05%, topk_entropy_acc=90.05%, window_entropy_acc=90.72%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 442/500 [36:33:10<4:18:42, 267.62s/it, attention_weighted_confidence_acc=90.72%, cer_entropy_weighted_mean_all_acc=90.72%, cer_prob_product_log_last_acc=90.05%, self_consistency_acc=90.95%, p_true_acc=91.18%, normilized_likelihood_acc=90.27%, normilized_entropy_acc=90.05%, topk_entropy_acc=90.05%, window_entropy_acc=90.72%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 8.210755592788855
    Answer: 17
    Ground truth:  17
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 8.210755592788855
    Answer: 17
    Ground truth:  17
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 15.999897241592407
    Answer: 17
    Ground truth:  17
Method 4: self_consistency
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 1.0
    Answer: 17
    Ground truth:  17
Method 5: p_true
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 14.6015625
    Answer: 17
    Ground truth:  17
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 1.6512292250990868
    Answer: 17
    Ground truth:  17
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 1.4684844464063644
    Answer: 17
    Ground truth:  17
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 1.4590017050504684
    Answer: 17
    Ground truth:  17
Method 9: window_entropy
  Batch 1:
    Text: To find out how many stickers Charlie has left, let's break down the process ste...
    Score: 6.029183357954025
    Answer: 17
    Ground truth:  17
Method name: attention_weighted_confidence, running accuracy: 90.74492099322799
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.74492099322799
Method name: cer_prob_product_log_last, running accuracy: 90.06772009029346
Method name: self_consistency, running accuracy: 90.9706546275395
Method name: p_true, running accuracy: 91.19638826185101
Method name: normilized_likelihood, running accuracy: 90.29345372460497
Method name: normilized_entropy, running accuracy: 90.06772009029346
Method name: topk_entropy, running accuracy: 90.06772009029346
Method name: window_entropy, running accuracy: 90.74492099322799
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  88%|████████▊ | 442/500 [36:35:25<4:18:42, 267.62s/it, attention_weighted_confidence_acc=90.74%, cer_entropy_weighted_mean_all_acc=90.74%, cer_prob_product_log_last_acc=90.07%, self_consistency_acc=90.97%, p_true_acc=91.20%, normilized_likelihood_acc=90.29%, normilized_entropy_acc=90.07%, topk_entropy_acc=90.07%, window_entropy_acc=90.74%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▊ | 443/500 [36:35:25<3:36:28, 227.87s/it, attention_weighted_confidence_acc=90.74%, cer_entropy_weighted_mean_all_acc=90.74%, cer_prob_product_log_last_acc=90.07%, self_consistency_acc=90.97%, p_true_acc=91.20%, normilized_likelihood_acc=90.29%, normilized_entropy_acc=90.07%, topk_entropy_acc=90.07%, window_entropy_acc=90.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 5.73074157287253
    Answer: 5
    Ground truth:  5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 5.73074157287253
    Answer: 5
    Ground truth:  5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 10.476950228214264
    Answer: 5
    Ground truth:  5
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 0.6875
    Answer: 5
    Ground truth:  5
Method 5: p_true
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 9.51171875
    Answer: 5
    Ground truth:  5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 13.80268932133913
    Answer: 5
    Ground truth:  5
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 13.71334958076477
    Answer: 5
    Ground truth:  5
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 11.895300537347794
    Answer: 5
    Ground truth:  5
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Kelly has left in her budget, we first need to calcul...
    Score: 21.643895000219345
    Answer: 5
    Ground truth:  5
Method name: attention_weighted_confidence, running accuracy: 90.76576576576578
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.76576576576578
Method name: cer_prob_product_log_last, running accuracy: 90.09009009009009
Method name: self_consistency, running accuracy: 90.990990990991
Method name: p_true, running accuracy: 91.21621621621621
Method name: normilized_likelihood, running accuracy: 90.31531531531532
Method name: normilized_entropy, running accuracy: 90.09009009009009
Method name: topk_entropy, running accuracy: 90.09009009009009
Method name: window_entropy, running accuracy: 90.76576576576578
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▊ | 443/500 [36:40:48<3:36:28, 227.87s/it, attention_weighted_confidence_acc=90.77%, cer_entropy_weighted_mean_all_acc=90.77%, cer_prob_product_log_last_acc=90.09%, self_consistency_acc=90.99%, p_true_acc=91.22%, normilized_likelihood_acc=90.32%, normilized_entropy_acc=90.09%, topk_entropy_acc=90.09%, window_entropy_acc=90.77%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 444/500 [36:40:48<3:59:25, 256.54s/it, attention_weighted_confidence_acc=90.77%, cer_entropy_weighted_mean_all_acc=90.77%, cer_prob_product_log_last_acc=90.09%, self_consistency_acc=90.99%, p_true_acc=91.22%, normilized_likelihood_acc=90.32%, normilized_entropy_acc=90.09%, topk_entropy_acc=90.09%, window_entropy_acc=90.77%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 7.87818065709126
    Answer: 1200
    Ground truth:  1200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 7.87818065709126
    Answer: 1200
    Ground truth:  1200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 7.907296419144496
    Answer: 1200
    Ground truth:  1200
Method 4: self_consistency
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 1.0
    Answer: 1200
    Ground truth:  1200
Method 5: p_true
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 12.58984375
    Answer: 1200
    Ground truth:  1200
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 1.7691629528999329
    Answer: 1200
    Ground truth:  1200
Method 7: normilized_entropy
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 1.2315485328435898
    Answer: 1200
    Ground truth:  1200
Method 8: topk_entropy
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 1.2317744046449661
    Answer: 1200
    Ground truth:  1200
Method 9: window_entropy
  Batch 1:
    Text: To find out the total amount of blood from ten sharks, we need to know how much ...
    Score: 3.228248357772827
    Answer: 1200
    Ground truth:  1200
Method name: attention_weighted_confidence, running accuracy: 90.78651685393258
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.78651685393258
Method name: cer_prob_product_log_last, running accuracy: 90.11235955056179
Method name: self_consistency, running accuracy: 91.01123595505618
Method name: p_true, running accuracy: 91.23595505617978
Method name: normilized_likelihood, running accuracy: 90.3370786516854
Method name: normilized_entropy, running accuracy: 90.11235955056179
Method name: topk_entropy, running accuracy: 90.11235955056179
Method name: window_entropy, running accuracy: 90.78651685393258
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 444/500 [36:43:06<3:59:25, 256.54s/it, attention_weighted_confidence_acc=90.79%, cer_entropy_weighted_mean_all_acc=90.79%, cer_prob_product_log_last_acc=90.11%, self_consistency_acc=91.01%, p_true_acc=91.24%, normilized_likelihood_acc=90.34%, normilized_entropy_acc=90.11%, topk_entropy_acc=90.11%, window_entropy_acc=90.79%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 445/500 [36:43:06<3:22:40, 221.10s/it, attention_weighted_confidence_acc=90.79%, cer_entropy_weighted_mean_all_acc=90.79%, cer_prob_product_log_last_acc=90.11%, self_consistency_acc=91.01%, p_true_acc=91.24%, normilized_likelihood_acc=90.34%, normilized_entropy_acc=90.11%, topk_entropy_acc=90.11%, window_entropy_acc=90.79%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 7.487150869799893
    Answer: 17
    Ground truth:  17
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 7.487150869799893
    Answer: 17
    Ground truth:  17
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 14.13418498635292
    Answer: 17
    Ground truth:  17
Method 4: self_consistency
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 0.9375
    Answer: 17
    Ground truth:  17
Method 5: p_true
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 13.95703125
    Answer: 17
    Ground truth:  17
Method 6: normilized_likelihood
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 13.630614817142487
    Answer: 17
    Ground truth:  17
Method 7: normilized_entropy
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 12.969533860683441
    Answer: 17
    Ground truth:  17
Method 8: topk_entropy
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 11.566734567284584
    Answer: 17
    Ground truth:  17
Method 9: window_entropy
  Batch 1:
    Text: 1. Let's start by defining the distances each person ran:
   - Paisley ran 4 mil...
    Score: 23.25715559720993
    Answer: 17
    Ground truth:  17
Method name: attention_weighted_confidence, running accuracy: 90.80717488789237
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.80717488789237
Method name: cer_prob_product_log_last, running accuracy: 90.13452914798206
Method name: self_consistency, running accuracy: 91.03139013452915
Method name: p_true, running accuracy: 91.25560538116592
Method name: normilized_likelihood, running accuracy: 90.35874439461884
Method name: normilized_entropy, running accuracy: 90.13452914798206
Method name: topk_entropy, running accuracy: 90.13452914798206
Method name: window_entropy, running accuracy: 90.80717488789237
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 445/500 [36:45:42<3:22:40, 221.10s/it, attention_weighted_confidence_acc=90.81%, cer_entropy_weighted_mean_all_acc=90.81%, cer_prob_product_log_last_acc=90.13%, self_consistency_acc=91.03%, p_true_acc=91.26%, normilized_likelihood_acc=90.36%, normilized_entropy_acc=90.13%, topk_entropy_acc=90.13%, window_entropy_acc=90.81%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 446/500 [36:45:42<3:01:17, 201.43s/it, attention_weighted_confidence_acc=90.81%, cer_entropy_weighted_mean_all_acc=90.81%, cer_prob_product_log_last_acc=90.13%, self_consistency_acc=91.03%, p_true_acc=91.26%, normilized_likelihood_acc=90.36%, normilized_entropy_acc=90.13%, topk_entropy_acc=90.13%, window_entropy_acc=90.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 7.815561291471876
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 7.815561291471876
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 15.999812483787537
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 1.0
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 14.90625
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 3.572569951415062
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 1.6457776427268982
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 1.6368855834007263
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: To find out how many elves are left, let's calculate the number of elves that qu...
    Score: 4.6644845604896545
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 90.82774049217002
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.82774049217002
Method name: cer_prob_product_log_last, running accuracy: 90.1565995525727
Method name: self_consistency, running accuracy: 91.05145413870245
Method name: p_true, running accuracy: 91.2751677852349
Method name: normilized_likelihood, running accuracy: 90.38031319910515
Method name: normilized_entropy, running accuracy: 90.1565995525727
Method name: topk_entropy, running accuracy: 90.1565995525727
Method name: window_entropy, running accuracy: 90.82774049217002
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 446/500 [36:47:58<3:01:17, 201.43s/it, attention_weighted_confidence_acc=90.83%, cer_entropy_weighted_mean_all_acc=90.83%, cer_prob_product_log_last_acc=90.16%, self_consistency_acc=91.05%, p_true_acc=91.28%, normilized_likelihood_acc=90.38%, normilized_entropy_acc=90.16%, topk_entropy_acc=90.16%, window_entropy_acc=90.83%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 447/500 [36:47:58<2:40:40, 181.90s/it, attention_weighted_confidence_acc=90.83%, cer_entropy_weighted_mean_all_acc=90.83%, cer_prob_product_log_last_acc=90.16%, self_consistency_acc=91.05%, p_true_acc=91.28%, normilized_likelihood_acc=90.38%, normilized_entropy_acc=90.16%, topk_entropy_acc=90.16%, window_entropy_acc=90.83%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 7.559169261554963
    Answer: 9
    Ground truth:  9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 7.559169261554963
    Answer: 9
    Ground truth:  9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 14.75725793838501
    Answer: 9
    Ground truth:  9
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 0.9375
    Answer: 9
    Ground truth:  9
Method 5: p_true
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 14.3984375
    Answer: 9
    Ground truth:  9
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 4.388346418738365
    Answer: 9
    Ground truth:  9
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 4.885016843676567
    Answer: 9
    Ground truth:  9
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 4.660823658108711
    Answer: 9
    Ground truth:  9
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step to solve the problem.

Let's assume that Sally has 'x' ...
    Score: 13.754036486148834
    Answer: 9
    Ground truth:  9
Method name: attention_weighted_confidence, running accuracy: 90.84821428571429
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.84821428571429
Method name: cer_prob_product_log_last, running accuracy: 90.17857142857143
Method name: self_consistency, running accuracy: 91.07142857142857
Method name: p_true, running accuracy: 91.29464285714286
Method name: normilized_likelihood, running accuracy: 90.40178571428571
Method name: normilized_entropy, running accuracy: 90.17857142857143
Method name: topk_entropy, running accuracy: 90.17857142857143
Method name: window_entropy, running accuracy: 90.84821428571429
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  89%|████████▉ | 447/500 [36:50:42<2:40:40, 181.90s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.18%, self_consistency_acc=91.07%, p_true_acc=91.29%, normilized_likelihood_acc=90.40%, normilized_entropy_acc=90.18%, topk_entropy_acc=90.18%, window_entropy_acc=90.85%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|████████▉ | 448/500 [36:50:42<2:32:51, 176.37s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.18%, self_consistency_acc=91.07%, p_true_acc=91.29%, normilized_likelihood_acc=90.40%, normilized_entropy_acc=90.18%, topk_entropy_acc=90.18%, window_entropy_acc=90.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 7.310945040775907
    Answer: 82
    Ground truth:  82
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 7.310945040775907
    Answer: 82
    Ground truth:  82
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 14.992286503314972
    Answer: 82
    Ground truth:  82
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 0.9375
    Answer: 82
    Ground truth:  82
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 10.498046875
    Answer: 82
    Ground truth:  82
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 6.102010667324066
    Answer: 82
    Ground truth:  82
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 5.651358276605606
    Answer: 82
    Ground truth:  82
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 5.3080264031887054
    Answer: 82
    Ground truth:  82
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

Step 1: The initial angl...
    Score: 11.142844259738922
    Answer: 82
    Ground truth:  82
Method name: attention_weighted_confidence, running accuracy: 90.8685968819599
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.8685968819599
Method name: cer_prob_product_log_last, running accuracy: 90.20044543429844
Method name: self_consistency, running accuracy: 91.0913140311804
Method name: p_true, running accuracy: 91.31403118040089
Method name: normilized_likelihood, running accuracy: 90.42316258351893
Method name: normilized_entropy, running accuracy: 90.20044543429844
Method name: topk_entropy, running accuracy: 90.20044543429844
Method name: window_entropy, running accuracy: 90.8685968819599
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|████████▉ | 448/500 [36:53:39<2:32:51, 176.37s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.20%, self_consistency_acc=91.09%, p_true_acc=91.31%, normilized_likelihood_acc=90.42%, normilized_entropy_acc=90.20%, topk_entropy_acc=90.20%, window_entropy_acc=90.87%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|████████▉ | 449/500 [36:53:39<2:30:09, 176.65s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.20%, self_consistency_acc=91.09%, p_true_acc=91.31%, normilized_likelihood_acc=90.42%, normilized_entropy_acc=90.20%, topk_entropy_acc=90.20%, window_entropy_acc=90.87%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 6.4350811697644765
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 6.4350811697644765
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 12.000998979317956
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 0.8125
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 11.28125
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 2.409723162651062
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 2.6401945054531097
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 2.617897540330887
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. First, we need to convert the cur...
    Score: 7.334763050079346
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 90.88888888888889
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.88888888888889
Method name: cer_prob_product_log_last, running accuracy: 90.22222222222223
Method name: self_consistency, running accuracy: 91.11111111111111
Method name: p_true, running accuracy: 91.33333333333333
Method name: normilized_likelihood, running accuracy: 90.44444444444444
Method name: normilized_entropy, running accuracy: 90.22222222222223
Method name: topk_entropy, running accuracy: 90.22222222222223
Method name: window_entropy, running accuracy: 90.88888888888889
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|████████▉ | 449/500 [36:56:21<2:30:09, 176.65s/it, attention_weighted_confidence_acc=90.89%, cer_entropy_weighted_mean_all_acc=90.89%, cer_prob_product_log_last_acc=90.22%, self_consistency_acc=91.11%, p_true_acc=91.33%, normilized_likelihood_acc=90.44%, normilized_entropy_acc=90.22%, topk_entropy_acc=90.22%, window_entropy_acc=90.89%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|█████████ | 450/500 [36:56:21<2:23:31, 172.22s/it, attention_weighted_confidence_acc=90.89%, cer_entropy_weighted_mean_all_acc=90.89%, cer_prob_product_log_last_acc=90.22%, self_consistency_acc=91.11%, p_true_acc=91.33%, normilized_likelihood_acc=90.44%, normilized_entropy_acc=90.22%, topk_entropy_acc=90.22%, window_entropy_acc=90.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 6.35499733457251
    Answer: 20
    Ground truth:  20
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 6.35499733457251
    Answer: 20
    Ground truth:  20
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 12.999854326248169
    Answer: 20
    Ground truth:  20
Method 4: self_consistency
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 0.8125
    Answer: 20
    Ground truth:  20
Method 5: p_true
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 6.587890625
    Answer: 20
    Ground truth:  20
Method 6: normilized_likelihood
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 9.156020909547806
    Answer: 20
    Ground truth:  20
Method 7: normilized_entropy
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 9.294077306985855
    Answer: 20
    Ground truth:  20
Method 8: topk_entropy
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 8.262253642082214
    Answer: 20
    Ground truth:  20
Method 9: window_entropy
  Batch 1:
    Text: Since the last name mentioned in the problem is Kim, not Lucas, I will continue ...
    Score: 15.943113386631012
    Answer: 20
    Ground truth:  20
Method name: attention_weighted_confidence, running accuracy: 90.9090909090909
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.9090909090909
Method name: cer_prob_product_log_last, running accuracy: 90.2439024390244
Method name: self_consistency, running accuracy: 91.13082039911308
Method name: p_true, running accuracy: 91.35254988913526
Method name: normilized_likelihood, running accuracy: 90.46563192904657
Method name: normilized_entropy, running accuracy: 90.2439024390244
Method name: topk_entropy, running accuracy: 90.2439024390244
Method name: window_entropy, running accuracy: 90.9090909090909
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|█████████ | 450/500 [37:00:36<2:23:31, 172.22s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.24%, self_consistency_acc=91.13%, p_true_acc=91.35%, normilized_likelihood_acc=90.47%, normilized_entropy_acc=90.24%, topk_entropy_acc=90.24%, window_entropy_acc=90.91%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|█████████ | 451/500 [37:00:36<2:41:02, 197.19s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.24%, self_consistency_acc=91.13%, p_true_acc=91.35%, normilized_likelihood_acc=90.47%, normilized_entropy_acc=90.24%, topk_entropy_acc=90.24%, window_entropy_acc=90.91%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 2.0448728916728083
    Answer: 6
    Ground truth:  6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 2.0448728916728083
    Answer: 6
    Ground truth:  6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 3.7015239000320435
    Answer: 6
    Ground truth:  6
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 0.25
    Answer: 6
    Ground truth:  6
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 3.6796875
    Answer: 6
    Ground truth:  6
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 3.4343506693840027
    Answer: 6
    Ground truth:  6
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 3.4482690393924713
    Answer: 6
    Ground truth:  6
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 3.041010022163391
    Answer: 6
    Ground truth:  6
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step.

Step 1: Calculate the total number of stuffed...
    Score: 6.187102496623993
    Answer: 6
    Ground truth:  6
Method name: attention_weighted_confidence, running accuracy: 90.929203539823
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.929203539823
Method name: cer_prob_product_log_last, running accuracy: 90.2654867256637
Method name: self_consistency, running accuracy: 91.1504424778761
Method name: p_true, running accuracy: 91.3716814159292
Method name: normilized_likelihood, running accuracy: 90.48672566371681
Method name: normilized_entropy, running accuracy: 90.2654867256637
Method name: topk_entropy, running accuracy: 90.2654867256637
Method name: window_entropy, running accuracy: 90.929203539823
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|█████████ | 451/500 [37:05:57<2:41:02, 197.19s/it, attention_weighted_confidence_acc=90.93%, cer_entropy_weighted_mean_all_acc=90.93%, cer_prob_product_log_last_acc=90.27%, self_consistency_acc=91.15%, p_true_acc=91.37%, normilized_likelihood_acc=90.49%, normilized_entropy_acc=90.27%, topk_entropy_acc=90.27%, window_entropy_acc=90.93%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|█████████ | 452/500 [37:05:57<3:07:17, 234.11s/it, attention_weighted_confidence_acc=90.93%, cer_entropy_weighted_mean_all_acc=90.93%, cer_prob_product_log_last_acc=90.27%, self_consistency_acc=91.15%, p_true_acc=91.37%, normilized_likelihood_acc=90.49%, normilized_entropy_acc=90.27%, topk_entropy_acc=90.27%, window_entropy_acc=90.93%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 4.841759854863879
    Answer: 655
    Ground truth:  655
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 4.841759854863879
    Answer: 655
    Ground truth:  655
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 9.999483227729797
    Answer: 655
    Ground truth:  655
Method 4: self_consistency
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 0.625
    Answer: 655
    Ground truth:  655
Method 5: p_true
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 7.84765625
    Answer: 655
    Ground truth:  655
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 3.4024706184864044
    Answer: 655
    Ground truth:  655
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 4.083800062537193
    Answer: 655
    Ground truth:  655
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 3.844755455851555
    Answer: 655
    Ground truth:  655
Method 9: window_entropy
  Batch 1:
    Text: To find out how many kilometers Bernice still needs to drive, we need to first d...
    Score: 9.276780664920807
    Answer: 655
    Ground truth:  655
Method name: attention_weighted_confidence, running accuracy: 90.94922737306842
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.94922737306842
Method name: cer_prob_product_log_last, running accuracy: 90.28697571743929
Method name: self_consistency, running accuracy: 91.16997792494482
Method name: p_true, running accuracy: 91.3907284768212
Method name: normilized_likelihood, running accuracy: 90.50772626931567
Method name: normilized_entropy, running accuracy: 90.28697571743929
Method name: topk_entropy, running accuracy: 90.28697571743929
Method name: window_entropy, running accuracy: 90.94922737306842
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  90%|█████████ | 452/500 [37:08:16<3:07:17, 234.11s/it, attention_weighted_confidence_acc=90.95%, cer_entropy_weighted_mean_all_acc=90.95%, cer_prob_product_log_last_acc=90.29%, self_consistency_acc=91.17%, p_true_acc=91.39%, normilized_likelihood_acc=90.51%, normilized_entropy_acc=90.29%, topk_entropy_acc=90.29%, window_entropy_acc=90.95%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 453/500 [37:08:16<2:41:07, 205.69s/it, attention_weighted_confidence_acc=90.95%, cer_entropy_weighted_mean_all_acc=90.95%, cer_prob_product_log_last_acc=90.29%, self_consistency_acc=91.17%, p_true_acc=91.39%, normilized_likelihood_acc=90.51%, normilized_entropy_acc=90.29%, topk_entropy_acc=90.29%, window_entropy_acc=90.95%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 7.874717467881242
    Answer: 45
    Ground truth:  45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 7.874717467881242
    Answer: 45
    Ground truth:  45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 15.99992287158966
    Answer: 45
    Ground truth:  45
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 1.0
    Answer: 45
    Ground truth:  45
Method 5: p_true
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 13.5546875
    Answer: 45
    Ground truth:  45
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 1.0151296555995941
    Answer: 45
    Ground truth:  45
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 0.6862505078315735
    Answer: 45
    Ground truth:  45
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 0.6915813535451889
    Answer: 45
    Ground truth:  45
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Calculate the number of sheep that produce 1 kg of milk and the number t...
    Score: 3.311465322971344
    Answer: 45
    Ground truth:  45
Method name: attention_weighted_confidence, running accuracy: 90.96916299559471
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.96916299559471
Method name: cer_prob_product_log_last, running accuracy: 90.30837004405286
Method name: self_consistency, running accuracy: 91.18942731277532
Method name: p_true, running accuracy: 91.40969162995594
Method name: normilized_likelihood, running accuracy: 90.52863436123349
Method name: normilized_entropy, running accuracy: 90.30837004405286
Method name: topk_entropy, running accuracy: 90.30837004405286
Method name: window_entropy, running accuracy: 90.96916299559471
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 453/500 [37:11:03<2:41:07, 205.69s/it, attention_weighted_confidence_acc=90.97%, cer_entropy_weighted_mean_all_acc=90.97%, cer_prob_product_log_last_acc=90.31%, self_consistency_acc=91.19%, p_true_acc=91.41%, normilized_likelihood_acc=90.53%, normilized_entropy_acc=90.31%, topk_entropy_acc=90.31%, window_entropy_acc=90.97%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 454/500 [37:11:03<2:28:50, 194.14s/it, attention_weighted_confidence_acc=90.97%, cer_entropy_weighted_mean_all_acc=90.97%, cer_prob_product_log_last_acc=90.31%, self_consistency_acc=91.19%, p_true_acc=91.41%, normilized_likelihood_acc=90.53%, normilized_entropy_acc=90.31%, topk_entropy_acc=90.31%, window_entropy_acc=90.97%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 7.708801416093106
    Answer: 3
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 7.708801416093106
    Answer: 3
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 14.84725570678711
    Answer: 3
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 0.9375
    Answer: 3
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 13.85546875
    Answer: 3
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 1.6187320202589035
    Answer: 3
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 1.3426005840301514
    Answer: 3
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 1.3352130651474
    Answer: 3
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find the amount of change Theo received, we need to determine the total cost ...
    Score: 2.5097323060035706
    Answer: 3
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 90.98901098901099
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.98901098901099
Method name: cer_prob_product_log_last, running accuracy: 90.32967032967034
Method name: self_consistency, running accuracy: 91.20879120879121
Method name: p_true, running accuracy: 91.42857142857143
Method name: normilized_likelihood, running accuracy: 90.54945054945055
Method name: normilized_entropy, running accuracy: 90.32967032967034
Method name: topk_entropy, running accuracy: 90.32967032967034
Method name: window_entropy, running accuracy: 90.98901098901099
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 454/500 [37:13:46<2:28:50, 194.14s/it, attention_weighted_confidence_acc=90.99%, cer_entropy_weighted_mean_all_acc=90.99%, cer_prob_product_log_last_acc=90.33%, self_consistency_acc=91.21%, p_true_acc=91.43%, normilized_likelihood_acc=90.55%, normilized_entropy_acc=90.33%, topk_entropy_acc=90.33%, window_entropy_acc=90.99%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 455/500 [37:13:46<2:18:38, 184.84s/it, attention_weighted_confidence_acc=90.99%, cer_entropy_weighted_mean_all_acc=90.99%, cer_prob_product_log_last_acc=90.33%, self_consistency_acc=91.21%, p_true_acc=91.43%, normilized_likelihood_acc=90.55%, normilized_entropy_acc=90.33%, topk_entropy_acc=90.33%, window_entropy_acc=90.99%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 8.554060137408912
    Answer: 3200
    Ground truth:  3200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 8.554060137408912
    Answer: 3200
    Ground truth:  3200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 7.999113470501758
    Answer: 3200
    Ground truth:  3200
Method 4: self_consistency
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 1.0
    Answer: 3200
    Ground truth:  3200
Method 5: p_true
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 11.3876953125
    Answer: 3200
    Ground truth:  3200
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 1.013171747326851
    Answer: 3200
    Ground truth:  3200
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 0.634186141192913
    Answer: 3200
    Ground truth:  3200
Method 8: topk_entropy
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 0.633965864777565
    Answer: 3200
    Ground truth:  3200
Method 9: window_entropy
  Batch 1:
    Text: To find the total amount spent on the seeds, we need to calculate the cost of th...
    Score: 1.726585030555725
    Answer: 3200
    Ground truth:  3200
Method name: attention_weighted_confidence, running accuracy: 91.00877192982456
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.00877192982456
Method name: cer_prob_product_log_last, running accuracy: 90.35087719298247
Method name: self_consistency, running accuracy: 91.22807017543859
Method name: p_true, running accuracy: 91.44736842105263
Method name: normilized_likelihood, running accuracy: 90.5701754385965
Method name: normilized_entropy, running accuracy: 90.35087719298247
Method name: topk_entropy, running accuracy: 90.35087719298247
Method name: window_entropy, running accuracy: 91.00877192982456
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 455/500 [37:16:51<2:18:38, 184.84s/it, attention_weighted_confidence_acc=91.01%, cer_entropy_weighted_mean_all_acc=91.01%, cer_prob_product_log_last_acc=90.35%, self_consistency_acc=91.23%, p_true_acc=91.45%, normilized_likelihood_acc=90.57%, normilized_entropy_acc=90.35%, topk_entropy_acc=90.35%, window_entropy_acc=91.01%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 456/500 [37:16:51<2:15:27, 184.71s/it, attention_weighted_confidence_acc=91.01%, cer_entropy_weighted_mean_all_acc=91.01%, cer_prob_product_log_last_acc=90.35%, self_consistency_acc=91.23%, p_true_acc=91.45%, normilized_likelihood_acc=90.57%, normilized_entropy_acc=90.35%, topk_entropy_acc=90.35%, window_entropy_acc=91.01%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 8.606237273560277
    Answer: 17
    Ground truth:  17
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 8.606237273560277
    Answer: 17
    Ground truth:  17
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 15.929439008235931
    Answer: 17
    Ground truth:  17
Method 4: self_consistency
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 1.0
    Answer: 17
    Ground truth:  17
Method 5: p_true
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 13.5
    Answer: 17
    Ground truth:  17
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 1.4321394860744476
    Answer: 17
    Ground truth:  17
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 0.36795438826084137
    Answer: 17
    Ground truth:  17
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 0.36965371668338776
    Answer: 17
    Ground truth:  17
Method 9: window_entropy
  Batch 1:
    Text: To find out how much change Carly gets back, we first need to calculate the tota...
    Score: 2.0858412384986877
    Answer: 17
    Ground truth:  17
Method name: attention_weighted_confidence, running accuracy: 91.02844638949672
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.02844638949672
Method name: cer_prob_product_log_last, running accuracy: 90.37199124726477
Method name: self_consistency, running accuracy: 91.2472647702407
Method name: p_true, running accuracy: 91.46608315098467
Method name: normilized_likelihood, running accuracy: 90.59080962800876
Method name: normilized_entropy, running accuracy: 90.37199124726477
Method name: topk_entropy, running accuracy: 90.37199124726477
Method name: window_entropy, running accuracy: 91.02844638949672
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████ | 456/500 [37:19:46<2:15:27, 184.71s/it, attention_weighted_confidence_acc=91.03%, cer_entropy_weighted_mean_all_acc=91.03%, cer_prob_product_log_last_acc=90.37%, self_consistency_acc=91.25%, p_true_acc=91.47%, normilized_likelihood_acc=90.59%, normilized_entropy_acc=90.37%, topk_entropy_acc=90.37%, window_entropy_acc=91.03%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████▏| 457/500 [37:19:46<2:10:18, 181.81s/it, attention_weighted_confidence_acc=91.03%, cer_entropy_weighted_mean_all_acc=91.03%, cer_prob_product_log_last_acc=90.37%, self_consistency_acc=91.25%, p_true_acc=91.47%, normilized_likelihood_acc=90.59%, normilized_entropy_acc=90.37%, topk_entropy_acc=90.37%, window_entropy_acc=91.03%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 7.874787346807761
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 7.874787346807761
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 15.913608014583588
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 1.0
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 13.7421875
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 2.1164882630109787
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 1.3826950043439865
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 1.371540054678917
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find out how many candy bars Marissa bought for Jimmy, we first need to find ...
    Score: 5.565592885017395
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 91.0480349344978
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.0480349344978
Method name: cer_prob_product_log_last, running accuracy: 90.39301310043668
Method name: self_consistency, running accuracy: 91.26637554585153
Method name: p_true, running accuracy: 91.48471615720524
Method name: normilized_likelihood, running accuracy: 90.61135371179039
Method name: normilized_entropy, running accuracy: 90.39301310043668
Method name: topk_entropy, running accuracy: 90.39301310043668
Method name: window_entropy, running accuracy: 91.0480349344978
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  91%|█████████▏| 457/500 [37:21:57<2:10:18, 181.81s/it, attention_weighted_confidence_acc=91.05%, cer_entropy_weighted_mean_all_acc=91.05%, cer_prob_product_log_last_acc=90.39%, self_consistency_acc=91.27%, p_true_acc=91.48%, normilized_likelihood_acc=90.61%, normilized_entropy_acc=90.39%, topk_entropy_acc=90.39%, window_entropy_acc=91.05%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 458/500 [37:21:57<1:56:35, 166.57s/it, attention_weighted_confidence_acc=91.05%, cer_entropy_weighted_mean_all_acc=91.05%, cer_prob_product_log_last_acc=90.39%, self_consistency_acc=91.27%, p_true_acc=91.48%, normilized_likelihood_acc=90.61%, normilized_entropy_acc=90.39%, topk_entropy_acc=90.39%, window_entropy_acc=91.05%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 4.742131750566948
    Answer: 147
    Ground truth:  147
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 4.742131750566948
    Answer: 147
    Ground truth:  147
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 8.95784479379654
    Answer: 147
    Ground truth:  147
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 0.5625
    Answer: 147
    Ground truth:  147
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 8.53125
    Answer: 147
    Ground truth:  147
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 5.219405546784401
    Answer: 147
    Ground truth:  147
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 4.825186610221863
    Answer: 147
    Ground truth:  147
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 4.333911135792732
    Answer: 147
    Ground truth:  147
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into steps.

Step 1: Let's assign va...
    Score: 15.718920826911926
    Answer: 147
    Ground truth:  147
Method name: attention_weighted_confidence, running accuracy: 91.06753812636165
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.06753812636165
Method name: cer_prob_product_log_last, running accuracy: 90.41394335511983
Method name: self_consistency, running accuracy: 91.28540305010894
Method name: p_true, running accuracy: 91.50326797385621
Method name: normilized_likelihood, running accuracy: 90.6318082788671
Method name: normilized_entropy, running accuracy: 90.41394335511983
Method name: topk_entropy, running accuracy: 90.41394335511983
Method name: window_entropy, running accuracy: 91.06753812636165
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 458/500 [37:27:15<1:56:35, 166.57s/it, attention_weighted_confidence_acc=91.07%, cer_entropy_weighted_mean_all_acc=91.07%, cer_prob_product_log_last_acc=90.41%, self_consistency_acc=91.29%, p_true_acc=91.50%, normilized_likelihood_acc=90.63%, normilized_entropy_acc=90.41%, topk_entropy_acc=90.41%, window_entropy_acc=91.07%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 459/500 [37:27:15<2:24:52, 212.01s/it, attention_weighted_confidence_acc=91.07%, cer_entropy_weighted_mean_all_acc=91.07%, cer_prob_product_log_last_acc=90.41%, self_consistency_acc=91.29%, p_true_acc=91.50%, normilized_likelihood_acc=90.63%, normilized_entropy_acc=90.41%, topk_entropy_acc=90.41%, window_entropy_acc=91.07%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 6.535309355776493
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 6.535309355776493
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 12.998801946640015
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 0.8125
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 10.48046875
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 11.610777333378792
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 10.126151025295258
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 9.00389176607132
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: Step 1: First, we need to determine how many boxes Nik will need to place all of...
    Score: 17.16736215353012
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 91.08695652173913
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.08695652173913
Method name: cer_prob_product_log_last, running accuracy: 90.43478260869566
Method name: self_consistency, running accuracy: 91.30434782608695
Method name: p_true, running accuracy: 91.52173913043478
Method name: normilized_likelihood, running accuracy: 90.65217391304347
Method name: normilized_entropy, running accuracy: 90.43478260869566
Method name: topk_entropy, running accuracy: 90.43478260869566
Method name: window_entropy, running accuracy: 91.08695652173913
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 459/500 [37:31:57<2:24:52, 212.01s/it, attention_weighted_confidence_acc=91.09%, cer_entropy_weighted_mean_all_acc=91.09%, cer_prob_product_log_last_acc=90.43%, self_consistency_acc=91.30%, p_true_acc=91.52%, normilized_likelihood_acc=90.65%, normilized_entropy_acc=90.43%, topk_entropy_acc=90.43%, window_entropy_acc=91.09%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 460/500 [37:31:57<2:35:24, 233.11s/it, attention_weighted_confidence_acc=91.09%, cer_entropy_weighted_mean_all_acc=91.09%, cer_prob_product_log_last_acc=90.43%, self_consistency_acc=91.30%, p_true_acc=91.52%, normilized_likelihood_acc=90.65%, normilized_entropy_acc=90.43%, topk_entropy_acc=90.43%, window_entropy_acc=91.09%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 7.965422632155672
    Answer: 64
    Ground truth:  64
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 7.965422632155672
    Answer: 64
    Ground truth:  64
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 15.988842785358429
    Answer: 64
    Ground truth:  64
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 1.0
    Answer: 64
    Ground truth:  64
Method 5: p_true
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 14.67578125
    Answer: 64
    Ground truth:  64
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 3.19351689517498
    Answer: 64
    Ground truth:  64
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 2.0355665385723114
    Answer: 64
    Ground truth:  64
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 2.0104214251041412
    Answer: 64
    Ground truth:  64
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Colby earns in a typical eight-hour workday, we need ...
    Score: 5.232449471950531
    Answer: 64
    Ground truth:  64
Method name: attention_weighted_confidence, running accuracy: 91.1062906724512
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.1062906724512
Method name: cer_prob_product_log_last, running accuracy: 90.45553145336225
Method name: self_consistency, running accuracy: 91.3232104121475
Method name: p_true, running accuracy: 91.54013015184381
Method name: normilized_likelihood, running accuracy: 90.67245119305856
Method name: normilized_entropy, running accuracy: 90.45553145336225
Method name: topk_entropy, running accuracy: 90.45553145336225
Method name: window_entropy, running accuracy: 91.1062906724512
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 460/500 [37:34:48<2:35:24, 233.11s/it, attention_weighted_confidence_acc=91.11%, cer_entropy_weighted_mean_all_acc=91.11%, cer_prob_product_log_last_acc=90.46%, self_consistency_acc=91.32%, p_true_acc=91.54%, normilized_likelihood_acc=90.67%, normilized_entropy_acc=90.46%, topk_entropy_acc=90.46%, window_entropy_acc=91.11%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 461/500 [37:34:48<2:19:15, 214.25s/it, attention_weighted_confidence_acc=91.11%, cer_entropy_weighted_mean_all_acc=91.11%, cer_prob_product_log_last_acc=90.46%, self_consistency_acc=91.32%, p_true_acc=91.54%, normilized_likelihood_acc=90.67%, normilized_entropy_acc=90.46%, topk_entropy_acc=90.46%, window_entropy_acc=91.11%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 5.8763411594128945
    Answer: 35
    Ground truth:  35
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 5.8763411594128945
    Answer: 35
    Ground truth:  35
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 11.989225089550018
    Answer: 35
    Ground truth:  35
Method 4: self_consistency
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 0.75
    Answer: 35
    Ground truth:  35
Method 5: p_true
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 11.42578125
    Answer: 35
    Ground truth:  35
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 8.3575619161129
    Answer: 35
    Ground truth:  35
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 8.306288123130798
    Answer: 35
    Ground truth:  35
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 7.359117835760117
    Answer: 35
    Ground truth:  35
Method 9: window_entropy
  Batch 1:
    Text: To find out how many questions Audrey needs to get right on the second test, we ...
    Score: 12.940034806728363
    Answer: 35
    Ground truth:  35
Method name: attention_weighted_confidence, running accuracy: 91.12554112554112
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.12554112554112
Method name: cer_prob_product_log_last, running accuracy: 90.47619047619048
Method name: self_consistency, running accuracy: 91.34199134199135
Method name: p_true, running accuracy: 91.55844155844156
Method name: normilized_likelihood, running accuracy: 90.6926406926407
Method name: normilized_entropy, running accuracy: 90.47619047619048
Method name: topk_entropy, running accuracy: 90.47619047619048
Method name: window_entropy, running accuracy: 91.12554112554112
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 461/500 [37:39:00<2:19:15, 214.25s/it, attention_weighted_confidence_acc=91.13%, cer_entropy_weighted_mean_all_acc=91.13%, cer_prob_product_log_last_acc=90.48%, self_consistency_acc=91.34%, p_true_acc=91.56%, normilized_likelihood_acc=90.69%, normilized_entropy_acc=90.48%, topk_entropy_acc=90.48%, window_entropy_acc=91.13%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 462/500 [37:39:00<2:23:01, 225.84s/it, attention_weighted_confidence_acc=91.13%, cer_entropy_weighted_mean_all_acc=91.13%, cer_prob_product_log_last_acc=90.48%, self_consistency_acc=91.34%, p_true_acc=91.56%, normilized_likelihood_acc=90.69%, normilized_entropy_acc=90.48%, topk_entropy_acc=90.48%, window_entropy_acc=91.13%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 7.651032833220991
    Answer: 1000
    Ground truth:  1000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 7.651032833220991
    Answer: 1000
    Ground truth:  1000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 7.170095938270974
    Answer: 1000
    Ground truth:  1000
Method 4: self_consistency
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 0.9375
    Answer: 1000
    Ground truth:  1000
Method 5: p_true
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 14.04296875
    Answer: 1000
    Ground truth:  1000
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 4.694538950920105
    Answer: 1000
    Ground truth:  1000
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 5.168569654226303
    Answer: 1000
    Ground truth:  1000
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 4.5235289335250854
    Answer: 1000
    Ground truth:  1000
Method 9: window_entropy
  Batch 1:
    Text: To find the number of people who had not voted by 16:00, we will first calculate...
    Score: 19.50704997777939
    Answer: 1000
    Ground truth:  1000
Method name: attention_weighted_confidence, running accuracy: 91.14470842332614
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.14470842332614
Method name: cer_prob_product_log_last, running accuracy: 90.49676025917927
Method name: self_consistency, running accuracy: 91.36069114470843
Method name: p_true, running accuracy: 91.5766738660907
Method name: normilized_likelihood, running accuracy: 90.71274298056156
Method name: normilized_entropy, running accuracy: 90.49676025917927
Method name: topk_entropy, running accuracy: 90.49676025917927
Method name: window_entropy, running accuracy: 91.14470842332614
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  92%|█████████▏| 462/500 [37:43:21<2:23:01, 225.84s/it, attention_weighted_confidence_acc=91.14%, cer_entropy_weighted_mean_all_acc=91.14%, cer_prob_product_log_last_acc=90.50%, self_consistency_acc=91.36%, p_true_acc=91.58%, normilized_likelihood_acc=90.71%, normilized_entropy_acc=90.50%, topk_entropy_acc=90.50%, window_entropy_acc=91.14%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 463/500 [37:43:21<2:25:41, 236.26s/it, attention_weighted_confidence_acc=91.14%, cer_entropy_weighted_mean_all_acc=91.14%, cer_prob_product_log_last_acc=90.50%, self_consistency_acc=91.36%, p_true_acc=91.58%, normilized_likelihood_acc=90.71%, normilized_entropy_acc=90.50%, topk_entropy_acc=90.50%, window_entropy_acc=91.14%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 6.564881680746385
    Answer: 240
    Ground truth:  240
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 6.564881680746385
    Answer: 240
    Ground truth:  240
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 12.99969094991684
    Answer: 240
    Ground truth:  240
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 0.8125
    Answer: 240
    Ground truth:  240
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 12.68359375
    Answer: 240
    Ground truth:  240
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 3.3708464801311493
    Answer: 240
    Ground truth:  240
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 2.2589669823646545
    Answer: 240
    Ground truth:  240
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 2.1633125245571136
    Answer: 240
    Ground truth:  240
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. March has 31 days, ...
    Score: 11.601575553417206
    Answer: 240
    Ground truth:  240
Method name: attention_weighted_confidence, running accuracy: 91.16379310344827
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.16379310344827
Method name: cer_prob_product_log_last, running accuracy: 90.51724137931035
Method name: self_consistency, running accuracy: 91.37931034482759
Method name: p_true, running accuracy: 91.59482758620689
Method name: normilized_likelihood, running accuracy: 90.73275862068965
Method name: normilized_entropy, running accuracy: 90.51724137931035
Method name: topk_entropy, running accuracy: 90.51724137931035
Method name: window_entropy, running accuracy: 91.16379310344827
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 463/500 [37:47:14<2:25:41, 236.26s/it, attention_weighted_confidence_acc=91.16%, cer_entropy_weighted_mean_all_acc=91.16%, cer_prob_product_log_last_acc=90.52%, self_consistency_acc=91.38%, p_true_acc=91.59%, normilized_likelihood_acc=90.73%, normilized_entropy_acc=90.52%, topk_entropy_acc=90.52%, window_entropy_acc=91.16%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 464/500 [37:47:14<2:21:07, 235.21s/it, attention_weighted_confidence_acc=91.16%, cer_entropy_weighted_mean_all_acc=91.16%, cer_prob_product_log_last_acc=90.52%, self_consistency_acc=91.38%, p_true_acc=91.59%, normilized_likelihood_acc=90.73%, normilized_entropy_acc=90.52%, topk_entropy_acc=90.52%, window_entropy_acc=91.16%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 7.37268410884309
    Answer: 227
    Ground truth:  227
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 7.37268410884309
    Answer: 227
    Ground truth:  227
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 13.99989366531372
    Answer: 227
    Ground truth:  227
Method 4: self_consistency
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 0.875
    Answer: 227
    Ground truth:  227
Method 5: p_true
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 11.24609375
    Answer: 227
    Ground truth:  227
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 6.16431150585413
    Answer: 227
    Ground truth:  227
Method 7: normilized_entropy
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 6.014858052134514
    Answer: 227
    Ground truth:  227
Method 8: topk_entropy
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 5.349919065833092
    Answer: 227
    Ground truth:  227
Method 9: window_entropy
  Batch 1:
    Text: To determine the total time it will take Andy to string all the racquets, we nee...
    Score: 20.48840767145157
    Answer: 227
    Ground truth:  227
Method name: attention_weighted_confidence, running accuracy: 91.18279569892474
Method name: cer_entropy_weighted_mean_all, running accuracy: 91.18279569892474
Method name: cer_prob_product_log_last, running accuracy: 90.53763440860216
Method name: self_consistency, running accuracy: 91.39784946236558
Method name: p_true, running accuracy: 91.61290322580645
Method name: normilized_likelihood, running accuracy: 90.75268817204301
Method name: normilized_entropy, running accuracy: 90.53763440860216
Method name: topk_entropy, running accuracy: 90.53763440860216
Method name: window_entropy, running accuracy: 91.18279569892474
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 464/500 [37:51:00<2:21:07, 235.21s/it, attention_weighted_confidence_acc=91.18%, cer_entropy_weighted_mean_all_acc=91.18%, cer_prob_product_log_last_acc=90.54%, self_consistency_acc=91.40%, p_true_acc=91.61%, normilized_likelihood_acc=90.75%, normilized_entropy_acc=90.54%, topk_entropy_acc=90.54%, window_entropy_acc=91.18%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 465/500 [37:51:00<2:15:40, 232.58s/it, attention_weighted_confidence_acc=91.18%, cer_entropy_weighted_mean_all_acc=91.18%, cer_prob_product_log_last_acc=90.54%, self_consistency_acc=91.40%, p_true_acc=91.61%, normilized_likelihood_acc=90.75%, normilized_entropy_acc=90.54%, topk_entropy_acc=90.54%, window_entropy_acc=91.18%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 6.243201361359251
    Answer: 0
    Ground truth:  2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 6.243201361359251
    Answer: 0
    Ground truth:  2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine the latest time Jordan can start making the cake, we need to calcul...
    Score: 0.9999986886978149
    Answer: 10
    Ground truth:  2
Method 4: self_consistency
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 0.8125
    Answer: 0
    Ground truth:  2
Method 5: p_true
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 11.6484375
    Answer: 0
    Ground truth:  2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 15.853796109557152
    Answer: 0
    Ground truth:  2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 15.926377713680267
    Answer: 0
    Ground truth:  2
Method 8: topk_entropy
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 13.410558834671974
    Answer: 0
    Ground truth:  2
Method 9: window_entropy
  Batch 1:
    Text: To find the latest time Jordan can start making the cake, we need to calculate t...
    Score: 18.662753880023956
    Answer: 0
    Ground truth:  2
Method name: attention_weighted_confidence, running accuracy: 90.98712446351931
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.98712446351931
Method name: cer_prob_product_log_last, running accuracy: 90.34334763948499
Method name: self_consistency, running accuracy: 91.20171673819742
Method name: p_true, running accuracy: 91.41630901287554
Method name: normilized_likelihood, running accuracy: 90.55793991416309
Method name: normilized_entropy, running accuracy: 90.34334763948499
Method name: topk_entropy, running accuracy: 90.34334763948499
Method name: window_entropy, running accuracy: 90.98712446351931
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 465/500 [37:55:34<2:15:40, 232.58s/it, attention_weighted_confidence_acc=90.99%, cer_entropy_weighted_mean_all_acc=90.99%, cer_prob_product_log_last_acc=90.34%, self_consistency_acc=91.20%, p_true_acc=91.42%, normilized_likelihood_acc=90.56%, normilized_entropy_acc=90.34%, topk_entropy_acc=90.34%, window_entropy_acc=90.99%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 466/500 [37:55:34<2:18:44, 244.84s/it, attention_weighted_confidence_acc=90.99%, cer_entropy_weighted_mean_all_acc=90.99%, cer_prob_product_log_last_acc=90.34%, self_consistency_acc=91.20%, p_true_acc=91.42%, normilized_likelihood_acc=90.56%, normilized_entropy_acc=90.34%, topk_entropy_acc=90.34%, window_entropy_acc=90.99%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, we first need ...
    Score: 0.5437528176801257
    Answer: 1281.13
    Ground truth:  9360
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, we first need ...
    Score: 0.5437528176801257
    Answer: 1281.13
    Ground truth:  9360
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, we first need ...
    Score: 0.49868625433769864
    Answer: 7920
    Ground truth:  9360
Method 4: self_consistency
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, we first need ...
    Score: 0.0625
    Answer: 1286.15
    Ground truth:  9360
Method 5: p_true
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, let's break it...
    Score: 0.8515625
    Answer: 10530.52
    Ground truth:  9360
Method 6: normilized_likelihood
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, we first need ...
    Score: 0.7449733316898346
    Answer: 1286.15
    Ground truth:  9360
Method 7: normilized_entropy
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, we first need ...
    Score: 0.6518151015043259
    Answer: 1286.15
    Ground truth:  9360
Method 8: topk_entropy
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, we first need ...
    Score: 0.6015808135271072
    Answer: 1286.15
    Ground truth:  9360
Method 9: window_entropy
  Batch 1:
    Text: To find Sylvie's annual salary after three more years of service, let's break it...
    Score: 1.3185955286026
    Answer: 10530.52
    Ground truth:  9360
Method name: attention_weighted_confidence, running accuracy: 90.79229122055675
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.79229122055675
Method name: cer_prob_product_log_last, running accuracy: 90.14989293361884
Method name: self_consistency, running accuracy: 91.00642398286938
Method name: p_true, running accuracy: 91.22055674518201
Method name: normilized_likelihood, running accuracy: 90.36402569593149
Method name: normilized_entropy, running accuracy: 90.14989293361884
Method name: topk_entropy, running accuracy: 90.14989293361884
Method name: window_entropy, running accuracy: 90.79229122055675
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 466/500 [38:01:47<2:18:44, 244.84s/it, attention_weighted_confidence_acc=90.79%, cer_entropy_weighted_mean_all_acc=90.79%, cer_prob_product_log_last_acc=90.15%, self_consistency_acc=91.01%, p_true_acc=91.22%, normilized_likelihood_acc=90.36%, normilized_entropy_acc=90.15%, topk_entropy_acc=90.15%, window_entropy_acc=90.79%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 467/500 [38:01:47<2:35:50, 283.36s/it, attention_weighted_confidence_acc=90.79%, cer_entropy_weighted_mean_all_acc=90.79%, cer_prob_product_log_last_acc=90.15%, self_consistency_acc=91.01%, p_true_acc=91.22%, normilized_likelihood_acc=90.36%, normilized_entropy_acc=90.15%, topk_entropy_acc=90.15%, window_entropy_acc=90.79%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 5.069978915961453
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 5.069978915961453
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 10.934345841407776
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 0.6875
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 9.65234375
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 6.488966643810272
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 6.145449072122574
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 5.3886538445949554
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find out how much James spends on candles, we need to determine the number of...
    Score: 9.598408043384552
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 90.8119658119658
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.8119658119658
Method name: cer_prob_product_log_last, running accuracy: 90.17094017094017
Method name: self_consistency, running accuracy: 91.02564102564102
Method name: p_true, running accuracy: 91.23931623931624
Method name: normilized_likelihood, running accuracy: 90.38461538461539
Method name: normilized_entropy, running accuracy: 90.17094017094017
Method name: topk_entropy, running accuracy: 90.17094017094017
Method name: window_entropy, running accuracy: 90.8119658119658
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  93%|█████████▎| 467/500 [38:05:40<2:35:50, 283.36s/it, attention_weighted_confidence_acc=90.81%, cer_entropy_weighted_mean_all_acc=90.81%, cer_prob_product_log_last_acc=90.17%, self_consistency_acc=91.03%, p_true_acc=91.24%, normilized_likelihood_acc=90.38%, normilized_entropy_acc=90.17%, topk_entropy_acc=90.17%, window_entropy_acc=90.81%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▎| 468/500 [38:05:40<2:23:09, 268.43s/it, attention_weighted_confidence_acc=90.81%, cer_entropy_weighted_mean_all_acc=90.81%, cer_prob_product_log_last_acc=90.17%, self_consistency_acc=91.03%, p_true_acc=91.24%, normilized_likelihood_acc=90.38%, normilized_entropy_acc=90.17%, topk_entropy_acc=90.17%, window_entropy_acc=90.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 5.357859028483877
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 5.357859028483877
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 10.988002300262451
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 0.6875
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 9.8671875
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 9.404593706130981
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 9.187804818153381
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 8.01476214826107
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of students good at math, we need to consider the stude...
    Score: 14.990794360637665
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 90.8315565031983
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.8315565031983
Method name: cer_prob_product_log_last, running accuracy: 90.19189765458422
Method name: self_consistency, running accuracy: 91.04477611940298
Method name: p_true, running accuracy: 91.25799573560768
Method name: normilized_likelihood, running accuracy: 90.40511727078892
Method name: normilized_entropy, running accuracy: 90.19189765458422
Method name: topk_entropy, running accuracy: 90.19189765458422
Method name: window_entropy, running accuracy: 90.8315565031983
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▎| 468/500 [38:11:07<2:23:09, 268.43s/it, attention_weighted_confidence_acc=90.83%, cer_entropy_weighted_mean_all_acc=90.83%, cer_prob_product_log_last_acc=90.19%, self_consistency_acc=91.04%, p_true_acc=91.26%, normilized_likelihood_acc=90.41%, normilized_entropy_acc=90.19%, topk_entropy_acc=90.19%, window_entropy_acc=90.83%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 469/500 [38:11:07<2:27:37, 285.73s/it, attention_weighted_confidence_acc=90.83%, cer_entropy_weighted_mean_all_acc=90.83%, cer_prob_product_log_last_acc=90.19%, self_consistency_acc=91.04%, p_true_acc=91.26%, normilized_likelihood_acc=90.41%, normilized_entropy_acc=90.19%, topk_entropy_acc=90.19%, window_entropy_acc=90.83%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 8.973694366659
    Answer: 45.0
    Ground truth:  45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 8.973694366659
    Answer: 45.0
    Ground truth:  45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 0.0
    Answer: 45.0
    Ground truth:  45
Method 4: self_consistency
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 1.0
    Answer: 45.0
    Ground truth:  45
Method 5: p_true
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 11.05859375
    Answer: 45.0
    Ground truth:  45
Method 6: normilized_likelihood
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 1.4372285418212414
    Answer: 45.0
    Ground truth:  45
Method 7: normilized_entropy
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 0.8527020737528801
    Answer: 45.0
    Ground truth:  45
Method 8: topk_entropy
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 0.8466564118862152
    Answer: 45.0
    Ground truth:  45
Method 9: window_entropy
  Batch 1:
    Text: To find Marcus' total bill, we need to calculate the cost of each item and then ...
    Score: 6.171378165483475
    Answer: 45.0
    Ground truth:  45
Method name: attention_weighted_confidence, running accuracy: 90.85106382978724
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.85106382978724
Method name: cer_prob_product_log_last, running accuracy: 90.2127659574468
Method name: self_consistency, running accuracy: 91.06382978723404
Method name: p_true, running accuracy: 91.27659574468086
Method name: normilized_likelihood, running accuracy: 90.42553191489363
Method name: normilized_entropy, running accuracy: 90.2127659574468
Method name: topk_entropy, running accuracy: 90.2127659574468
Method name: window_entropy, running accuracy: 90.85106382978724
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 469/500 [38:15:24<2:27:37, 285.73s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.21%, self_consistency_acc=91.06%, p_true_acc=91.28%, normilized_likelihood_acc=90.43%, normilized_entropy_acc=90.21%, topk_entropy_acc=90.21%, window_entropy_acc=90.85%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 470/500 [38:15:24<2:18:37, 277.26s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.21%, self_consistency_acc=91.06%, p_true_acc=91.28%, normilized_likelihood_acc=90.43%, normilized_entropy_acc=90.21%, topk_entropy_acc=90.21%, window_entropy_acc=90.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 8.507283674499448
    Answer: 4
    Ground truth:  4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 8.507283674499448
    Answer: 4
    Ground truth:  4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 15.99865972995758
    Answer: 4
    Ground truth:  4
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 1.0
    Answer: 4
    Ground truth:  4
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 15.18359375
    Answer: 4
    Ground truth:  4
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 1.8608002364635468
    Answer: 4
    Ground truth:  4
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 1.8171158581972122
    Answer: 4
    Ground truth:  4
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 1.803379014134407
    Answer: 4
    Ground truth:  4
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we will break it down step by step.

1. Clara has 20 book...
    Score: 3.7008351385593414
    Answer: 4
    Ground truth:  4
Method name: attention_weighted_confidence, running accuracy: 90.87048832271762
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.87048832271762
Method name: cer_prob_product_log_last, running accuracy: 90.23354564755839
Method name: self_consistency, running accuracy: 91.0828025477707
Method name: p_true, running accuracy: 91.29511677282377
Method name: normilized_likelihood, running accuracy: 90.44585987261146
Method name: normilized_entropy, running accuracy: 90.23354564755839
Method name: topk_entropy, running accuracy: 90.23354564755839
Method name: window_entropy, running accuracy: 90.87048832271762
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 470/500 [38:18:33<2:18:37, 277.26s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.23%, self_consistency_acc=91.08%, p_true_acc=91.30%, normilized_likelihood_acc=90.45%, normilized_entropy_acc=90.23%, topk_entropy_acc=90.23%, window_entropy_acc=90.87%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 471/500 [38:18:33<2:01:13, 250.81s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.23%, self_consistency_acc=91.08%, p_true_acc=91.30%, normilized_likelihood_acc=90.45%, normilized_entropy_acc=90.23%, topk_entropy_acc=90.23%, window_entropy_acc=90.87%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 5.162822792356843
    Answer: 10
    Ground truth:  10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 5.162822792356843
    Answer: 10
    Ground truth:  10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 9.998879432678223
    Answer: 10
    Ground truth:  10
Method 4: self_consistency
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 0.625
    Answer: 10
    Ground truth:  10
Method 5: p_true
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 9.6171875
    Answer: 10
    Ground truth:  10
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 0.8822296559810638
    Answer: 10
    Ground truth:  10
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 0.4933229386806488
    Answer: 10
    Ground truth:  10
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 0.45860037207603455
    Answer: 10
    Ground truth:  10
Method 9: window_entropy
  Batch 1:
    Text: Step 1: The bakery produces 60 loaves of bread each day. 

Step 2: Two-thirds of...
    Score: 4.611410617828369
    Answer: 10
    Ground truth:  10
Method name: attention_weighted_confidence, running accuracy: 90.88983050847457
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.88983050847457
Method name: cer_prob_product_log_last, running accuracy: 90.2542372881356
Method name: self_consistency, running accuracy: 91.10169491525424
Method name: p_true, running accuracy: 91.3135593220339
Method name: normilized_likelihood, running accuracy: 90.46610169491525
Method name: normilized_entropy, running accuracy: 90.2542372881356
Method name: topk_entropy, running accuracy: 90.2542372881356
Method name: window_entropy, running accuracy: 90.88983050847457
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 471/500 [38:21:37<2:01:13, 250.81s/it, attention_weighted_confidence_acc=90.89%, cer_entropy_weighted_mean_all_acc=90.89%, cer_prob_product_log_last_acc=90.25%, self_consistency_acc=91.10%, p_true_acc=91.31%, normilized_likelihood_acc=90.47%, normilized_entropy_acc=90.25%, topk_entropy_acc=90.25%, window_entropy_acc=90.89%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 472/500 [38:21:37<1:47:38, 230.67s/it, attention_weighted_confidence_acc=90.89%, cer_entropy_weighted_mean_all_acc=90.89%, cer_prob_product_log_last_acc=90.25%, self_consistency_acc=91.10%, p_true_acc=91.31%, normilized_likelihood_acc=90.47%, normilized_entropy_acc=90.25%, topk_entropy_acc=90.25%, window_entropy_acc=90.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 8.092596242184136
    Answer: 1218
    Ground truth:  1218
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 8.092596242184136
    Answer: 1218
    Ground truth:  1218
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 4.377027207138376
    Answer: 1218
    Ground truth:  1218
Method 4: self_consistency
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 1.0
    Answer: 1218
    Ground truth:  1218
Method 5: p_true
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 13.9609375
    Answer: 1218
    Ground truth:  1218
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 2.0556242913007736
    Answer: 1218
    Ground truth:  1218
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 1.1946672201156616
    Answer: 1218
    Ground truth:  1218
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 1.1835881173610687
    Answer: 1218
    Ground truth:  1218
Method 9: window_entropy
  Batch 1:
    Text: To find out how many meals Mamou has left to distribute, we need to determine th...
    Score: 3.6153963804244995
    Answer: 1218
    Ground truth:  1218
Method name: attention_weighted_confidence, running accuracy: 90.9090909090909
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.9090909090909
Method name: cer_prob_product_log_last, running accuracy: 90.27484143763213
Method name: self_consistency, running accuracy: 91.12050739957716
Method name: p_true, running accuracy: 91.33192389006342
Method name: normilized_likelihood, running accuracy: 90.48625792811839
Method name: normilized_entropy, running accuracy: 90.27484143763213
Method name: topk_entropy, running accuracy: 90.27484143763213
Method name: window_entropy, running accuracy: 90.9090909090909
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  94%|█████████▍| 472/500 [38:24:09<1:47:38, 230.67s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.27%, self_consistency_acc=91.12%, p_true_acc=91.33%, normilized_likelihood_acc=90.49%, normilized_entropy_acc=90.27%, topk_entropy_acc=90.27%, window_entropy_acc=90.91%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▍| 473/500 [38:24:09<1:33:16, 207.27s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.27%, self_consistency_acc=91.12%, p_true_acc=91.33%, normilized_likelihood_acc=90.49%, normilized_entropy_acc=90.27%, topk_entropy_acc=90.27%, window_entropy_acc=90.91%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 8.132733606335723
    Answer: 7
    Ground truth:  7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 8.132733606335723
    Answer: 7
    Ground truth:  7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 15.999263167381287
    Answer: 7
    Ground truth:  7
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 1.0
    Answer: 7
    Ground truth:  7
Method 5: p_true
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 13.8046875
    Answer: 7
    Ground truth:  7
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 1.784848503768444
    Answer: 7
    Ground truth:  7
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 0.8132008165121078
    Answer: 7
    Ground truth:  7
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 0.8115972876548767
    Answer: 7
    Ground truth:  7
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of matches Joey played in one week, let's break down th...
    Score: 2.9820032119750977
    Answer: 7
    Ground truth:  7
Method name: attention_weighted_confidence, running accuracy: 90.92827004219409
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.92827004219409
Method name: cer_prob_product_log_last, running accuracy: 90.29535864978902
Method name: self_consistency, running accuracy: 91.13924050632912
Method name: p_true, running accuracy: 91.35021097046413
Method name: normilized_likelihood, running accuracy: 90.50632911392405
Method name: normilized_entropy, running accuracy: 90.29535864978902
Method name: topk_entropy, running accuracy: 90.29535864978902
Method name: window_entropy, running accuracy: 90.92827004219409
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▍| 473/500 [38:26:15<1:33:16, 207.27s/it, attention_weighted_confidence_acc=90.93%, cer_entropy_weighted_mean_all_acc=90.93%, cer_prob_product_log_last_acc=90.30%, self_consistency_acc=91.14%, p_true_acc=91.35%, normilized_likelihood_acc=90.51%, normilized_entropy_acc=90.30%, topk_entropy_acc=90.30%, window_entropy_acc=90.93%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▍| 474/500 [38:26:15<1:19:14, 182.85s/it, attention_weighted_confidence_acc=90.93%, cer_entropy_weighted_mean_all_acc=90.93%, cer_prob_product_log_last_acc=90.30%, self_consistency_acc=91.14%, p_true_acc=91.35%, normilized_likelihood_acc=90.51%, normilized_entropy_acc=90.30%, topk_entropy_acc=90.30%, window_entropy_acc=90.93%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 7.788784591450344
    Answer: 15
    Ground truth:  15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 7.788784591450344
    Answer: 15
    Ground truth:  15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 14.963115215301514
    Answer: 15
    Ground truth:  15
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 0.9375
    Answer: 15
    Ground truth:  15
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 10.822265625
    Answer: 15
    Ground truth:  15
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 14.630472391843796
    Answer: 15
    Ground truth:  15
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 14.565871208906174
    Answer: 15
    Ground truth:  15
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 12.682079508900642
    Answer: 15
    Ground truth:  15
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem into two types of animals: sheep and geese.

Sheep ...
    Score: 27.146167039871216
    Answer: 15
    Ground truth:  15
Method name: attention_weighted_confidence, running accuracy: 90.94736842105263
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.94736842105263
Method name: cer_prob_product_log_last, running accuracy: 90.31578947368422
Method name: self_consistency, running accuracy: 91.15789473684211
Method name: p_true, running accuracy: 91.36842105263158
Method name: normilized_likelihood, running accuracy: 90.52631578947368
Method name: normilized_entropy, running accuracy: 90.31578947368422
Method name: topk_entropy, running accuracy: 90.31578947368422
Method name: window_entropy, running accuracy: 90.94736842105263
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▍| 474/500 [38:30:27<1:19:14, 182.85s/it, attention_weighted_confidence_acc=90.95%, cer_entropy_weighted_mean_all_acc=90.95%, cer_prob_product_log_last_acc=90.32%, self_consistency_acc=91.16%, p_true_acc=91.37%, normilized_likelihood_acc=90.53%, normilized_entropy_acc=90.32%, topk_entropy_acc=90.32%, window_entropy_acc=90.95%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▌| 475/500 [38:30:27<1:24:48, 203.52s/it, attention_weighted_confidence_acc=90.95%, cer_entropy_weighted_mean_all_acc=90.95%, cer_prob_product_log_last_acc=90.32%, self_consistency_acc=91.16%, p_true_acc=91.37%, normilized_likelihood_acc=90.53%, normilized_entropy_acc=90.32%, topk_entropy_acc=90.32%, window_entropy_acc=90.95%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 5.4188526222247715
    Answer: 106.12
    Ground truth:  106
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 5.4188526222247715
    Answer: 106.12
    Ground truth:  106
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the amount Mandy needs to pay back to Benedict with 2% interest for...
    Score: 3.9999961853027344
    Answer: 106
    Ground truth:  106
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 0.6875
    Answer: 106.12
    Ground truth:  106
Method 5: p_true
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 7.666015625
    Answer: 106.12
    Ground truth:  106
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 3.871685266494751
    Answer: 106.12
    Ground truth:  106
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 3.7759339958429337
    Answer: 106.12
    Ground truth:  106
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 3.1987354457378387
    Answer: 106.12
    Ground truth:  106
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Mandy should give to Benedict after 3 months with a 2% mont...
    Score: 9.635163694620132
    Answer: 106.12
    Ground truth:  106
Method name: attention_weighted_confidence, running accuracy: 90.96638655462185
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.96638655462185
Method name: cer_prob_product_log_last, running accuracy: 90.33613445378151
Method name: self_consistency, running accuracy: 91.17647058823529
Method name: p_true, running accuracy: 91.38655462184873
Method name: normilized_likelihood, running accuracy: 90.54621848739495
Method name: normilized_entropy, running accuracy: 90.33613445378151
Method name: topk_entropy, running accuracy: 90.33613445378151
Method name: window_entropy, running accuracy: 90.96638655462185
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▌| 475/500 [38:34:18<1:24:48, 203.52s/it, attention_weighted_confidence_acc=90.97%, cer_entropy_weighted_mean_all_acc=90.97%, cer_prob_product_log_last_acc=90.34%, self_consistency_acc=91.18%, p_true_acc=91.39%, normilized_likelihood_acc=90.55%, normilized_entropy_acc=90.34%, topk_entropy_acc=90.34%, window_entropy_acc=90.97%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▌| 476/500 [38:34:18<1:24:41, 211.71s/it, attention_weighted_confidence_acc=90.97%, cer_entropy_weighted_mean_all_acc=90.97%, cer_prob_product_log_last_acc=90.34%, self_consistency_acc=91.18%, p_true_acc=91.39%, normilized_likelihood_acc=90.55%, normilized_entropy_acc=90.34%, topk_entropy_acc=90.34%, window_entropy_acc=90.97%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 6.95000941309393
    Answer: 18.8
    Ground truth:  1128
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 6.95000941309393
    Answer: 18.8
    Ground truth:  1128
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 4.64282693341616
    Answer: 18.8
    Ground truth:  1128
Method 4: self_consistency
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 0.875
    Answer: 18.8
    Ground truth:  1128
Method 5: p_true
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 13.0859375
    Answer: 18.8
    Ground truth:  1128
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 4.810667619109154
    Answer: 18.8
    Ground truth:  1128
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 4.150483936071396
    Answer: 18.8
    Ground truth:  1128
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 3.60209821164608
    Answer: 18.8
    Ground truth:  1128
Method 9: window_entropy
  Batch 1:
    Text: To find out how much time Britany spends on TikTok in a month, we need to calcul...
    Score: 18.4299818277359
    Answer: 18.8
    Ground truth:  1128
Method name: attention_weighted_confidence, running accuracy: 90.77568134171908
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.77568134171908
Method name: cer_prob_product_log_last, running accuracy: 90.14675052410901
Method name: self_consistency, running accuracy: 90.98532494758909
Method name: p_true, running accuracy: 91.19496855345912
Method name: normilized_likelihood, running accuracy: 90.35639412997904
Method name: normilized_entropy, running accuracy: 90.14675052410901
Method name: topk_entropy, running accuracy: 90.14675052410901
Method name: window_entropy, running accuracy: 90.77568134171908
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▌| 476/500 [38:38:42<1:24:41, 211.71s/it, attention_weighted_confidence_acc=90.78%, cer_entropy_weighted_mean_all_acc=90.78%, cer_prob_product_log_last_acc=90.15%, self_consistency_acc=90.99%, p_true_acc=91.19%, normilized_likelihood_acc=90.36%, normilized_entropy_acc=90.15%, topk_entropy_acc=90.15%, window_entropy_acc=90.78%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▌| 477/500 [38:38:42<1:27:09, 227.37s/it, attention_weighted_confidence_acc=90.78%, cer_entropy_weighted_mean_all_acc=90.78%, cer_prob_product_log_last_acc=90.15%, self_consistency_acc=90.99%, p_true_acc=91.19%, normilized_likelihood_acc=90.36%, normilized_entropy_acc=90.15%, topk_entropy_acc=90.15%, window_entropy_acc=90.78%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 5.26291417746025
    Answer: 25
    Ground truth:  25
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 5.26291417746025
    Answer: 25
    Ground truth:  25
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 10.582789599895477
    Answer: 25
    Ground truth:  25
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 0.6875
    Answer: 25
    Ground truth:  25
Method 5: p_true
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 9.5
    Answer: 25
    Ground truth:  25
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 9.755419373512268
    Answer: 25
    Ground truth:  25
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 7.233760833740234
    Answer: 25
    Ground truth:  25
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 6.318308413028717
    Answer: 25
    Ground truth:  25
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the steps:

Initially, there are 5 children following the truck...
    Score: 12.681965172290802
    Answer: 25
    Ground truth:  25
Method name: attention_weighted_confidence, running accuracy: 90.7949790794979
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.7949790794979
Method name: cer_prob_product_log_last, running accuracy: 90.1673640167364
Method name: self_consistency, running accuracy: 91.0041841004184
Method name: p_true, running accuracy: 91.21338912133892
Method name: normilized_likelihood, running accuracy: 90.3765690376569
Method name: normilized_entropy, running accuracy: 90.1673640167364
Method name: topk_entropy, running accuracy: 90.1673640167364
Method name: window_entropy, running accuracy: 90.7949790794979
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  95%|█████████▌| 477/500 [38:42:01<1:27:09, 227.37s/it, attention_weighted_confidence_acc=90.79%, cer_entropy_weighted_mean_all_acc=90.79%, cer_prob_product_log_last_acc=90.17%, self_consistency_acc=91.00%, p_true_acc=91.21%, normilized_likelihood_acc=90.38%, normilized_entropy_acc=90.17%, topk_entropy_acc=90.17%, window_entropy_acc=90.79%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 478/500 [38:42:01<1:20:16, 218.92s/it, attention_weighted_confidence_acc=90.79%, cer_entropy_weighted_mean_all_acc=90.79%, cer_prob_product_log_last_acc=90.17%, self_consistency_acc=91.00%, p_true_acc=91.21%, normilized_likelihood_acc=90.38%, normilized_entropy_acc=90.17%, topk_entropy_acc=90.17%, window_entropy_acc=90.79%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 6.9323914380243
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 6.9323914380243
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 13.73163104057312
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 0.875
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 10.90234375
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 5.04931765794754
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 3.7192653119564056
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 3.4368065297603607
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To find out the number of rows of 5-star flags, we need to first find the total ...
    Score: 9.062366008758545
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 90.81419624217119
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.81419624217119
Method name: cer_prob_product_log_last, running accuracy: 90.18789144050105
Method name: self_consistency, running accuracy: 91.02296450939458
Method name: p_true, running accuracy: 91.23173277661796
Method name: normilized_likelihood, running accuracy: 90.39665970772442
Method name: normilized_entropy, running accuracy: 90.18789144050105
Method name: topk_entropy, running accuracy: 90.18789144050105
Method name: window_entropy, running accuracy: 90.81419624217119
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 478/500 [38:45:18<1:20:16, 218.92s/it, attention_weighted_confidence_acc=90.81%, cer_entropy_weighted_mean_all_acc=90.81%, cer_prob_product_log_last_acc=90.19%, self_consistency_acc=91.02%, p_true_acc=91.23%, normilized_likelihood_acc=90.40%, normilized_entropy_acc=90.19%, topk_entropy_acc=90.19%, window_entropy_acc=90.81%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 479/500 [38:45:18<1:14:15, 212.19s/it, attention_weighted_confidence_acc=90.81%, cer_entropy_weighted_mean_all_acc=90.81%, cer_prob_product_log_last_acc=90.19%, self_consistency_acc=91.02%, p_true_acc=91.23%, normilized_likelihood_acc=90.40%, normilized_entropy_acc=90.19%, topk_entropy_acc=90.19%, window_entropy_acc=90.81%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 2.763229438909123
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 2.763229438909123
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 5.993457555770874
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 0.375
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 5.60546875
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 4.41328626871109
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 4.6327663362026215
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 4.1148668229579926
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to determine the rate at which the boat is taking...
    Score: 5.8827179074287415
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 90.83333333333333
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.83333333333333
Method name: cer_prob_product_log_last, running accuracy: 90.20833333333333
Method name: self_consistency, running accuracy: 91.04166666666667
Method name: p_true, running accuracy: 91.25
Method name: normilized_likelihood, running accuracy: 90.41666666666667
Method name: normilized_entropy, running accuracy: 90.20833333333333
Method name: topk_entropy, running accuracy: 90.20833333333333
Method name: window_entropy, running accuracy: 90.83333333333333
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 479/500 [38:50:58<1:14:15, 212.19s/it, attention_weighted_confidence_acc=90.83%, cer_entropy_weighted_mean_all_acc=90.83%, cer_prob_product_log_last_acc=90.21%, self_consistency_acc=91.04%, p_true_acc=91.25%, normilized_likelihood_acc=90.42%, normilized_entropy_acc=90.21%, topk_entropy_acc=90.21%, window_entropy_acc=90.83%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 480/500 [38:50:58<1:23:34, 250.75s/it, attention_weighted_confidence_acc=90.83%, cer_entropy_weighted_mean_all_acc=90.83%, cer_prob_product_log_last_acc=90.21%, self_consistency_acc=91.04%, p_true_acc=91.25%, normilized_likelihood_acc=90.42%, normilized_entropy_acc=90.21%, topk_entropy_acc=90.21%, window_entropy_acc=90.83%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 6.631362240040662
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 6.631362240040662
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 13.999934554100037
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 0.875
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 12.18359375
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 9.774607092142105
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 10.434772282838821
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 9.322305917739868
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: To find out how much money Joe has left after the purchase, we need to find out ...
    Score: 15.053630769252777
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 90.85239085239085
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.85239085239085
Method name: cer_prob_product_log_last, running accuracy: 90.22869022869024
Method name: self_consistency, running accuracy: 91.06029106029106
Method name: p_true, running accuracy: 91.26819126819127
Method name: normilized_likelihood, running accuracy: 90.43659043659044
Method name: normilized_entropy, running accuracy: 90.22869022869024
Method name: topk_entropy, running accuracy: 90.22869022869024
Method name: window_entropy, running accuracy: 90.85239085239085
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 480/500 [38:54:57<1:23:34, 250.75s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.23%, self_consistency_acc=91.06%, p_true_acc=91.27%, normilized_likelihood_acc=90.44%, normilized_entropy_acc=90.23%, topk_entropy_acc=90.23%, window_entropy_acc=90.85%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 481/500 [38:54:57<1:18:17, 247.22s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.23%, self_consistency_acc=91.06%, p_true_acc=91.27%, normilized_likelihood_acc=90.44%, normilized_entropy_acc=90.23%, topk_entropy_acc=90.23%, window_entropy_acc=90.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 7.194375344525075
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 7.194375344525075
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 13.940976917743683
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 0.875
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 11.81640625
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 10.653341144323349
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 10.093991562724113
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 8.828532129526138
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down step by step:

1. First, calculate the total quarts of tea d...
    Score: 17.11995893716812
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 90.87136929460581
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.87136929460581
Method name: cer_prob_product_log_last, running accuracy: 90.24896265560166
Method name: self_consistency, running accuracy: 91.07883817427386
Method name: p_true, running accuracy: 91.28630705394191
Method name: normilized_likelihood, running accuracy: 90.45643153526972
Method name: normilized_entropy, running accuracy: 90.24896265560166
Method name: topk_entropy, running accuracy: 90.24896265560166
Method name: window_entropy, running accuracy: 90.87136929460581
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▌| 481/500 [38:58:59<1:18:17, 247.22s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.25%, self_consistency_acc=91.08%, p_true_acc=91.29%, normilized_likelihood_acc=90.46%, normilized_entropy_acc=90.25%, topk_entropy_acc=90.25%, window_entropy_acc=90.87%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▋| 482/500 [38:58:59<1:13:39, 245.55s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.25%, self_consistency_acc=91.08%, p_true_acc=91.29%, normilized_likelihood_acc=90.46%, normilized_entropy_acc=90.25%, topk_entropy_acc=90.25%, window_entropy_acc=90.87%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 7.580510501083675
    Answer: 67
    Ground truth:  67
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 7.580510501083675
    Answer: 67
    Ground truth:  67
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 14.998711347579956
    Answer: 67
    Ground truth:  67
Method 4: self_consistency
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 0.9375
    Answer: 67
    Ground truth:  67
Method 5: p_true
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 14.109375
    Answer: 67
    Ground truth:  67
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 2.429063469171524
    Answer: 67
    Ground truth:  67
Method 7: normilized_entropy
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 2.2047314941883087
    Answer: 67
    Ground truth:  67
Method 8: topk_entropy
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 2.195545047521591
    Answer: 67
    Ground truth:  67
Method 9: window_entropy
  Batch 1:
    Text: Let's analyze the problem step by step.

Since Albert is 36" tall, and Anne is t...
    Score: 3.385930299758911
    Answer: 67
    Ground truth:  67
Method name: attention_weighted_confidence, running accuracy: 90.89026915113871
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.89026915113871
Method name: cer_prob_product_log_last, running accuracy: 90.26915113871635
Method name: self_consistency, running accuracy: 91.09730848861284
Method name: p_true, running accuracy: 91.30434782608695
Method name: normilized_likelihood, running accuracy: 90.47619047619048
Method name: normilized_entropy, running accuracy: 90.26915113871635
Method name: topk_entropy, running accuracy: 90.26915113871635
Method name: window_entropy, running accuracy: 90.89026915113871
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  96%|█████████▋| 482/500 [39:01:42<1:13:39, 245.55s/it, attention_weighted_confidence_acc=90.89%, cer_entropy_weighted_mean_all_acc=90.89%, cer_prob_product_log_last_acc=90.27%, self_consistency_acc=91.10%, p_true_acc=91.30%, normilized_likelihood_acc=90.48%, normilized_entropy_acc=90.27%, topk_entropy_acc=90.27%, window_entropy_acc=90.89%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 483/500 [39:01:42<1:02:35, 220.89s/it, attention_weighted_confidence_acc=90.89%, cer_entropy_weighted_mean_all_acc=90.89%, cer_prob_product_log_last_acc=90.27%, self_consistency_acc=91.10%, p_true_acc=91.30%, normilized_likelihood_acc=90.48%, normilized_entropy_acc=90.27%, topk_entropy_acc=90.27%, window_entropy_acc=90.89%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 8.398807137572252
    Answer: 8
    Ground truth:  8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 8.398807137572252
    Answer: 8
    Ground truth:  8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 15.999908328056335
    Answer: 8
    Ground truth:  8
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 1.0
    Answer: 8
    Ground truth:  8
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 12.5625
    Answer: 8
    Ground truth:  8
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 1.0151761770248413
    Answer: 8
    Ground truth:  8
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 1.5380273908376694
    Answer: 8
    Ground truth:  8
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 1.5332949757575989
    Answer: 8
    Ground truth:  8
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. Chenny is 10 years old.
2. Alyana...
    Score: 7.488691061735153
    Answer: 8
    Ground truth:  8
Method name: attention_weighted_confidence, running accuracy: 90.9090909090909
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.9090909090909
Method name: cer_prob_product_log_last, running accuracy: 90.28925619834712
Method name: self_consistency, running accuracy: 91.11570247933885
Method name: p_true, running accuracy: 91.32231404958677
Method name: normilized_likelihood, running accuracy: 90.49586776859503
Method name: normilized_entropy, running accuracy: 90.28925619834712
Method name: topk_entropy, running accuracy: 90.28925619834712
Method name: window_entropy, running accuracy: 90.9090909090909
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 483/500 [39:03:42<1:02:35, 220.89s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.29%, self_consistency_acc=91.12%, p_true_acc=91.32%, normilized_likelihood_acc=90.50%, normilized_entropy_acc=90.29%, topk_entropy_acc=90.29%, window_entropy_acc=90.91%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 484/500 [39:03:42<50:48, 190.51s/it, attention_weighted_confidence_acc=90.91%, cer_entropy_weighted_mean_all_acc=90.91%, cer_prob_product_log_last_acc=90.29%, self_consistency_acc=91.12%, p_true_acc=91.32%, normilized_likelihood_acc=90.50%, normilized_entropy_acc=90.29%, topk_entropy_acc=90.29%, window_entropy_acc=90.91%]  Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 6.726518271118829
    Answer: 5600
    Ground truth:  5600
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 6.726518271118829
    Answer: 5600
    Ground truth:  5600
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 5.318164917570343
    Answer: 5600
    Ground truth:  5600
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 0.875
    Answer: 5600
    Ground truth:  5600
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 11.36328125
    Answer: 5600
    Ground truth:  5600
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 1.9013336300849915
    Answer: 5600
    Ground truth:  5600
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 1.7806071937084198
    Answer: 5600
    Ground truth:  5600
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 1.7478221654891968
    Answer: 5600
    Ground truth:  5600
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to first calculate the amount of milk that Mr. Ma...
    Score: 4.72424441576004
    Answer: 5600
    Ground truth:  5600
Method name: attention_weighted_confidence, running accuracy: 90.9278350515464
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.9278350515464
Method name: cer_prob_product_log_last, running accuracy: 90.30927835051547
Method name: self_consistency, running accuracy: 91.1340206185567
Method name: p_true, running accuracy: 91.34020618556701
Method name: normilized_likelihood, running accuracy: 90.51546391752578
Method name: normilized_entropy, running accuracy: 90.30927835051547
Method name: topk_entropy, running accuracy: 90.30927835051547
Method name: window_entropy, running accuracy: 90.9278350515464
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 484/500 [39:06:54<50:48, 190.51s/it, attention_weighted_confidence_acc=90.93%, cer_entropy_weighted_mean_all_acc=90.93%, cer_prob_product_log_last_acc=90.31%, self_consistency_acc=91.13%, p_true_acc=91.34%, normilized_likelihood_acc=90.52%, normilized_entropy_acc=90.31%, topk_entropy_acc=90.31%, window_entropy_acc=90.93%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 485/500 [39:06:54<47:45, 191.04s/it, attention_weighted_confidence_acc=90.93%, cer_entropy_weighted_mean_all_acc=90.93%, cer_prob_product_log_last_acc=90.31%, self_consistency_acc=91.13%, p_true_acc=91.34%, normilized_likelihood_acc=90.52%, normilized_entropy_acc=90.31%, topk_entropy_acc=90.31%, window_entropy_acc=90.93%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 5.4422101258076365
    Answer: 2
    Ground truth:  17
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 5.4422101258076365
    Answer: 2
    Ground truth:  17
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 9.65186733007431
    Answer: 2
    Ground truth:  17
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 0.6875
    Answer: 2
    Ground truth:  17
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 9.390625
    Answer: 2
    Ground truth:  17
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 2.5464126467704773
    Answer: 2
    Ground truth:  17
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 2.1418584138154984
    Answer: 2
    Ground truth:  17
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 2.0598197281360626
    Answer: 2
    Ground truth:  17
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

We know that four friends ordered fo...
    Score: 5.808002948760986
    Answer: 2
    Ground truth:  17
Method name: attention_weighted_confidence, running accuracy: 90.74074074074075
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.74074074074075
Method name: cer_prob_product_log_last, running accuracy: 90.12345679012346
Method name: self_consistency, running accuracy: 90.94650205761316
Method name: p_true, running accuracy: 91.1522633744856
Method name: normilized_likelihood, running accuracy: 90.3292181069959
Method name: normilized_entropy, running accuracy: 90.12345679012346
Method name: topk_entropy, running accuracy: 90.12345679012346
Method name: window_entropy, running accuracy: 90.74074074074075
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 485/500 [39:09:45<47:45, 191.04s/it, attention_weighted_confidence_acc=90.74%, cer_entropy_weighted_mean_all_acc=90.74%, cer_prob_product_log_last_acc=90.12%, self_consistency_acc=90.95%, p_true_acc=91.15%, normilized_likelihood_acc=90.33%, normilized_entropy_acc=90.12%, topk_entropy_acc=90.12%, window_entropy_acc=90.74%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 486/500 [39:09:45<43:08, 184.92s/it, attention_weighted_confidence_acc=90.74%, cer_entropy_weighted_mean_all_acc=90.74%, cer_prob_product_log_last_acc=90.12%, self_consistency_acc=90.95%, p_true_acc=91.15%, normilized_likelihood_acc=90.33%, normilized_entropy_acc=90.12%, topk_entropy_acc=90.12%, window_entropy_acc=90.74%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 8.099426570244216
    Answer: 30
    Ground truth:  30
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 8.099426570244216
    Answer: 30
    Ground truth:  30
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 14.053078293800354
    Answer: 30
    Ground truth:  30
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 0.9375
    Answer: 30
    Ground truth:  30
Method 5: p_true
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 12.75390625
    Answer: 30
    Ground truth:  30
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 0.985375665128231
    Answer: 30
    Ground truth:  30
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 1.0628478229045868
    Answer: 30
    Ground truth:  30
Method 8: topk_entropy
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 1.0615714192390442
    Answer: 30
    Ground truth:  30
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of unoccupied units in the building, we'll need to calc...
    Score: 8.346434444189072
    Answer: 30
    Ground truth:  30
Method name: attention_weighted_confidence, running accuracy: 90.75975359342917
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.75975359342917
Method name: cer_prob_product_log_last, running accuracy: 90.14373716632443
Method name: self_consistency, running accuracy: 90.96509240246407
Method name: p_true, running accuracy: 91.17043121149896
Method name: normilized_likelihood, running accuracy: 90.34907597535934
Method name: normilized_entropy, running accuracy: 90.14373716632443
Method name: topk_entropy, running accuracy: 90.14373716632443
Method name: window_entropy, running accuracy: 90.75975359342917
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 486/500 [39:12:47<43:08, 184.92s/it, attention_weighted_confidence_acc=90.76%, cer_entropy_weighted_mean_all_acc=90.76%, cer_prob_product_log_last_acc=90.14%, self_consistency_acc=90.97%, p_true_acc=91.17%, normilized_likelihood_acc=90.35%, normilized_entropy_acc=90.14%, topk_entropy_acc=90.14%, window_entropy_acc=90.76%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 487/500 [39:12:47<39:54, 184.18s/it, attention_weighted_confidence_acc=90.76%, cer_entropy_weighted_mean_all_acc=90.76%, cer_prob_product_log_last_acc=90.14%, self_consistency_acc=90.97%, p_true_acc=91.17%, normilized_likelihood_acc=90.35%, normilized_entropy_acc=90.14%, topk_entropy_acc=90.14%, window_entropy_acc=90.76%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: First, we need to find the area of the bedroom in square feet. 

Area = length *...
    Score: 1.0858144643878922
    Answer: 11332
    Ground truth:  11232
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, we need to find the area of the bedroom in square feet. 

Area = length *...
    Score: 1.0858144643878922
    Answer: 11332
    Ground truth:  11232
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To calculate the total cost for Michael to replace the carpet, we need to find t...
    Score: 0.9992097022327968
    Answer: 11232
    Ground truth:  11232
Method 4: self_consistency
  Batch 1:
    Text: To calculate the total cost for Michael to replace the carpet, we need to find t...
    Score: 0.125
    Answer: 11232
    Ground truth:  11232
Method 5: p_true
  Batch 1:
    Text: First, we need to find the area of the bedroom in square feet. 

Area = length *...
    Score: 1.65234375
    Answer: 11332
    Ground truth:  11232
Method 6: normilized_likelihood
  Batch 1:
    Text: First, we need to find the area of the bedroom in square feet. 

Area = length *...
    Score: 0.43577926605939865
    Answer: 11332
    Ground truth:  11232
Method 7: normilized_entropy
  Batch 1:
    Text: To calculate the total cost for Michael to replace the carpet, we need to find t...
    Score: 0.4343656748533249
    Answer: 11232
    Ground truth:  11232
Method 8: topk_entropy
  Batch 1:
    Text: To calculate the total cost for Michael to replace the carpet, we need to find t...
    Score: 0.42430511116981506
    Answer: 11232
    Ground truth:  11232
Method 9: window_entropy
  Batch 1:
    Text: First, we need to find the area of the bedroom in square feet. 

Area = length *...
    Score: 2.525983691215515
    Answer: 11332
    Ground truth:  11232
Method name: attention_weighted_confidence, running accuracy: 90.77868852459017
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.77868852459017
Method name: cer_prob_product_log_last, running accuracy: 90.1639344262295
Method name: self_consistency, running accuracy: 90.98360655737704
Method name: p_true, running accuracy: 91.18852459016394
Method name: normilized_likelihood, running accuracy: 90.3688524590164
Method name: normilized_entropy, running accuracy: 90.1639344262295
Method name: topk_entropy, running accuracy: 90.1639344262295
Method name: window_entropy, running accuracy: 90.77868852459017
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  97%|█████████▋| 487/500 [39:17:46<39:54, 184.18s/it, attention_weighted_confidence_acc=90.78%, cer_entropy_weighted_mean_all_acc=90.78%, cer_prob_product_log_last_acc=90.16%, self_consistency_acc=90.98%, p_true_acc=91.19%, normilized_likelihood_acc=90.37%, normilized_entropy_acc=90.16%, topk_entropy_acc=90.16%, window_entropy_acc=90.78%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 488/500 [39:17:46<43:43, 218.62s/it, attention_weighted_confidence_acc=90.78%, cer_entropy_weighted_mean_all_acc=90.78%, cer_prob_product_log_last_acc=90.16%, self_consistency_acc=90.98%, p_true_acc=91.19%, normilized_likelihood_acc=90.37%, normilized_entropy_acc=90.16%, topk_entropy_acc=90.16%, window_entropy_acc=90.78%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 4.577171855084385
    Answer: 330000
    Ground truth:  330000
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 4.577171855084385
    Answer: 330000
    Ground truth:  330000
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 0.0
    Answer: 330000
    Ground truth:  330000
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 0.5625
    Answer: 330000
    Ground truth:  330000
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 7.17578125
    Answer: 330000
    Ground truth:  330000
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 1.8323918581008911
    Answer: 330000
    Ground truth:  330000
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 1.008822962641716
    Answer: 330000
    Ground truth:  330000
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 1.0041241943836212
    Answer: 330000
    Ground truth:  330000
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1. The initial cost of the plane is ...
    Score: 5.114881455898285
    Answer: 330000
    Ground truth:  330000
Method name: attention_weighted_confidence, running accuracy: 90.79754601226993
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.79754601226993
Method name: cer_prob_product_log_last, running accuracy: 90.1840490797546
Method name: self_consistency, running accuracy: 91.00204498977506
Method name: p_true, running accuracy: 91.20654396728017
Method name: normilized_likelihood, running accuracy: 90.38854805725971
Method name: normilized_entropy, running accuracy: 90.1840490797546
Method name: topk_entropy, running accuracy: 90.1840490797546
Method name: window_entropy, running accuracy: 90.79754601226993
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 488/500 [39:21:17<43:43, 218.62s/it, attention_weighted_confidence_acc=90.80%, cer_entropy_weighted_mean_all_acc=90.80%, cer_prob_product_log_last_acc=90.18%, self_consistency_acc=91.00%, p_true_acc=91.21%, normilized_likelihood_acc=90.39%, normilized_entropy_acc=90.18%, topk_entropy_acc=90.18%, window_entropy_acc=90.80%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 489/500 [39:21:17<39:39, 216.30s/it, attention_weighted_confidence_acc=90.80%, cer_entropy_weighted_mean_all_acc=90.80%, cer_prob_product_log_last_acc=90.18%, self_consistency_acc=91.00%, p_true_acc=91.21%, normilized_likelihood_acc=90.39%, normilized_entropy_acc=90.18%, topk_entropy_acc=90.18%, window_entropy_acc=90.80%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 7.73089855781406
    Answer: 32
    Ground truth:  32
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 7.73089855781406
    Answer: 32
    Ground truth:  32
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 14.987356841564178
    Answer: 32
    Ground truth:  32
Method 4: self_consistency
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 0.9375
    Answer: 32
    Ground truth:  32
Method 5: p_true
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 13.9453125
    Answer: 32
    Ground truth:  32
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 3.4105000644922256
    Answer: 32
    Ground truth:  32
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 3.1684731394052505
    Answer: 32
    Ground truth:  32
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 2.96333085000515
    Answer: 32
    Ground truth:  32
Method 9: window_entropy
  Batch 1:
    Text: To find out how much each person paid, we first need to calculate the total cost...
    Score: 7.938248157501221
    Answer: 32
    Ground truth:  32
Method name: attention_weighted_confidence, running accuracy: 90.81632653061224
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.81632653061224
Method name: cer_prob_product_log_last, running accuracy: 90.20408163265307
Method name: self_consistency, running accuracy: 91.02040816326532
Method name: p_true, running accuracy: 91.22448979591836
Method name: normilized_likelihood, running accuracy: 90.40816326530611
Method name: normilized_entropy, running accuracy: 90.20408163265307
Method name: topk_entropy, running accuracy: 90.20408163265307
Method name: window_entropy, running accuracy: 90.81632653061224
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 489/500 [39:23:45<39:39, 216.30s/it, attention_weighted_confidence_acc=90.82%, cer_entropy_weighted_mean_all_acc=90.82%, cer_prob_product_log_last_acc=90.20%, self_consistency_acc=91.02%, p_true_acc=91.22%, normilized_likelihood_acc=90.41%, normilized_entropy_acc=90.20%, topk_entropy_acc=90.20%, window_entropy_acc=90.82%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 490/500 [39:23:45<32:37, 195.70s/it, attention_weighted_confidence_acc=90.82%, cer_entropy_weighted_mean_all_acc=90.82%, cer_prob_product_log_last_acc=90.20%, self_consistency_acc=91.02%, p_true_acc=91.22%, normilized_likelihood_acc=90.41%, normilized_entropy_acc=90.20%, topk_entropy_acc=90.20%, window_entropy_acc=90.82%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 4.176563833156734
    Answer: 60
    Ground truth:  60
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 4.176563833156734
    Answer: 60
    Ground truth:  60
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 8.985055088996887
    Answer: 60
    Ground truth:  60
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 0.5625
    Answer: 60
    Ground truth:  60
Method 5: p_true
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 7.9453125
    Answer: 60
    Ground truth:  60
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 6.140868157148361
    Answer: 60
    Ground truth:  60
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 5.059283912181854
    Answer: 60
    Ground truth:  60
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 4.369529455900192
    Answer: 60
    Ground truth:  60
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step:

1. Martha started collecting shells at the age of 5. ...
    Score: 10.018815040588379
    Answer: 60
    Ground truth:  60
Method name: attention_weighted_confidence, running accuracy: 90.83503054989816
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.83503054989816
Method name: cer_prob_product_log_last, running accuracy: 90.22403258655804
Method name: self_consistency, running accuracy: 91.0386965376782
Method name: p_true, running accuracy: 91.24236252545825
Method name: normilized_likelihood, running accuracy: 90.4276985743381
Method name: normilized_entropy, running accuracy: 90.22403258655804
Method name: topk_entropy, running accuracy: 90.22403258655804
Method name: window_entropy, running accuracy: 90.83503054989816
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 490/500 [39:26:42<32:37, 195.70s/it, attention_weighted_confidence_acc=90.84%, cer_entropy_weighted_mean_all_acc=90.84%, cer_prob_product_log_last_acc=90.22%, self_consistency_acc=91.04%, p_true_acc=91.24%, normilized_likelihood_acc=90.43%, normilized_entropy_acc=90.22%, topk_entropy_acc=90.22%, window_entropy_acc=90.84%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 491/500 [39:26:42<28:31, 190.15s/it, attention_weighted_confidence_acc=90.84%, cer_entropy_weighted_mean_all_acc=90.84%, cer_prob_product_log_last_acc=90.22%, self_consistency_acc=91.04%, p_true_acc=91.24%, normilized_likelihood_acc=90.43%, normilized_entropy_acc=90.22%, topk_entropy_acc=90.22%, window_entropy_acc=90.84%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 7.974482993898162
    Answer: 12
    Ground truth:  12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 7.974482993898162
    Answer: 12
    Ground truth:  12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 15.984573781490326
    Answer: 12
    Ground truth:  12
Method 4: self_consistency
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 1.0
    Answer: 12
    Ground truth:  12
Method 5: p_true
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 14.0625
    Answer: 12
    Ground truth:  12
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 3.112886741757393
    Answer: 12
    Ground truth:  12
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 2.234911695122719
    Answer: 12
    Ground truth:  12
Method 8: topk_entropy
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 2.0768971741199493
    Answer: 12
    Ground truth:  12
Method 9: window_entropy
  Batch 1:
    Text: To find out how many inches of ribbon Monica can use for each gift bow, we first...
    Score: 12.918253421783447
    Answer: 12
    Ground truth:  12
Method name: attention_weighted_confidence, running accuracy: 90.85365853658537
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.85365853658537
Method name: cer_prob_product_log_last, running accuracy: 90.2439024390244
Method name: self_consistency, running accuracy: 91.05691056910568
Method name: p_true, running accuracy: 91.26016260162602
Method name: normilized_likelihood, running accuracy: 90.4471544715447
Method name: normilized_entropy, running accuracy: 90.2439024390244
Method name: topk_entropy, running accuracy: 90.2439024390244
Method name: window_entropy, running accuracy: 90.85365853658537
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 491/500 [39:29:26<28:31, 190.15s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.24%, self_consistency_acc=91.06%, p_true_acc=91.26%, normilized_likelihood_acc=90.45%, normilized_entropy_acc=90.24%, topk_entropy_acc=90.24%, window_entropy_acc=90.85%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 492/500 [39:29:26<24:17, 182.24s/it, attention_weighted_confidence_acc=90.85%, cer_entropy_weighted_mean_all_acc=90.85%, cer_prob_product_log_last_acc=90.24%, self_consistency_acc=91.06%, p_true_acc=91.26%, normilized_likelihood_acc=90.45%, normilized_entropy_acc=90.24%, topk_entropy_acc=90.24%, window_entropy_acc=90.85%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 7.835596734185201
    Answer: 100
    Ground truth:  100
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 7.835596734185201
    Answer: 100
    Ground truth:  100
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 15.376054644584656
    Answer: 100
    Ground truth:  100
Method 4: self_consistency
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 1.0
    Answer: 100
    Ground truth:  100
Method 5: p_true
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 13.84765625
    Answer: 100
    Ground truth:  100
Method 6: normilized_likelihood
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 2.541692227125168
    Answer: 100
    Ground truth:  100
Method 7: normilized_entropy
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 1.5012450367212296
    Answer: 100
    Ground truth:  100
Method 8: topk_entropy
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 1.4583265632390976
    Answer: 100
    Ground truth:  100
Method 9: window_entropy
  Batch 1:
    Text: To find Mark's initial weight, we first need to calculate the total weight he lo...
    Score: 3.8297443985939026
    Answer: 100
    Ground truth:  100
Method name: attention_weighted_confidence, running accuracy: 90.87221095334685
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.87221095334685
Method name: cer_prob_product_log_last, running accuracy: 90.26369168356997
Method name: self_consistency, running accuracy: 91.07505070993915
Method name: p_true, running accuracy: 91.27789046653145
Method name: normilized_likelihood, running accuracy: 90.46653144016227
Method name: normilized_entropy, running accuracy: 90.26369168356997
Method name: topk_entropy, running accuracy: 90.26369168356997
Method name: window_entropy, running accuracy: 90.87221095334685
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  98%|█████████▊| 492/500 [39:31:32<24:17, 182.24s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.26%, self_consistency_acc=91.08%, p_true_acc=91.28%, normilized_likelihood_acc=90.47%, normilized_entropy_acc=90.26%, topk_entropy_acc=90.26%, window_entropy_acc=90.87%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▊| 493/500 [39:31:32<19:18, 165.54s/it, attention_weighted_confidence_acc=90.87%, cer_entropy_weighted_mean_all_acc=90.87%, cer_prob_product_log_last_acc=90.26%, self_consistency_acc=91.08%, p_true_acc=91.28%, normilized_likelihood_acc=90.47%, normilized_entropy_acc=90.26%, topk_entropy_acc=90.26%, window_entropy_acc=90.87%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Can not find final answer index
Can not find final answer index
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 3.3318083658450863
    Answer: 3.5
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 3.3318083658450863
    Answer: 3.5
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the average loss, we need to find the total loss for the day and then di...
    Score: 0.6197524467475315
    Answer: 7.17
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 0.4375
    Answer: 3.5
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 5.408203125
    Answer: 3.5
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 4.78778550028801
    Answer: 3.5
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 4.703170299530029
    Answer: 3.5
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 4.119176834821701
    Answer: 3.5
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: Let's think step by step to solve this problem.

The total amount Johnny's dad l...
    Score: 6.601295471191406
    Answer: 3.5
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 90.68825910931174
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.68825910931174
Method name: cer_prob_product_log_last, running accuracy: 90.08097165991903
Method name: self_consistency, running accuracy: 90.89068825910931
Method name: p_true, running accuracy: 91.09311740890689
Method name: normilized_likelihood, running accuracy: 90.2834008097166
Method name: normilized_entropy, running accuracy: 90.08097165991903
Method name: topk_entropy, running accuracy: 90.08097165991903
Method name: window_entropy, running accuracy: 90.68825910931174
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▊| 493/500 [39:36:08<19:18, 165.54s/it, attention_weighted_confidence_acc=90.69%, cer_entropy_weighted_mean_all_acc=90.69%, cer_prob_product_log_last_acc=90.08%, self_consistency_acc=90.89%, p_true_acc=91.09%, normilized_likelihood_acc=90.28%, normilized_entropy_acc=90.08%, topk_entropy_acc=90.08%, window_entropy_acc=90.69%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 494/500 [39:36:08<19:50, 198.49s/it, attention_weighted_confidence_acc=90.69%, cer_entropy_weighted_mean_all_acc=90.69%, cer_prob_product_log_last_acc=90.08%, self_consistency_acc=90.89%, p_true_acc=91.09%, normilized_likelihood_acc=90.28%, normilized_entropy_acc=90.08%, topk_entropy_acc=90.08%, window_entropy_acc=90.69%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 2.4812840741978044
    Answer: 6
    Ground truth:  3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 2.4812840741978044
    Answer: 6
    Ground truth:  3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 4.844536740449257
    Answer: 6
    Ground truth:  3
Method 4: self_consistency
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 0.375
    Answer: 6
    Ground truth:  3
Method 5: p_true
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 1.9228515625
    Answer: 6
    Ground truth:  3
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 4.760355770587921
    Answer: 6
    Ground truth:  3
Method 7: normilized_entropy
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 4.754788935184479
    Answer: 6
    Ground truth:  3
Method 8: topk_entropy
  Batch 1:
    Text: To determine the number of groups needed, we need to find how many students are ...
    Score: 3.9975433945655823
    Answer: 6
    Ground truth:  3
Method 9: window_entropy
  Batch 1:
    Text: To find the number of students per group, divide the total number of students (5...
    Score: 2.685241997241974
    Answer: 54
    Ground truth:  3
Method name: attention_weighted_confidence, running accuracy: 90.5050505050505
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.5050505050505
Method name: cer_prob_product_log_last, running accuracy: 89.8989898989899
Method name: self_consistency, running accuracy: 90.7070707070707
Method name: p_true, running accuracy: 90.9090909090909
Method name: normilized_likelihood, running accuracy: 90.1010101010101
Method name: normilized_entropy, running accuracy: 89.8989898989899
Method name: topk_entropy, running accuracy: 89.8989898989899
Method name: window_entropy, running accuracy: 90.5050505050505
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 494/500 [39:41:08<19:50, 198.49s/it, attention_weighted_confidence_acc=90.51%, cer_entropy_weighted_mean_all_acc=90.51%, cer_prob_product_log_last_acc=89.90%, self_consistency_acc=90.71%, p_true_acc=90.91%, normilized_likelihood_acc=90.10%, normilized_entropy_acc=89.90%, topk_entropy_acc=89.90%, window_entropy_acc=90.51%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 495/500 [39:41:08<19:05, 229.04s/it, attention_weighted_confidence_acc=90.51%, cer_entropy_weighted_mean_all_acc=90.51%, cer_prob_product_log_last_acc=89.90%, self_consistency_acc=90.71%, p_true_acc=90.91%, normilized_likelihood_acc=90.10%, normilized_entropy_acc=89.90%, topk_entropy_acc=89.90%, window_entropy_acc=90.51%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 7.745573702912591
    Answer: 306
    Ground truth:  306
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 7.745573702912591
    Answer: 306
    Ground truth:  306
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 15.735274374485016
    Answer: 306
    Ground truth:  306
Method 4: self_consistency
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 1.0
    Answer: 306
    Ground truth:  306
Method 5: p_true
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 13.828125
    Answer: 306
    Ground truth:  306
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 2.8520316034555435
    Answer: 306
    Ground truth:  306
Method 7: normilized_entropy
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 1.9980902522802353
    Answer: 306
    Ground truth:  306
Method 8: topk_entropy
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 1.9884370416402817
    Answer: 306
    Ground truth:  306
Method 9: window_entropy
  Batch 1:
    Text: To find the total cost of the coal, we first need to determine the number of bag...
    Score: 4.134076535701752
    Answer: 306
    Ground truth:  306
Method name: attention_weighted_confidence, running accuracy: 90.5241935483871
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.5241935483871
Method name: cer_prob_product_log_last, running accuracy: 89.91935483870968
Method name: self_consistency, running accuracy: 90.7258064516129
Method name: p_true, running accuracy: 90.92741935483872
Method name: normilized_likelihood, running accuracy: 90.12096774193549
Method name: normilized_entropy, running accuracy: 89.91935483870968
Method name: topk_entropy, running accuracy: 89.91935483870968
Method name: window_entropy, running accuracy: 90.5241935483871
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 495/500 [39:43:26<19:05, 229.04s/it, attention_weighted_confidence_acc=90.52%, cer_entropy_weighted_mean_all_acc=90.52%, cer_prob_product_log_last_acc=89.92%, self_consistency_acc=90.73%, p_true_acc=90.93%, normilized_likelihood_acc=90.12%, normilized_entropy_acc=89.92%, topk_entropy_acc=89.92%, window_entropy_acc=90.52%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 496/500 [39:43:26<13:26, 201.73s/it, attention_weighted_confidence_acc=90.52%, cer_entropy_weighted_mean_all_acc=90.52%, cer_prob_product_log_last_acc=89.92%, self_consistency_acc=90.73%, p_true_acc=90.93%, normilized_likelihood_acc=90.12%, normilized_entropy_acc=89.92%, topk_entropy_acc=89.92%, window_entropy_acc=90.52%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Can not find final answer index
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 5.927216731045872
    Answer: 200
    Ground truth:  200
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 5.927216731045872
    Answer: 200
    Ground truth:  200
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 11.998947381973267
    Answer: 200
    Ground truth:  200
Method 4: self_consistency
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 0.75
    Answer: 200
    Ground truth:  200
Method 5: p_true
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 10.7265625
    Answer: 200
    Ground truth:  200
Method 6: normilized_likelihood
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 17.307933032512665
    Answer: 200
    Ground truth:  200
Method 7: normilized_entropy
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 16.88772913813591
    Answer: 200
    Ground truth:  200
Method 8: topk_entropy
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 14.086861729621887
    Answer: 200
    Ground truth:  200
Method 9: window_entropy
  Batch 1:
    Text: To find out how much Tom has to pay per month, we need to determine how much he ...
    Score: 20.839346885681152
    Answer: 200
    Ground truth:  200
Method name: attention_weighted_confidence, running accuracy: 90.54325955734407
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.54325955734407
Method name: cer_prob_product_log_last, running accuracy: 89.93963782696177
Method name: self_consistency, running accuracy: 90.74446680080483
Method name: p_true, running accuracy: 90.94567404426559
Method name: normilized_likelihood, running accuracy: 90.14084507042254
Method name: normilized_entropy, running accuracy: 89.93963782696177
Method name: topk_entropy, running accuracy: 89.93963782696177
Method name: window_entropy, running accuracy: 90.54325955734407
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 496/500 [39:47:06<13:26, 201.73s/it, attention_weighted_confidence_acc=90.54%, cer_entropy_weighted_mean_all_acc=90.54%, cer_prob_product_log_last_acc=89.94%, self_consistency_acc=90.74%, p_true_acc=90.95%, normilized_likelihood_acc=90.14%, normilized_entropy_acc=89.94%, topk_entropy_acc=89.94%, window_entropy_acc=90.54%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 497/500 [39:47:06<10:22, 207.34s/it, attention_weighted_confidence_acc=90.54%, cer_entropy_weighted_mean_all_acc=90.54%, cer_prob_product_log_last_acc=89.94%, self_consistency_acc=90.74%, p_true_acc=90.95%, normilized_likelihood_acc=90.14%, normilized_entropy_acc=89.94%, topk_entropy_acc=89.94%, window_entropy_acc=90.54%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 7.713676815039834
    Answer: 160
    Ground truth:  160
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 7.713676815039834
    Answer: 160
    Ground truth:  160
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 14.436756491661072
    Answer: 160
    Ground truth:  160
Method 4: self_consistency
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 0.9375
    Answer: 160
    Ground truth:  160
Method 5: p_true
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 6.1456298828125
    Answer: 160
    Ground truth:  160
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 2.5407701283693314
    Answer: 160
    Ground truth:  160
Method 7: normilized_entropy
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 1.116059958934784
    Answer: 160
    Ground truth:  160
Method 8: topk_entropy
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 1.1150221228599548
    Answer: 160
    Ground truth:  160
Method 9: window_entropy
  Batch 1:
    Text: To find the cost of the red car, first, we need to calculate 40% of the cost of ...
    Score: 5.975905418395996
    Answer: 160
    Ground truth:  160
Method name: attention_weighted_confidence, running accuracy: 90.56224899598394
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.56224899598394
Method name: cer_prob_product_log_last, running accuracy: 89.95983935742971
Method name: self_consistency, running accuracy: 90.76305220883533
Method name: p_true, running accuracy: 90.96385542168674
Method name: normilized_likelihood, running accuracy: 90.16064257028113
Method name: normilized_entropy, running accuracy: 89.95983935742971
Method name: topk_entropy, running accuracy: 89.95983935742971
Method name: window_entropy, running accuracy: 90.56224899598394
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  99%|█████████▉| 497/500 [39:49:59<10:22, 207.34s/it, attention_weighted_confidence_acc=90.56%, cer_entropy_weighted_mean_all_acc=90.56%, cer_prob_product_log_last_acc=89.96%, self_consistency_acc=90.76%, p_true_acc=90.96%, normilized_likelihood_acc=90.16%, normilized_entropy_acc=89.96%, topk_entropy_acc=89.96%, window_entropy_acc=90.56%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct: 100%|█████████▉| 498/500 [39:49:59<06:33, 196.82s/it, attention_weighted_confidence_acc=90.56%, cer_entropy_weighted_mean_all_acc=90.56%, cer_prob_product_log_last_acc=89.96%, self_consistency_acc=90.76%, p_true_acc=90.96%, normilized_likelihood_acc=90.16%, normilized_entropy_acc=89.96%, topk_entropy_acc=89.96%, window_entropy_acc=90.56%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 7.914776567103612
    Answer: 16
    Ground truth:  16
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 7.914776567103612
    Answer: 16
    Ground truth:  16
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 15.999924778938293
    Answer: 16
    Ground truth:  16
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 1.0
    Answer: 16
    Ground truth:  16
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 15.0703125
    Answer: 16
    Ground truth:  16
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 1.6511919498443604
    Answer: 16
    Ground truth:  16
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 1.7077974826097488
    Answer: 16
    Ground truth:  16
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 1.702144905924797
    Answer: 16
    Ground truth:  16
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step.

1. Sarah has 9 books.
2. Joseph has ...
    Score: 7.19723978638649
    Answer: 16
    Ground truth:  16
Method name: attention_weighted_confidence, running accuracy: 90.5811623246493
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.5811623246493
Method name: cer_prob_product_log_last, running accuracy: 89.97995991983969
Method name: self_consistency, running accuracy: 90.78156312625251
Method name: p_true, running accuracy: 90.98196392785572
Method name: normilized_likelihood, running accuracy: 90.18036072144288
Method name: normilized_entropy, running accuracy: 89.97995991983969
Method name: topk_entropy, running accuracy: 89.97995991983969
Method name: window_entropy, running accuracy: 90.5811623246493
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct: 100%|█████████▉| 498/500 [39:51:43<06:33, 196.82s/it, attention_weighted_confidence_acc=90.58%, cer_entropy_weighted_mean_all_acc=90.58%, cer_prob_product_log_last_acc=89.98%, self_consistency_acc=90.78%, p_true_acc=90.98%, normilized_likelihood_acc=90.18%, normilized_entropy_acc=89.98%, topk_entropy_acc=89.98%, window_entropy_acc=90.58%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct: 100%|█████████▉| 499/500 [39:51:43<02:49, 169.12s/it, attention_weighted_confidence_acc=90.58%, cer_entropy_weighted_mean_all_acc=90.58%, cer_prob_product_log_last_acc=89.98%, self_consistency_acc=90.78%, p_true_acc=90.98%, normilized_likelihood_acc=90.18%, normilized_entropy_acc=89.98%, topk_entropy_acc=89.98%, window_entropy_acc=90.58%]Group name: sampling
Current method: attention_weighted_confidence, method config: {'decoding_mode': '', 'method': 'attention_weighted_confidence', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: attention_weighted_confidence
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 8.381593815560365
    Answer: 70
    Ground truth:  70
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 8.381593815560365
    Answer: 70
    Ground truth:  70
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 15.999578416347504
    Answer: 70
    Ground truth:  70
Method 4: self_consistency
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 1.0
    Answer: 70
    Ground truth:  70
Method 5: p_true
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 14.90625
    Answer: 70
    Ground truth:  70
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 2.110592558979988
    Answer: 70
    Ground truth:  70
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 1.7929593175649643
    Answer: 70
    Ground truth:  70
Method 8: topk_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 1.7723851054906845
    Answer: 70
    Ground truth:  70
Method 9: window_entropy
  Batch 1:
    Text: Let's break down the problem step by step:

1.  Let the number of silver coins b...
    Score: 4.656008094549179
    Answer: 70
    Ground truth:  70
Method name: attention_weighted_confidence, running accuracy: 90.60000000000001
Method name: cer_entropy_weighted_mean_all, running accuracy: 90.60000000000001
Method name: cer_prob_product_log_last, running accuracy: 90.0
Method name: self_consistency, running accuracy: 90.8
Method name: p_true, running accuracy: 91.0
Method name: normilized_likelihood, running accuracy: 90.2
Method name: normilized_entropy, running accuracy: 90.0
Method name: topk_entropy, running accuracy: 90.0
Method name: window_entropy, running accuracy: 90.60000000000001
Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct: 100%|█████████▉| 499/500 [39:55:02<02:49, 169.12s/it, attention_weighted_confidence_acc=90.60%, cer_entropy_weighted_mean_all_acc=90.60%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.80%, p_true_acc=91.00%, normilized_likelihood_acc=90.20%, normilized_entropy_acc=90.00%, topk_entropy_acc=90.00%, window_entropy_acc=90.60%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct: 100%|██████████| 500/500 [39:55:02<00:00, 178.17s/it, attention_weighted_confidence_acc=90.60%, cer_entropy_weighted_mean_all_acc=90.60%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.80%, p_true_acc=91.00%, normilized_likelihood_acc=90.20%, normilized_entropy_acc=90.00%, topk_entropy_acc=90.00%, window_entropy_acc=90.60%]Processing gsm8k__data_sunqiao_projects_models_Llama-3.1-8B-Instruct: 100%|██████████| 500/500 [39:55:02<00:00, 287.41s/it, attention_weighted_confidence_acc=90.60%, cer_entropy_weighted_mean_all_acc=90.60%, cer_prob_product_log_last_acc=90.00%, self_consistency_acc=90.80%, p_true_acc=91.00%, normilized_likelihood_acc=90.20%, normilized_entropy_acc=90.00%, topk_entropy_acc=90.00%, window_entropy_acc=90.60%]

=== Final Accuracies ===
attention_weighted_confidence: 90.60%
cer_entropy_weighted_mean_all: 90.60%
cer_prob_product_log_last: 90.00%
self_consistency: 90.80%
p_true: 91.00%
normilized_likelihood: 90.20%
normilized_entropy: 90.00%
topk_entropy: 90.00%
window_entropy: 90.60%
Finished evaluating: gsm8k
==================================================


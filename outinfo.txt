==================================================
Configurations:
Model name: /data/sunqiao/projects/models/Llama-3.1-8B-Instruct
Lingua model name: microsoft/llmlingua-2-xlm-roberta-large-meetingbank
Aggregate: True
k: 16
Number of samples: 100
Seed: 102
Data directory: data
Batch size: 1
Dataset files: {'math': 'src_custom_datasets_math_dataset_test_processed.parquet'}
==================================================

`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
`torch_dtype` is deprecated! Use `dtype` instead!

Dataset name: math
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 0/100 [00:00<?, ?it/s]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 5.591837644577026
    Answer: 4
    Ground truth: 4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 13.99972768291826
    Answer: 4
    Ground truth: 4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 13.999748945236206
    Answer: 4
    Ground truth: 4
Method 4: self_consistency
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 1.0
    Answer: 4
    Ground truth: 4
Method 5: p_true
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 8.3046875
    Answer: 4
    Ground truth: 4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 9.060942992568016
    Answer: 4
    Ground truth: 4
Method 7: normilized_entropy
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 8.20901882648468
    Answer: 4
    Ground truth: 4
Method 8: topk_entropy
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 7.311453610658646
    Answer: 4
    Ground truth: 4
Method 9: window_entropy
  Batch 1:
    Text: To find the prime factorization of $117\cdot119$, we first need to find the prim...
    Score: 11.684480249881744
    Answer: 4
    Ground truth: 4

==================================================
Method name: group_entropy, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   0%|          | 0/100 [03:17<?, ?it/s, group_entropy_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 1/100 [03:17<5:25:19, 197.16s/it, group_entropy_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 1.9286643266677856
    Answer: 215
    Ground truth: 215
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 4.973262442073331
    Answer: 215
    Ground truth: 215
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 4.94202733039856
    Answer: 215
    Ground truth: 215
Method 4: self_consistency
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 0.5
    Answer: 215
    Ground truth: 215
Method 5: p_true
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 4.265625
    Answer: 215
    Ground truth: 215
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 2.231070399284363
    Answer: 215
    Ground truth: 215
Method 7: normilized_entropy
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 2.1738677620887756
    Answer: 215
    Ground truth: 215
Method 8: topk_entropy
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 1.805268108844757
    Answer: 215
    Ground truth: 215
Method 9: window_entropy
  Batch 1:
    Text: To find the smallest number of stamps your friend can have, we need to determine...
    Score: 3.8510228395462036
    Answer: 215
    Ground truth: 215

==================================================
Method name: group_entropy, running accuracy: 100.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 100.0
Method name: cer_prob_product_log_last, running accuracy: 100.0
Method name: self_consistency, running accuracy: 100.0
Method name: p_true, running accuracy: 100.0
Method name: normilized_likelihood, running accuracy: 100.0
Method name: normilized_entropy, running accuracy: 100.0
Method name: topk_entropy, running accuracy: 100.0
Method name: window_entropy, running accuracy: 100.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   1%|          | 1/100 [09:16<5:25:19, 197.16s/it, group_entropy_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 2/100 [09:16<7:58:03, 292.69s/it, group_entropy_acc=100.00%, cer_entropy_weighted_mean_all_acc=100.00%, cer_prob_product_log_last_acc=100.00%, self_consistency_acc=100.00%, p_true_acc=100.00%, normilized_likelihood_acc=100.00%, normilized_entropy_acc=100.00%, topk_entropy_acc=100.00%, window_entropy_acc=100.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 1.4115822315216064
    Answer: 11
    Ground truth: 13
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 6.8406081518842345
    Answer: 11
    Ground truth: 13
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 6.367206156253815
    Answer: 11
    Ground truth: 13
Method 4: self_consistency
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 0.5
    Answer: 11
    Ground truth: 13
Method 5: p_true
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 4.3876953125
    Answer: 11
    Ground truth: 13
Method 6: normilized_likelihood
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 1.8389981091022491
    Answer: 11
    Ground truth: 13
Method 7: normilized_entropy
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 1.9710001051425934
    Answer: 11
    Ground truth: 13
Method 8: topk_entropy
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 1.7451549470424652
    Answer: 11
    Ground truth: 13
Method 9: window_entropy
  Batch 1:
    Text: To paint two faces of a die blue, so that the product of the numbers on the pain...
    Score: 3.465220093727112
    Answer: 11
    Ground truth: 13

==================================================
Method name: group_entropy, running accuracy: 66.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 66.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 66.66666666666666
Method name: self_consistency, running accuracy: 66.66666666666666
Method name: p_true, running accuracy: 66.66666666666666
Method name: normilized_likelihood, running accuracy: 66.66666666666666
Method name: normilized_entropy, running accuracy: 66.66666666666666
Method name: topk_entropy, running accuracy: 66.66666666666666
Method name: window_entropy, running accuracy: 66.66666666666666
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   2%|▏         | 2/100 [13:54<7:58:03, 292.69s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=66.67%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=66.67%, topk_entropy_acc=66.67%, window_entropy_acc=66.67%]         Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 3/100 [13:54<7:42:11, 285.89s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=66.67%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=66.67%, topk_entropy_acc=66.67%, window_entropy_acc=66.67%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 3.6038416624069214
    Answer: 5
    Ground truth: 5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 11.996881744129805
    Answer: 5
    Ground truth: 5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 11.99927943944931
    Answer: 5
    Ground truth: 5
Method 4: self_consistency
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 0.8571428571428571
    Answer: 5
    Ground truth: 5
Method 5: p_true
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 4.8857421875
    Answer: 5
    Ground truth: 5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 5.108456075191498
    Answer: 5
    Ground truth: 5
Method 7: normilized_entropy
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 4.754728615283966
    Answer: 5
    Ground truth: 5
Method 8: topk_entropy
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 4.219277262687683
    Answer: 5
    Ground truth: 5
Method 9: window_entropy
  Batch 1:
    Text: To find the ones digit of $35^{12},$ we can first observe the pattern in the one...
    Score: 8.119729936122894
    Answer: 5
    Ground truth: 5

==================================================
Method name: group_entropy, running accuracy: 75.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 75.0
Method name: cer_prob_product_log_last, running accuracy: 75.0
Method name: self_consistency, running accuracy: 75.0
Method name: p_true, running accuracy: 75.0
Method name: normilized_likelihood, running accuracy: 75.0
Method name: normilized_entropy, running accuracy: 75.0
Method name: topk_entropy, running accuracy: 75.0
Method name: window_entropy, running accuracy: 75.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   3%|▎         | 3/100 [17:34<7:42:11, 285.89s/it, group_entropy_acc=75.00%, cer_entropy_weighted_mean_all_acc=75.00%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=75.00%, p_true_acc=75.00%, normilized_likelihood_acc=75.00%, normilized_entropy_acc=75.00%, topk_entropy_acc=75.00%, window_entropy_acc=75.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 4/100 [17:34<6:55:30, 259.69s/it, group_entropy_acc=75.00%, cer_entropy_weighted_mean_all_acc=75.00%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=75.00%, p_true_acc=75.00%, normilized_likelihood_acc=75.00%, normilized_entropy_acc=75.00%, topk_entropy_acc=75.00%, window_entropy_acc=75.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve the problem, let's first understand what we're given. We have a right t...
    Score: 1.752453088760376
    Answer: 15
    Ground truth: 15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the problem, let's first understand what we're given. We have a right t...
    Score: 3.997880000008357
    Answer: 15
    Ground truth: 15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the problem, let's first understand what we're given. We have a right t...
    Score: 3.9974741339683533
    Answer: 15
    Ground truth: 15
Method 4: self_consistency
  Batch 1:
    Text: To solve the problem, let's first understand what we're given. We have a right t...
    Score: 0.26666666666666666
    Answer: 15
    Ground truth: 15
Method 5: p_true
  Batch 1:
    Text: To solve the problem, let's first understand what we're given. We have a right t...
    Score: 2.287109375
    Answer: 15
    Ground truth: 15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find $JL$, we first need to use the given information that $\tan K = \frac{15...
    Score: 1.7419697940349579
    Answer: \frac{120}{17}
    Ground truth: 15
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the problem, let's first understand what we're given. We have a right t...
    Score: 1.9829801321029663
    Answer: 15
    Ground truth: 15
Method 8: topk_entropy
  Batch 1:
    Text: To find $JL$, we first need to use the given information that $\tan K = \frac{15...
    Score: 1.7274071276187897
    Answer: \frac{120}{17}
    Ground truth: 15
Method 9: window_entropy
  Batch 1:
    Text: To solve the problem, let's first understand what we're given. We have a right t...
    Score: 4.958083689212799
    Answer: 15
    Ground truth: 15

==================================================
Method name: group_entropy, running accuracy: 80.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 80.0
Method name: cer_prob_product_log_last, running accuracy: 80.0
Method name: self_consistency, running accuracy: 80.0
Method name: p_true, running accuracy: 80.0
Method name: normilized_likelihood, running accuracy: 60.0
Method name: normilized_entropy, running accuracy: 80.0
Method name: topk_entropy, running accuracy: 60.0
Method name: window_entropy, running accuracy: 80.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   4%|▍         | 4/100 [25:01<6:55:30, 259.69s/it, group_entropy_acc=80.00%, cer_entropy_weighted_mean_all_acc=80.00%, cer_prob_product_log_last_acc=80.00%, self_consistency_acc=80.00%, p_true_acc=80.00%, normilized_likelihood_acc=60.00%, normilized_entropy_acc=80.00%, topk_entropy_acc=60.00%, window_entropy_acc=80.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 5/100 [25:01<8:38:28, 327.46s/it, group_entropy_acc=80.00%, cer_entropy_weighted_mean_all_acc=80.00%, cer_prob_product_log_last_acc=80.00%, self_consistency_acc=80.00%, p_true_acc=80.00%, normilized_likelihood_acc=60.00%, normilized_entropy_acc=80.00%, topk_entropy_acc=60.00%, window_entropy_acc=80.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ as a product of two nonconstant polynom...
    Score: 0.4162435531616211
    Answer: 97
    Ground truth: 8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ into two nonconstant polynomials with i...
    Score: 0.9994402872800502
    Answer: 31
    Ground truth: 8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ into two nonconstant polynomials with i...
    Score: 0.9992939233779907
    Answer: 31
    Ground truth: 8
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  To find the smallest positive integer $n$ such that the polynomial $x^4...
    Score: 0.08333333333333333
    Answer: 18
    Ground truth: 8
Method 5: p_true
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ into two nonconstant polynomials with i...
    Score: 0.9140625
    Answer: -63
    Ground truth: 8
Method 6: normilized_likelihood
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ as a product of two nonconstant polynom...
    Score: 0.9653911590576172
    Answer: 97
    Ground truth: 8
Method 7: normilized_entropy
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ as a product of two nonconstant polynom...
    Score: 0.9156625270843506
    Answer: 97
    Ground truth: 8
Method 8: topk_entropy
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ into two non-constant polynomials with ...
    Score: 0.7779603004455566
    Answer: 24
    Ground truth: 8
Method 9: window_entropy
  Batch 1:
    Text: To factor the polynomial $x^4 - nx + 63$ as a product of two nonconstant polynom...
    Score: 0.6456798315048218
    Answer: 97
    Ground truth: 8

==================================================
Method name: group_entropy, running accuracy: 66.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 66.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 66.66666666666666
Method name: self_consistency, running accuracy: 66.66666666666666
Method name: p_true, running accuracy: 66.66666666666666
Method name: normilized_likelihood, running accuracy: 50.0
Method name: normilized_entropy, running accuracy: 66.66666666666666
Method name: topk_entropy, running accuracy: 50.0
Method name: window_entropy, running accuracy: 66.66666666666666
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   5%|▌         | 5/100 [38:56<8:38:28, 327.46s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=66.67%, normilized_likelihood_acc=50.00%, normilized_entropy_acc=66.67%, topk_entropy_acc=50.00%, window_entropy_acc=66.67%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 6/100 [38:56<13:03:03, 499.82s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=66.67%, normilized_likelihood_acc=50.00%, normilized_entropy_acc=66.67%, topk_entropy_acc=50.00%, window_entropy_acc=66.67%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 1.696331262588501
    Answer: 63
    Ground truth: 63
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 3.999905956682955
    Answer: 63
    Ground truth: 63
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 3.9999219179153442
    Answer: 63
    Ground truth: 63
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 0.36363636363636365
    Answer: 63
    Ground truth: 63
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 3.0390625
    Answer: 63
    Ground truth: 63
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 3.591248959302902
    Answer: 63
    Ground truth: 63
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 3.449398994445801
    Answer: 63
    Ground truth: 63
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 3.0983847081661224
    Answer: 63
    Ground truth: 63
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to find the relationships between ligs, lags, and...
    Score: 5.741638660430908
    Answer: 63
    Ground truth: 63

==================================================
Method name: group_entropy, running accuracy: 71.42857142857143
Method name: cer_entropy_weighted_mean_all, running accuracy: 71.42857142857143
Method name: cer_prob_product_log_last, running accuracy: 71.42857142857143
Method name: self_consistency, running accuracy: 71.42857142857143
Method name: p_true, running accuracy: 71.42857142857143
Method name: normilized_likelihood, running accuracy: 57.14285714285714
Method name: normilized_entropy, running accuracy: 71.42857142857143
Method name: topk_entropy, running accuracy: 57.14285714285714
Method name: window_entropy, running accuracy: 71.42857142857143
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   6%|▌         | 6/100 [44:42<13:03:03, 499.82s/it, group_entropy_acc=71.43%, cer_entropy_weighted_mean_all_acc=71.43%, cer_prob_product_log_last_acc=71.43%, self_consistency_acc=71.43%, p_true_acc=71.43%, normilized_likelihood_acc=57.14%, normilized_entropy_acc=71.43%, topk_entropy_acc=57.14%, window_entropy_acc=71.43%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 7/100 [44:42<11:37:12, 449.81s/it, group_entropy_acc=71.43%, cer_entropy_weighted_mean_all_acc=71.43%, cer_prob_product_log_last_acc=71.43%, self_consistency_acc=71.43%, p_true_acc=71.43%, normilized_likelihood_acc=57.14%, normilized_entropy_acc=71.43%, topk_entropy_acc=57.14%, window_entropy_acc=71.43%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find $11^{-1} \pmod{508}$ given that $33^{-1} \equiv 77 \pmod{508}$, we can u...
    Score: 0.6810039281845093
    Answer: 231
    Ground truth: 231
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Since we are given that $33^{-1} \equiv 77 \pmod{508}$ and we want to find $11^{...
    Score: 1.9992173373839928
    Answer: 431
    Ground truth: 231
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Since we are given that $33^{-1} \equiv 77 \pmod{508}$ and we want to find $11^{...
    Score: 1.9991968870162964
    Answer: 431
    Ground truth: 231
Method 4: self_consistency
  Batch 1:
    Text: Since we are given that $33^{-1} \equiv 77 \pmod{508}$ and we want to find $11^{...
    Score: 0.14285714285714285
    Answer: 431
    Ground truth: 231
Method 5: p_true
  Batch 1:
    Text: To find $11^{-1} \pmod{508}$ given that $33^{-1} \equiv 77 \pmod{508}$, we can u...
    Score: 1.38671875
    Answer: 231
    Ground truth: 231
Method 6: normilized_likelihood
  Batch 1:
    Text: To find $11^{-1} \pmod{508}$ given that $33^{-1} \equiv 77 \pmod{508}$, we can u...
    Score: 1.0853787958621979
    Answer: 231
    Ground truth: 231
Method 7: normilized_entropy
  Batch 1:
    Text: To find $11^{-1} \pmod{508}$ given that $33^{-1} \equiv 77 \pmod{508}$, we can u...
    Score: 0.9806059896945953
    Answer: 231
    Ground truth: 231
Method 8: topk_entropy
  Batch 1:
    Text: To find $11^{-1} \pmod{508}$ given that $33^{-1} \equiv 77 \pmod{508}$, we can u...
    Score: 0.8339806795120239
    Answer: 231
    Ground truth: 231
Method 9: window_entropy
  Batch 1:
    Text: To find $11^{-1} \pmod{508}$, we can use the properties of modular arithmetic an...
    Score: 1.4390490651130676
    Answer: 307
    Ground truth: 231

==================================================
Method name: group_entropy, running accuracy: 75.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 62.5
Method name: cer_prob_product_log_last, running accuracy: 62.5
Method name: self_consistency, running accuracy: 62.5
Method name: p_true, running accuracy: 75.0
Method name: normilized_likelihood, running accuracy: 62.5
Method name: normilized_entropy, running accuracy: 75.0
Method name: topk_entropy, running accuracy: 62.5
Method name: window_entropy, running accuracy: 62.5
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   7%|▋         | 7/100 [51:43<11:37:12, 449.81s/it, group_entropy_acc=75.00%, cer_entropy_weighted_mean_all_acc=62.50%, cer_prob_product_log_last_acc=62.50%, self_consistency_acc=62.50%, p_true_acc=75.00%, normilized_likelihood_acc=62.50%, normilized_entropy_acc=75.00%, topk_entropy_acc=62.50%, window_entropy_acc=62.50%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 8/100 [51:43<11:15:17, 440.40s/it, group_entropy_acc=75.00%, cer_entropy_weighted_mean_all_acc=62.50%, cer_prob_product_log_last_acc=62.50%, self_consistency_acc=62.50%, p_true_acc=75.00%, normilized_likelihood_acc=62.50%, normilized_entropy_acc=75.00%, topk_entropy_acc=62.50%, window_entropy_acc=62.50%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 2.4911391735076904
    Answer: 720
    Ground truth: 720
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 6.924578712025406
    Answer: 720
    Ground truth: 720
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 6.823393404483795
    Answer: 720
    Ground truth: 720
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 0.5384615384615384
    Answer: 720
    Ground truth: 720
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 4.892578125
    Answer: 720
    Ground truth: 720
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 3.704238697886467
    Answer: 720
    Ground truth: 720
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 3.7537069618701935
    Answer: 720
    Ground truth: 720
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 3.360765516757965
    Answer: 720
    Ground truth: 720
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can use the concept of combinatory logic and permutati...
    Score: 6.5130937695503235
    Answer: 720
    Ground truth: 720

==================================================
Method name: group_entropy, running accuracy: 77.77777777777779
Method name: cer_entropy_weighted_mean_all, running accuracy: 66.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 66.66666666666666
Method name: self_consistency, running accuracy: 66.66666666666666
Method name: p_true, running accuracy: 77.77777777777779
Method name: normilized_likelihood, running accuracy: 66.66666666666666
Method name: normilized_entropy, running accuracy: 77.77777777777779
Method name: topk_entropy, running accuracy: 66.66666666666666
Method name: window_entropy, running accuracy: 66.66666666666666
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   8%|▊         | 8/100 [55:47<11:15:17, 440.40s/it, group_entropy_acc=77.78%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=77.78%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=77.78%, topk_entropy_acc=66.67%, window_entropy_acc=66.67%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 9/100 [55:47<9:35:12, 379.26s/it, group_entropy_acc=77.78%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=77.78%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=77.78%, topk_entropy_acc=66.67%, window_entropy_acc=66.67%] 
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 2.8298542499542236
    Answer: 0.4
    Ground truth: 0.4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 10.91562002138412
    Answer: 0.4
    Ground truth: 0.4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 3.2782313617199885
    Answer: 0.4
    Ground truth: 0.4
Method 4: self_consistency
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 0.8461538461538461
    Answer: 0.4
    Ground truth: 0.4
Method 5: p_true
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 1.9468994140625
    Answer: 0.4
    Ground truth: 0.4
Method 6: normilized_likelihood
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 4.531031161546707
    Answer: 0.4
    Ground truth: 0.4
Method 7: normilized_entropy
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 4.348184704780579
    Answer: 0.4
    Ground truth: 0.4
Method 8: topk_entropy
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 3.8163748681545258
    Answer: 0.4
    Ground truth: 0.4
Method 9: window_entropy
  Batch 1:
    Text: To find $2.4 \div 6$, we can use long division or simple division. 

Here's the ...
    Score: 8.117564678192139
    Answer: 0.4
    Ground truth: 0.4

==================================================
Method name: group_entropy, running accuracy: 80.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 70.0
Method name: cer_prob_product_log_last, running accuracy: 70.0
Method name: self_consistency, running accuracy: 70.0
Method name: p_true, running accuracy: 80.0
Method name: normilized_likelihood, running accuracy: 70.0
Method name: normilized_entropy, running accuracy: 80.0
Method name: topk_entropy, running accuracy: 70.0
Method name: window_entropy, running accuracy: 70.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:   9%|▉         | 9/100 [59:52<9:35:12, 379.26s/it, group_entropy_acc=80.00%, cer_entropy_weighted_mean_all_acc=70.00%, cer_prob_product_log_last_acc=70.00%, self_consistency_acc=70.00%, p_true_acc=80.00%, normilized_likelihood_acc=70.00%, normilized_entropy_acc=80.00%, topk_entropy_acc=70.00%, window_entropy_acc=70.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 10/100 [59:52<8:26:37, 337.75s/it, group_entropy_acc=80.00%, cer_entropy_weighted_mean_all_acc=70.00%, cer_prob_product_log_last_acc=70.00%, self_consistency_acc=70.00%, p_true_acc=80.00%, normilized_likelihood_acc=70.00%, normilized_entropy_acc=80.00%, topk_entropy_acc=70.00%, window_entropy_acc=70.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 3.0750088691711426
    Answer: 2
    Ground truth: 2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 5.975015759220337
    Answer: 2
    Ground truth: 2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 5.953272044658661
    Answer: 2
    Ground truth: 2
Method 4: self_consistency
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 0.42857142857142855
    Answer: 2
    Ground truth: 2
Method 5: p_true
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 2.10546875
    Answer: 2
    Ground truth: 2
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 3.5362982153892517
    Answer: 2
    Ground truth: 2
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 3.276392638683319
    Answer: 2
    Ground truth: 2
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 2.8439506888389587
    Answer: 2
    Ground truth: 2
Method 9: window_entropy
  Batch 1:
    Text: To find the number of integers between $3\sqrt{5}$ and $5\sqrt{3}$, we first nee...
    Score: 6.242104649543762
    Answer: 2
    Ground truth: 2

==================================================
Method name: group_entropy, running accuracy: 81.81818181818183
Method name: cer_entropy_weighted_mean_all, running accuracy: 72.72727272727273
Method name: cer_prob_product_log_last, running accuracy: 72.72727272727273
Method name: self_consistency, running accuracy: 72.72727272727273
Method name: p_true, running accuracy: 81.81818181818183
Method name: normilized_likelihood, running accuracy: 72.72727272727273
Method name: normilized_entropy, running accuracy: 81.81818181818183
Method name: topk_entropy, running accuracy: 72.72727272727273
Method name: window_entropy, running accuracy: 72.72727272727273
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  10%|█         | 10/100 [1:03:09<8:26:37, 337.75s/it, group_entropy_acc=81.82%, cer_entropy_weighted_mean_all_acc=72.73%, cer_prob_product_log_last_acc=72.73%, self_consistency_acc=72.73%, p_true_acc=81.82%, normilized_likelihood_acc=72.73%, normilized_entropy_acc=81.82%, topk_entropy_acc=72.73%, window_entropy_acc=72.73%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 11/100 [1:03:09<7:16:51, 294.51s/it, group_entropy_acc=81.82%, cer_entropy_weighted_mean_all_acc=72.73%, cer_prob_product_log_last_acc=72.73%, self_consistency_acc=72.73%, p_true_acc=81.82%, normilized_likelihood_acc=72.73%, normilized_entropy_acc=81.82%, topk_entropy_acc=72.73%, window_entropy_acc=72.73%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 4.622289538383484
    Answer: 3720
    Ground truth: 3720
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 11.99990027578705
    Answer: 3720
    Ground truth: 3720
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 5.999901950381229
    Answer: 3720
    Ground truth: 3720
Method 4: self_consistency
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 0.8
    Answer: 3720
    Ground truth: 3720
Method 5: p_true
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 7.650390625
    Answer: 3720
    Ground truth: 3720
Method 6: normilized_likelihood
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 7.5149155259132385
    Answer: 3720
    Ground truth: 3720
Method 7: normilized_entropy
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 7.200280457735062
    Answer: 3720
    Ground truth: 3720
Method 8: topk_entropy
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 6.601072087883949
    Answer: 3720
    Ground truth: 3720
Method 9: window_entropy
  Batch 1:
    Text: To find $3.72 \times 1000$, we need to multiply 3.72 by 1000.

Here are the step...
    Score: 10.67121160030365
    Answer: 3720
    Ground truth: 3720

==================================================
Method name: group_entropy, running accuracy: 83.33333333333334
Method name: cer_entropy_weighted_mean_all, running accuracy: 75.0
Method name: cer_prob_product_log_last, running accuracy: 75.0
Method name: self_consistency, running accuracy: 75.0
Method name: p_true, running accuracy: 83.33333333333334
Method name: normilized_likelihood, running accuracy: 75.0
Method name: normilized_entropy, running accuracy: 83.33333333333334
Method name: topk_entropy, running accuracy: 75.0
Method name: window_entropy, running accuracy: 75.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  11%|█         | 11/100 [1:05:24<7:16:51, 294.51s/it, group_entropy_acc=83.33%, cer_entropy_weighted_mean_all_acc=75.00%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=75.00%, p_true_acc=83.33%, normilized_likelihood_acc=75.00%, normilized_entropy_acc=83.33%, topk_entropy_acc=75.00%, window_entropy_acc=75.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 12/100 [1:05:24<6:00:54, 246.07s/it, group_entropy_acc=83.33%, cer_entropy_weighted_mean_all_acc=75.00%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=75.00%, p_true_acc=83.33%, normilized_likelihood_acc=75.00%, normilized_entropy_acc=83.33%, topk_entropy_acc=75.00%, window_entropy_acc=75.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 1.8616063594818115
    Answer: 15
    Ground truth: 15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 4.623810139788247
    Answer: 15
    Ground truth: 15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 3.6248800456523895
    Answer: 15
    Ground truth: 15
Method 4: self_consistency
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 0.45454545454545453
    Answer: 15
    Ground truth: 15
Method 5: p_true
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 4.2265625
    Answer: 15
    Ground truth: 15
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 3.328607738018036
    Answer: 15
    Ground truth: 15
Method 7: normilized_entropy
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 3.306521862745285
    Answer: 15
    Ground truth: 15
Method 8: topk_entropy
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 2.872050106525421
    Answer: 15
    Ground truth: 15
Method 9: window_entropy
  Batch 1:
    Text: To find the smallest positive integer $n$ such that $(\cos 84^\circ + i \sin 84^...
    Score: 3.6413848400115967
    Answer: 15
    Ground truth: 15

==================================================
Method name: group_entropy, running accuracy: 84.61538461538461
Method name: cer_entropy_weighted_mean_all, running accuracy: 76.92307692307693
Method name: cer_prob_product_log_last, running accuracy: 76.92307692307693
Method name: self_consistency, running accuracy: 76.92307692307693
Method name: p_true, running accuracy: 84.61538461538461
Method name: normilized_likelihood, running accuracy: 76.92307692307693
Method name: normilized_entropy, running accuracy: 84.61538461538461
Method name: topk_entropy, running accuracy: 76.92307692307693
Method name: window_entropy, running accuracy: 76.92307692307693
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  12%|█▏        | 12/100 [1:14:19<6:00:54, 246.07s/it, group_entropy_acc=84.62%, cer_entropy_weighted_mean_all_acc=76.92%, cer_prob_product_log_last_acc=76.92%, self_consistency_acc=76.92%, p_true_acc=84.62%, normilized_likelihood_acc=76.92%, normilized_entropy_acc=84.62%, topk_entropy_acc=76.92%, window_entropy_acc=76.92%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 13/100 [1:14:19<8:03:44, 333.61s/it, group_entropy_acc=84.62%, cer_entropy_weighted_mean_all_acc=76.92%, cer_prob_product_log_last_acc=76.92%, self_consistency_acc=76.92%, p_true_acc=84.62%, normilized_likelihood_acc=76.92%, normilized_entropy_acc=84.62%, topk_entropy_acc=76.92%, window_entropy_acc=76.92%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 4.161556601524353
    Answer: 12
    Ground truth: 12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 14.99981327148749
    Answer: 12
    Ground truth: 12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 14.999844551086426
    Answer: 12
    Ground truth: 12
Method 4: self_consistency
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 0.9375
    Answer: 12
    Ground truth: 12
Method 5: p_true
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 11.310546875
    Answer: 12
    Ground truth: 12
Method 6: normilized_likelihood
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 3.434134006500244
    Answer: 12
    Ground truth: 12
Method 7: normilized_entropy
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 3.6366629153490067
    Answer: 12
    Ground truth: 12
Method 8: topk_entropy
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 3.396465837955475
    Answer: 12
    Ground truth: 12
Method 9: window_entropy
  Batch 1:
    Text: To simplify the given expression $\frac{\sqrt{2.5^2-0.7^2}}{2.7-2.5}$, we need t...
    Score: 13.674584209918976
    Answer: 12
    Ground truth: 12

==================================================
Method name: group_entropy, running accuracy: 85.71428571428571
Method name: cer_entropy_weighted_mean_all, running accuracy: 78.57142857142857
Method name: cer_prob_product_log_last, running accuracy: 78.57142857142857
Method name: self_consistency, running accuracy: 78.57142857142857
Method name: p_true, running accuracy: 85.71428571428571
Method name: normilized_likelihood, running accuracy: 78.57142857142857
Method name: normilized_entropy, running accuracy: 85.71428571428571
Method name: topk_entropy, running accuracy: 78.57142857142857
Method name: window_entropy, running accuracy: 78.57142857142857
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  13%|█▎        | 13/100 [1:17:32<8:03:44, 333.61s/it, group_entropy_acc=85.71%, cer_entropy_weighted_mean_all_acc=78.57%, cer_prob_product_log_last_acc=78.57%, self_consistency_acc=78.57%, p_true_acc=85.71%, normilized_likelihood_acc=78.57%, normilized_entropy_acc=85.71%, topk_entropy_acc=78.57%, window_entropy_acc=78.57%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 14/100 [1:17:32<6:57:19, 291.16s/it, group_entropy_acc=85.71%, cer_entropy_weighted_mean_all_acc=78.57%, cer_prob_product_log_last_acc=78.57%, self_consistency_acc=78.57%, p_true_acc=85.71%, normilized_likelihood_acc=78.57%, normilized_entropy_acc=85.71%, topk_entropy_acc=78.57%, window_entropy_acc=78.57%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 2.3208398818969727
    Answer: 13
    Ground truth: 13
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 5.989710546934118
    Answer: 13
    Ground truth: 13
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 5.980590224266052
    Answer: 13
    Ground truth: 13
Method 4: self_consistency
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 0.42857142857142855
    Answer: 13
    Ground truth: 13
Method 5: p_true
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 4.21484375
    Answer: 13
    Ground truth: 13
Method 6: normilized_likelihood
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 2.4297484755516052
    Answer: 13
    Ground truth: 13
Method 7: normilized_entropy
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 2.2661861926317215
    Answer: 13
    Ground truth: 13
Method 8: topk_entropy
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 2.0013193637132645
    Answer: 13
    Ground truth: 13
Method 9: window_entropy
  Batch 1:
    Text: To simplify $\frac{1+\sqrt{2}}{2+\sqrt{3}}$, we can use a process called "ration...
    Score: 6.124068439006805
    Answer: 13
    Ground truth: 13

==================================================
Method name: group_entropy, running accuracy: 86.66666666666667
Method name: cer_entropy_weighted_mean_all, running accuracy: 80.0
Method name: cer_prob_product_log_last, running accuracy: 80.0
Method name: self_consistency, running accuracy: 80.0
Method name: p_true, running accuracy: 86.66666666666667
Method name: normilized_likelihood, running accuracy: 80.0
Method name: normilized_entropy, running accuracy: 86.66666666666667
Method name: topk_entropy, running accuracy: 80.0
Method name: window_entropy, running accuracy: 80.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  14%|█▍        | 14/100 [1:26:18<6:57:19, 291.16s/it, group_entropy_acc=86.67%, cer_entropy_weighted_mean_all_acc=80.00%, cer_prob_product_log_last_acc=80.00%, self_consistency_acc=80.00%, p_true_acc=86.67%, normilized_likelihood_acc=80.00%, normilized_entropy_acc=86.67%, topk_entropy_acc=80.00%, window_entropy_acc=80.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 15/100 [1:26:18<8:32:46, 361.96s/it, group_entropy_acc=86.67%, cer_entropy_weighted_mean_all_acc=80.00%, cer_prob_product_log_last_acc=80.00%, self_consistency_acc=80.00%, p_true_acc=86.67%, normilized_likelihood_acc=80.00%, normilized_entropy_acc=86.67%, topk_entropy_acc=80.00%, window_entropy_acc=80.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we w...
    Score: 0.6974380016326904
    Answer: -27
    Ground truth: -27
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we w...
    Score: 1.9980364439931173
    Answer: -27
    Ground truth: -27
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we n...
    Score: 1.9887719750404358
    Answer: 0
    Ground truth: -27
Method 4: self_consistency
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we w...
    Score: 0.16666666666666666
    Answer: -27
    Ground truth: -27
Method 5: p_true
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we c...
    Score: 0.77734375
    Answer: -3
    Ground truth: -27
Method 6: normilized_likelihood
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we w...
    Score: 1.599380761384964
    Answer: -27
    Ground truth: -27
Method 7: normilized_entropy
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we w...
    Score: 1.5097291767597198
    Answer: -27
    Ground truth: -27
Method 8: topk_entropy
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we w...
    Score: 1.315291702747345
    Answer: -27
    Ground truth: -27
Method 9: window_entropy
  Batch 1:
    Text: To rewrite the expression $g^4 + 12g^2 + 9$ in the form $c(g^2 + p)^2 + q$, we w...
    Score: 1.9539284706115723
    Answer: -27
    Ground truth: -27

==================================================
Method name: group_entropy, running accuracy: 87.5
Method name: cer_entropy_weighted_mean_all, running accuracy: 81.25
Method name: cer_prob_product_log_last, running accuracy: 75.0
Method name: self_consistency, running accuracy: 81.25
Method name: p_true, running accuracy: 81.25
Method name: normilized_likelihood, running accuracy: 81.25
Method name: normilized_entropy, running accuracy: 87.5
Method name: topk_entropy, running accuracy: 81.25
Method name: window_entropy, running accuracy: 81.25
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  15%|█▌        | 15/100 [1:33:25<8:32:46, 361.96s/it, group_entropy_acc=87.50%, cer_entropy_weighted_mean_all_acc=81.25%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=81.25%, p_true_acc=81.25%, normilized_likelihood_acc=81.25%, normilized_entropy_acc=87.50%, topk_entropy_acc=81.25%, window_entropy_acc=81.25%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 16/100 [1:33:25<8:53:59, 381.42s/it, group_entropy_acc=87.50%, cer_entropy_weighted_mean_all_acc=81.25%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=81.25%, p_true_acc=81.25%, normilized_likelihood_acc=81.25%, normilized_entropy_acc=87.50%, topk_entropy_acc=81.25%, window_entropy_acc=81.25%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we need to plug in different integer values into the poly...
    Score: 0.39194726943969727
    Answer: 4
    Ground truth: 41
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 2.914789686326891
    Answer: 40
    Ground truth: 41
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 2.9731799960136414
    Answer: 40
    Ground truth: 41
Method 4: self_consistency
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 0.2727272727272727
    Answer: 40
    Ground truth: 41
Method 5: p_true
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 1.6484375
    Answer: 40
    Ground truth: 41
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 1.0545275807380676
    Answer: 40
    Ground truth: 41
Method 7: normilized_entropy
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 0.9013683795928955
    Answer: 40
    Ground truth: 41
Method 8: topk_entropy
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 0.7530428171157837
    Answer: 40
    Ground truth: 41
Method 9: window_entropy
  Batch 1:
    Text: To find the smallest positive integer $n$ for which $p(n)$ and $p(n+1)$ share a ...
    Score: 0.9040017127990723
    Answer: 26
    Ground truth: 41

==================================================
Method name: group_entropy, running accuracy: 82.35294117647058
Method name: cer_entropy_weighted_mean_all, running accuracy: 76.47058823529412
Method name: cer_prob_product_log_last, running accuracy: 70.58823529411765
Method name: self_consistency, running accuracy: 76.47058823529412
Method name: p_true, running accuracy: 76.47058823529412
Method name: normilized_likelihood, running accuracy: 76.47058823529412
Method name: normilized_entropy, running accuracy: 82.35294117647058
Method name: topk_entropy, running accuracy: 76.47058823529412
Method name: window_entropy, running accuracy: 76.47058823529412
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  16%|█▌        | 16/100 [1:43:08<8:53:59, 381.42s/it, group_entropy_acc=82.35%, cer_entropy_weighted_mean_all_acc=76.47%, cer_prob_product_log_last_acc=70.59%, self_consistency_acc=76.47%, p_true_acc=76.47%, normilized_likelihood_acc=76.47%, normilized_entropy_acc=82.35%, topk_entropy_acc=76.47%, window_entropy_acc=76.47%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 17/100 [1:43:08<10:11:46, 442.24s/it, group_entropy_acc=82.35%, cer_entropy_weighted_mean_all_acc=76.47%, cer_prob_product_log_last_acc=70.59%, self_consistency_acc=76.47%, p_true_acc=76.47%, normilized_likelihood_acc=76.47%, normilized_entropy_acc=82.35%, topk_entropy_acc=76.47%, window_entropy_acc=76.47%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 2.8653221130371094
    Answer: 9
    Ground truth: 9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 11.992534396517996
    Answer: 9
    Ground truth: 9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 11.990461647510529
    Answer: 9
    Ground truth: 9
Method 4: self_consistency
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 0.9230769230769231
    Answer: 9
    Ground truth: 9
Method 5: p_true
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 10.49609375
    Answer: 9
    Ground truth: 9
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 2.2023844569921494
    Answer: 9
    Ground truth: 9
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 1.9569158107042313
    Answer: 9
    Ground truth: 9
Method 8: topk_entropy
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 1.7887725532054901
    Answer: 9
    Ground truth: 9
Method 9: window_entropy
  Batch 1:
    Text: To solve the equation $5 \cdot 25_b = 137_b$ for a positive base $b$, we need to...
    Score: 5.964395999908447
    Answer: 9
    Ground truth: 9

==================================================
Method name: group_entropy, running accuracy: 83.33333333333334
Method name: cer_entropy_weighted_mean_all, running accuracy: 77.77777777777779
Method name: cer_prob_product_log_last, running accuracy: 72.22222222222221
Method name: self_consistency, running accuracy: 77.77777777777779
Method name: p_true, running accuracy: 77.77777777777779
Method name: normilized_likelihood, running accuracy: 77.77777777777779
Method name: normilized_entropy, running accuracy: 83.33333333333334
Method name: topk_entropy, running accuracy: 77.77777777777779
Method name: window_entropy, running accuracy: 77.77777777777779
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  17%|█▋        | 17/100 [1:49:39<10:11:46, 442.24s/it, group_entropy_acc=83.33%, cer_entropy_weighted_mean_all_acc=77.78%, cer_prob_product_log_last_acc=72.22%, self_consistency_acc=77.78%, p_true_acc=77.78%, normilized_likelihood_acc=77.78%, normilized_entropy_acc=83.33%, topk_entropy_acc=77.78%, window_entropy_acc=77.78%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 18/100 [1:49:39<9:43:19, 426.82s/it, group_entropy_acc=83.33%, cer_entropy_weighted_mean_all_acc=77.78%, cer_prob_product_log_last_acc=72.22%, self_consistency_acc=77.78%, p_true_acc=77.78%, normilized_likelihood_acc=77.78%, normilized_entropy_acc=83.33%, topk_entropy_acc=77.78%, window_entropy_acc=77.78%] 
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 6.173943996429443
    Answer: 19
    Ground truth: 19
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 11.997501879455928
    Answer: 19
    Ground truth: 19
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 11.997108519077301
    Answer: 19
    Ground truth: 19
Method 4: self_consistency
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 0.75
    Answer: 19
    Ground truth: 19
Method 5: p_true
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 5.0048828125
    Answer: 19
    Ground truth: 19
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 10.814255982637405
    Answer: 19
    Ground truth: 19
Method 7: normilized_entropy
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 10.515885919332504
    Answer: 19
    Ground truth: 19
Method 8: topk_entropy
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 8.740980237722397
    Answer: 19
    Ground truth: 19
Method 9: window_entropy
  Batch 1:
    Text: To determine the number of years it will take for the new green technology to pa...
    Score: 13.120582401752472
    Answer: 19
    Ground truth: 19

==================================================
Method name: group_entropy, running accuracy: 84.21052631578947
Method name: cer_entropy_weighted_mean_all, running accuracy: 78.94736842105263
Method name: cer_prob_product_log_last, running accuracy: 73.68421052631578
Method name: self_consistency, running accuracy: 78.94736842105263
Method name: p_true, running accuracy: 78.94736842105263
Method name: normilized_likelihood, running accuracy: 78.94736842105263
Method name: normilized_entropy, running accuracy: 84.21052631578947
Method name: topk_entropy, running accuracy: 78.94736842105263
Method name: window_entropy, running accuracy: 78.94736842105263
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  18%|█▊        | 18/100 [1:54:03<9:43:19, 426.82s/it, group_entropy_acc=84.21%, cer_entropy_weighted_mean_all_acc=78.95%, cer_prob_product_log_last_acc=73.68%, self_consistency_acc=78.95%, p_true_acc=78.95%, normilized_likelihood_acc=78.95%, normilized_entropy_acc=84.21%, topk_entropy_acc=78.95%, window_entropy_acc=78.95%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 19/100 [1:54:03<8:30:06, 377.86s/it, group_entropy_acc=84.21%, cer_entropy_weighted_mean_all_acc=78.95%, cer_prob_product_log_last_acc=73.68%, self_consistency_acc=78.95%, p_true_acc=78.95%, normilized_likelihood_acc=78.95%, normilized_entropy_acc=84.21%, topk_entropy_acc=78.95%, window_entropy_acc=78.95%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1: The given equation is $\cos 4x = -\frac{1}{2}.$ We need to find a positi...
    Score: 0.6219291687011719
    Answer: 6
    Ground truth: 3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  To find the solutions of the given equation $\cos 4x = -\frac{1}{2}$ in...
    Score: 1.9858284665647532
    Answer: 3
    Ground truth: 3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  To find the solutions of the given equation $\cos 4x = -\frac{1}{2}$ in...
    Score: 1.9704435467720032
    Answer: 3
    Ground truth: 3
Method 4: self_consistency
  Batch 1:
    Text: Step 1: The given equation is $\cos 4x = -\frac{1}{2}.$ We need to find a positi...
    Score: 0.18181818181818182
    Answer: 6
    Ground truth: 3
Method 5: p_true
  Batch 1:
    Text: Step 1: The given equation is $\cos 4x = -\frac{1}{2}.$ We need to find a positi...
    Score: 1.6484375
    Answer: 6
    Ground truth: 3
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: The given equation is $\cos 4x = -\frac{1}{2}.$ We need to find a positi...
    Score: 1.33413627743721
    Answer: 6
    Ground truth: 3
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: The given equation is $\cos 4x = -\frac{1}{2}.$ We need to find a positi...
    Score: 1.33844393491745
    Answer: 6
    Ground truth: 3
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: The given equation is $\cos 4x = -\frac{1}{2}.$ We need to find a positi...
    Score: 1.1795251071453094
    Answer: 6
    Ground truth: 3
Method 9: window_entropy
  Batch 1:
    Text: Step 1: The given equation is $\cos 4x = -\frac{1}{2}.$ We need to find a positi...
    Score: 1.317078948020935
    Answer: 6
    Ground truth: 3

==================================================
Method name: group_entropy, running accuracy: 80.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 80.0
Method name: cer_prob_product_log_last, running accuracy: 75.0
Method name: self_consistency, running accuracy: 75.0
Method name: p_true, running accuracy: 75.0
Method name: normilized_likelihood, running accuracy: 75.0
Method name: normilized_entropy, running accuracy: 80.0
Method name: topk_entropy, running accuracy: 75.0
Method name: window_entropy, running accuracy: 75.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  19%|█▉        | 19/100 [2:06:01<8:30:06, 377.86s/it, group_entropy_acc=80.00%, cer_entropy_weighted_mean_all_acc=80.00%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=75.00%, p_true_acc=75.00%, normilized_likelihood_acc=75.00%, normilized_entropy_acc=80.00%, topk_entropy_acc=75.00%, window_entropy_acc=75.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 20/100 [2:06:01<10:39:52, 479.90s/it, group_entropy_acc=80.00%, cer_entropy_weighted_mean_all_acc=80.00%, cer_prob_product_log_last_acc=75.00%, self_consistency_acc=75.00%, p_true_acc=75.00%, normilized_likelihood_acc=75.00%, normilized_entropy_acc=80.00%, topk_entropy_acc=75.00%, window_entropy_acc=75.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 0.9420506954193115
    Answer: 400
    Ground truth: 500
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 2.9672691738841666
    Answer: 400
    Ground truth: 500
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 2.8791404366493225
    Answer: 400
    Ground truth: 500
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 0.21428571428571427
    Answer: 400
    Ground truth: 500
Method 5: p_true
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 1.6796875
    Answer: 400
    Ground truth: 500
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 2.0805879533290863
    Answer: 400
    Ground truth: 500
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 2.012217402458191
    Answer: 400
    Ground truth: 500
Method 8: topk_entropy
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 1.6741843819618225
    Answer: 400
    Ground truth: 500
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  To find the maximum value of $a - 5b$, we are given the equation $\sqrt...
    Score: 2.240249752998352
    Answer: 400
    Ground truth: 500

==================================================
Method name: group_entropy, running accuracy: 76.19047619047619
Method name: cer_entropy_weighted_mean_all, running accuracy: 76.19047619047619
Method name: cer_prob_product_log_last, running accuracy: 71.42857142857143
Method name: self_consistency, running accuracy: 71.42857142857143
Method name: p_true, running accuracy: 71.42857142857143
Method name: normilized_likelihood, running accuracy: 71.42857142857143
Method name: normilized_entropy, running accuracy: 76.19047619047619
Method name: topk_entropy, running accuracy: 71.42857142857143
Method name: window_entropy, running accuracy: 71.42857142857143
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  20%|██        | 20/100 [2:15:54<10:39:52, 479.90s/it, group_entropy_acc=76.19%, cer_entropy_weighted_mean_all_acc=76.19%, cer_prob_product_log_last_acc=71.43%, self_consistency_acc=71.43%, p_true_acc=71.43%, normilized_likelihood_acc=71.43%, normilized_entropy_acc=76.19%, topk_entropy_acc=71.43%, window_entropy_acc=71.43%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 21/100 [2:15:54<11:16:32, 513.83s/it, group_entropy_acc=76.19%, cer_entropy_weighted_mean_all_acc=76.19%, cer_prob_product_log_last_acc=71.43%, self_consistency_acc=71.43%, p_true_acc=71.43%, normilized_likelihood_acc=71.43%, normilized_entropy_acc=76.19%, topk_entropy_acc=71.43%, window_entropy_acc=71.43%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 4.71443247795105
    Answer: -12
    Ground truth: -12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 13.997147858782148
    Answer: -12
    Ground truth: -12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 6.9964041840769795
    Answer: -12
    Ground truth: -12
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 0.875
    Answer: -12
    Ground truth: -12
Method 5: p_true
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 5.7021484375
    Answer: -12
    Ground truth: -12
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 5.022336393594742
    Answer: -12
    Ground truth: -12
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 4.343951120972633
    Answer: -12
    Ground truth: -12
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 3.946035385131836
    Answer: -12
    Ground truth: -12
Method 9: window_entropy
  Batch 1:
    Text: Step 1: Recall that the dot product of two vectors is given by the formula $\mat...
    Score: 12.31650698184967
    Answer: -12
    Ground truth: -12

==================================================
Method name: group_entropy, running accuracy: 77.27272727272727
Method name: cer_entropy_weighted_mean_all, running accuracy: 77.27272727272727
Method name: cer_prob_product_log_last, running accuracy: 72.72727272727273
Method name: self_consistency, running accuracy: 72.72727272727273
Method name: p_true, running accuracy: 72.72727272727273
Method name: normilized_likelihood, running accuracy: 72.72727272727273
Method name: normilized_entropy, running accuracy: 77.27272727272727
Method name: topk_entropy, running accuracy: 72.72727272727273
Method name: window_entropy, running accuracy: 72.72727272727273
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  21%|██        | 21/100 [2:20:20<11:16:32, 513.83s/it, group_entropy_acc=77.27%, cer_entropy_weighted_mean_all_acc=77.27%, cer_prob_product_log_last_acc=72.73%, self_consistency_acc=72.73%, p_true_acc=72.73%, normilized_likelihood_acc=72.73%, normilized_entropy_acc=77.27%, topk_entropy_acc=72.73%, window_entropy_acc=72.73%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 22/100 [2:20:20<9:31:23, 439.53s/it, group_entropy_acc=77.27%, cer_entropy_weighted_mean_all_acc=77.27%, cer_prob_product_log_last_acc=72.73%, self_consistency_acc=72.73%, p_true_acc=72.73%, normilized_likelihood_acc=72.73%, normilized_entropy_acc=77.27%, topk_entropy_acc=72.73%, window_entropy_acc=72.73%] 
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1:  We are given the polynomial $P(x) = x^7 - 7$ and are asked to find the ...
    Score: 0.3060731887817383
    Answer: 7
    Ground truth: 117649
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  The given problem involves finding the product of all distinct complex ...
    Score: 1.7639098591573616
    Answer: 16807
    Ground truth: 117649
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  The problem asks us to find the value of $K^2$, where $K$ is the produc...
    Score: 0.9998981952667236
    Answer: 0
    Ground truth: 117649
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  The given problem involves finding the product of all distinct complex ...
    Score: 0.2
    Answer: 16807
    Ground truth: 117649
Method 5: p_true
  Batch 1:
    Text: Step 1:  The given problem involves finding the product of all distinct complex ...
    Score: 1.02734375
    Answer: 16807
    Ground truth: 117649
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1:  The given problem involves finding the product of all distinct complex ...
    Score: 1.2170392274856567
    Answer: 16807
    Ground truth: 117649
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1:  The given problem involves finding the product of all distinct complex ...
    Score: 1.0825873613357544
    Answer: 16807
    Ground truth: 117649
Method 8: topk_entropy
  Batch 1:
    Text: Step 1:  The given problem involves finding the product of all distinct complex ...
    Score: 1.0186607241630554
    Answer: 16807
    Ground truth: 117649
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  We are given the polynomial $P(x) = x^7 - 7$ and are asked to find the ...
    Score: 1.1565479040145874
    Answer: 7
    Ground truth: 117649

==================================================
Method name: group_entropy, running accuracy: 73.91304347826086
Method name: cer_entropy_weighted_mean_all, running accuracy: 73.91304347826086
Method name: cer_prob_product_log_last, running accuracy: 69.56521739130434
Method name: self_consistency, running accuracy: 69.56521739130434
Method name: p_true, running accuracy: 69.56521739130434
Method name: normilized_likelihood, running accuracy: 69.56521739130434
Method name: normilized_entropy, running accuracy: 73.91304347826086
Method name: topk_entropy, running accuracy: 69.56521739130434
Method name: window_entropy, running accuracy: 69.56521739130434
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  22%|██▏       | 22/100 [2:37:58<9:31:23, 439.53s/it, group_entropy_acc=73.91%, cer_entropy_weighted_mean_all_acc=73.91%, cer_prob_product_log_last_acc=69.57%, self_consistency_acc=69.57%, p_true_acc=69.57%, normilized_likelihood_acc=69.57%, normilized_entropy_acc=73.91%, topk_entropy_acc=69.57%, window_entropy_acc=69.57%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 23/100 [2:37:58<13:21:59, 624.93s/it, group_entropy_acc=73.91%, cer_entropy_weighted_mean_all_acc=73.91%, cer_prob_product_log_last_acc=69.57%, self_consistency_acc=69.57%, p_true_acc=69.57%, normilized_likelihood_acc=69.57%, normilized_entropy_acc=73.91%, topk_entropy_acc=69.57%, window_entropy_acc=69.57%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find $17^{-1}\pmod{83}$, we need to find an integer $x$ such that $17x \equiv...
    Score: 0.5729360580444336
    Answer: 50
    Ground truth: 44
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To compute $17^{-1}\pmod{83}$, we are looking for an integer $x$ such that $17x ...
    Score: 1.9987084288614045
    Answer: 42
    Ground truth: 44
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To compute $17^{-1}\pmod{83}$, we are looking for an integer $x$ such that $17x ...
    Score: 1.9985478520393372
    Answer: 42
    Ground truth: 44
Method 4: self_consistency
  Batch 1:
    Text: To compute $17^{-1}\pmod{83}$, we are looking for an integer $x$ such that $17x ...
    Score: 0.15384615384615385
    Answer: 42
    Ground truth: 44
Method 5: p_true
  Batch 1:
    Text: To compute $17^{-1}\pmod{83}$, we are looking for an integer $x$ such that $17x ...
    Score: 1.05078125
    Answer: 42
    Ground truth: 44
Method 6: normilized_likelihood
  Batch 1:
    Text: To compute $17^{-1}\pmod{83}$, we need to find a number $x$ such that $17x \equi...
    Score: 1.1186175048351288
    Answer: 40
    Ground truth: 44
Method 7: normilized_entropy
  Batch 1:
    Text: To find $17^{-1}\pmod{83}$, we need to find an integer $x$ such that $17x \equiv...
    Score: 1.0939170718193054
    Answer: 50
    Ground truth: 44
Method 8: topk_entropy
  Batch 1:
    Text: To find $17^{-1}\pmod{83}$, we need to find an integer $x$ such that $17x \equiv...
    Score: 0.9473444819450378
    Answer: 50
    Ground truth: 44
Method 9: window_entropy
  Batch 1:
    Text: To compute $17^{-1}\pmod{83}$, we need to find a number $x$ such that $17x \equi...
    Score: 1.498427391052246
    Answer: 40
    Ground truth: 44

==================================================
Method name: group_entropy, running accuracy: 70.83333333333334
Method name: cer_entropy_weighted_mean_all, running accuracy: 70.83333333333334
Method name: cer_prob_product_log_last, running accuracy: 66.66666666666666
Method name: self_consistency, running accuracy: 66.66666666666666
Method name: p_true, running accuracy: 66.66666666666666
Method name: normilized_likelihood, running accuracy: 66.66666666666666
Method name: normilized_entropy, running accuracy: 70.83333333333334
Method name: topk_entropy, running accuracy: 66.66666666666666
Method name: window_entropy, running accuracy: 66.66666666666666
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  23%|██▎       | 23/100 [2:43:56<13:21:59, 624.93s/it, group_entropy_acc=70.83%, cer_entropy_weighted_mean_all_acc=70.83%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=66.67%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=70.83%, topk_entropy_acc=66.67%, window_entropy_acc=66.67%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 24/100 [2:43:56<11:30:15, 544.94s/it, group_entropy_acc=70.83%, cer_entropy_weighted_mean_all_acc=70.83%, cer_prob_product_log_last_acc=66.67%, self_consistency_acc=66.67%, p_true_acc=66.67%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=70.83%, topk_entropy_acc=66.67%, window_entropy_acc=66.67%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 0.4049715995788574
    Answer: 0
    Ground truth: 6
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 4.808900124329648
    Answer: 0
    Ground truth: 6
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 4.4593585729599
    Answer: 0
    Ground truth: 6
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 0.45454545454545453
    Answer: 0
    Ground truth: 6
Method 5: p_true
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 1.626953125
    Answer: 0
    Ground truth: 6
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 1.1327981352806091
    Answer: 0
    Ground truth: 6
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 1.0084248781204224
    Answer: 0
    Ground truth: 6
Method 8: topk_entropy
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 0.8889872431755066
    Answer: 0
    Ground truth: 6
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  We are given a system of equations involving real solutions, and we nee...
    Score: 1.7998936176300049
    Answer: 0
    Ground truth: 6

==================================================
Method name: group_entropy, running accuracy: 68.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 68.0
Method name: cer_prob_product_log_last, running accuracy: 64.0
Method name: self_consistency, running accuracy: 64.0
Method name: p_true, running accuracy: 64.0
Method name: normilized_likelihood, running accuracy: 64.0
Method name: normilized_entropy, running accuracy: 68.0
Method name: topk_entropy, running accuracy: 64.0
Method name: window_entropy, running accuracy: 64.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  24%|██▍       | 24/100 [3:00:10<11:30:15, 544.94s/it, group_entropy_acc=68.00%, cer_entropy_weighted_mean_all_acc=68.00%, cer_prob_product_log_last_acc=64.00%, self_consistency_acc=64.00%, p_true_acc=64.00%, normilized_likelihood_acc=64.00%, normilized_entropy_acc=68.00%, topk_entropy_acc=64.00%, window_entropy_acc=64.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 25/100 [3:00:10<14:02:11, 673.75s/it, group_entropy_acc=68.00%, cer_entropy_weighted_mean_all_acc=68.00%, cer_prob_product_log_last_acc=64.00%, self_consistency_acc=64.00%, p_true_acc=64.00%, normilized_likelihood_acc=64.00%, normilized_entropy_acc=68.00%, topk_entropy_acc=64.00%, window_entropy_acc=64.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, let's denote the number of pennies Betty originally had a...
    Score: 0.594501256942749
    Answer: 41
    Ground truth: 45
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  First, let's consider the constraints given to us. Betty has between 40...
    Score: 2.8137691666318614
    Answer: 43
    Ground truth: 45
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  First, let's consider the constraints given to us. Betty has between 40...
    Score: 2.666565716266632
    Answer: 43
    Ground truth: 45
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  First, let's consider the constraints given to us. Betty has between 40...
    Score: 0.25
    Answer: 43
    Ground truth: 45
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's denote the number of pennies Betty originally had a...
    Score: 1.45703125
    Answer: 41
    Ground truth: 45
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of pennies Betty originally had, let's consider the factors o...
    Score: 2.3071165084838867
    Answer: 48
    Ground truth: 45
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of pennies Betty originally had, let's consider the factors o...
    Score: 2.293558120727539
    Answer: 48
    Ground truth: 45
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of pennies Betty originally had, let's consider the factors o...
    Score: 1.945131003856659
    Answer: 48
    Ground truth: 45
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's denote the number of pennies Betty originally had a...
    Score: 1.5167332887649536
    Answer: 41
    Ground truth: 45

==================================================
Method name: group_entropy, running accuracy: 65.38461538461539
Method name: cer_entropy_weighted_mean_all, running accuracy: 65.38461538461539
Method name: cer_prob_product_log_last, running accuracy: 61.53846153846154
Method name: self_consistency, running accuracy: 61.53846153846154
Method name: p_true, running accuracy: 61.53846153846154
Method name: normilized_likelihood, running accuracy: 61.53846153846154
Method name: normilized_entropy, running accuracy: 65.38461538461539
Method name: topk_entropy, running accuracy: 61.53846153846154
Method name: window_entropy, running accuracy: 61.53846153846154
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  25%|██▌       | 25/100 [3:08:14<14:02:11, 673.75s/it, group_entropy_acc=65.38%, cer_entropy_weighted_mean_all_acc=65.38%, cer_prob_product_log_last_acc=61.54%, self_consistency_acc=61.54%, p_true_acc=61.54%, normilized_likelihood_acc=61.54%, normilized_entropy_acc=65.38%, topk_entropy_acc=61.54%, window_entropy_acc=61.54%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 26/100 [3:08:14<12:40:37, 616.72s/it, group_entropy_acc=65.38%, cer_entropy_weighted_mean_all_acc=65.38%, cer_prob_product_log_last_acc=61.54%, self_consistency_acc=61.54%, p_true_acc=61.54%, normilized_likelihood_acc=61.54%, normilized_entropy_acc=65.38%, topk_entropy_acc=61.54%, window_entropy_acc=61.54%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 4.278199076652527
    Answer: 10
    Ground truth: 10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 9.998727493864203
    Answer: 10
    Ground truth: 10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 9.998437821865082
    Answer: 10
    Ground truth: 10
Method 4: self_consistency
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 0.6666666666666666
    Answer: 10
    Ground truth: 10
Method 5: p_true
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 7.53515625
    Answer: 10
    Ground truth: 10
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 6.95915350317955
    Answer: 10
    Ground truth: 10
Method 7: normilized_entropy
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 6.549843728542328
    Answer: 10
    Ground truth: 10
Method 8: topk_entropy
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 5.632405906915665
    Answer: 10
    Ground truth: 10
Method 9: window_entropy
  Batch 1:
    Text: To find the product of the divisors of 300 and then the sum of the distinct prim...
    Score: 7.591909766197205
    Answer: 10
    Ground truth: 10

==================================================
Method name: group_entropy, running accuracy: 66.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 66.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 62.96296296296296
Method name: self_consistency, running accuracy: 62.96296296296296
Method name: p_true, running accuracy: 62.96296296296296
Method name: normilized_likelihood, running accuracy: 62.96296296296296
Method name: normilized_entropy, running accuracy: 66.66666666666666
Method name: topk_entropy, running accuracy: 62.96296296296296
Method name: window_entropy, running accuracy: 62.96296296296296
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  26%|██▌       | 26/100 [3:15:15<12:40:37, 616.72s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=62.96%, self_consistency_acc=62.96%, p_true_acc=62.96%, normilized_likelihood_acc=62.96%, normilized_entropy_acc=66.67%, topk_entropy_acc=62.96%, window_entropy_acc=62.96%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 27/100 [3:15:15<11:19:03, 558.13s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=62.96%, self_consistency_acc=62.96%, p_true_acc=62.96%, normilized_likelihood_acc=62.96%, normilized_entropy_acc=66.67%, topk_entropy_acc=62.96%, window_entropy_acc=62.96%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we need to first recall that the sum of the divisors of a...
    Score: 0.29942965507507324
    Answer: 4044
    Ground truth: 2016
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to first recall that the sum of the divisors of a...
    Score: 0.9999465057229773
    Answer: 4044
    Ground truth: 2016
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  The problem asks us to find the sum of all the nice numbers in the give...
    Score: 0.9995080232620239
    Answer: 487
    Ground truth: 2016
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  The problem asks us to find the sum of all the nice numbers in the give...
    Score: 0.1111111111111111
    Answer: 487
    Ground truth: 2016
Method 5: p_true
  Batch 1:
    Text: To find the 'nice' numbers in the given set, we first need to understand the pro...
    Score: 0.70703125
    Answer: 2073
    Ground truth: 2016
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to first recall that the sum of the divisors of a...
    Score: 1.0367937684059143
    Answer: 4044
    Ground truth: 2016
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to first recall that the sum of the divisors of a...
    Score: 0.9884777665138245
    Answer: 4044
    Ground truth: 2016
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to first recall that the sum of the divisors of a...
    Score: 0.8218144774436951
    Answer: 4044
    Ground truth: 2016
Method 9: window_entropy
  Batch 1:
    Text: To find the nice numbers in the set $\{ 2010,2011,2012,\dots,2019 \}$, we need t...
    Score: 0.6554224491119385
    Answer: 4029
    Ground truth: 2016

==================================================
Method name: group_entropy, running accuracy: 64.28571428571429
Method name: cer_entropy_weighted_mean_all, running accuracy: 64.28571428571429
Method name: cer_prob_product_log_last, running accuracy: 60.71428571428571
Method name: self_consistency, running accuracy: 60.71428571428571
Method name: p_true, running accuracy: 60.71428571428571
Method name: normilized_likelihood, running accuracy: 60.71428571428571
Method name: normilized_entropy, running accuracy: 64.28571428571429
Method name: topk_entropy, running accuracy: 60.71428571428571
Method name: window_entropy, running accuracy: 60.71428571428571
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  27%|██▋       | 27/100 [3:26:39<11:19:03, 558.13s/it, group_entropy_acc=64.29%, cer_entropy_weighted_mean_all_acc=64.29%, cer_prob_product_log_last_acc=60.71%, self_consistency_acc=60.71%, p_true_acc=60.71%, normilized_likelihood_acc=60.71%, normilized_entropy_acc=64.29%, topk_entropy_acc=60.71%, window_entropy_acc=60.71%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 28/100 [3:26:39<11:55:09, 595.96s/it, group_entropy_acc=64.29%, cer_entropy_weighted_mean_all_acc=64.29%, cer_prob_product_log_last_acc=60.71%, self_consistency_acc=60.71%, p_true_acc=60.71%, normilized_likelihood_acc=60.71%, normilized_entropy_acc=64.29%, topk_entropy_acc=60.71%, window_entropy_acc=60.71%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to understand how base conv...
    Score: 1.1164782047271729
    Answer: 2743
    Ground truth: 2743
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to consider that a 3-digit ...
    Score: 3.9999839109718325
    Answer: 241
    Ground truth: 2743
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to consider that a 3-digit ...
    Score: 3.999989151954651
    Answer: 241
    Ground truth: 2743
Method 4: self_consistency
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to understand how base conv...
    Score: 0.25
    Answer: 2743
    Ground truth: 2743
Method 5: p_true
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to understand how base conv...
    Score: 3.7109375
    Answer: 2743
    Ground truth: 2743
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to understand how base conv...
    Score: 1.9707864820957184
    Answer: 2743
    Ground truth: 2743
Method 7: normilized_entropy
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to understand how base conv...
    Score: 1.8195699453353882
    Answer: 2743
    Ground truth: 2743
Method 8: topk_entropy
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to understand how base conv...
    Score: 1.7258096635341644
    Answer: 2743
    Ground truth: 2743
Method 9: window_entropy
  Batch 1:
    Text: To find the largest 3-digit base 14 integer, we need to understand how base conv...
    Score: 4.685016930103302
    Answer: 2743
    Ground truth: 2743

==================================================
Method name: group_entropy, running accuracy: 65.51724137931035
Method name: cer_entropy_weighted_mean_all, running accuracy: 62.06896551724138
Method name: cer_prob_product_log_last, running accuracy: 58.620689655172406
Method name: self_consistency, running accuracy: 62.06896551724138
Method name: p_true, running accuracy: 62.06896551724138
Method name: normilized_likelihood, running accuracy: 62.06896551724138
Method name: normilized_entropy, running accuracy: 65.51724137931035
Method name: topk_entropy, running accuracy: 62.06896551724138
Method name: window_entropy, running accuracy: 62.06896551724138
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  28%|██▊       | 28/100 [3:30:44<11:55:09, 595.96s/it, group_entropy_acc=65.52%, cer_entropy_weighted_mean_all_acc=62.07%, cer_prob_product_log_last_acc=58.62%, self_consistency_acc=62.07%, p_true_acc=62.07%, normilized_likelihood_acc=62.07%, normilized_entropy_acc=65.52%, topk_entropy_acc=62.07%, window_entropy_acc=62.07%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 29/100 [3:30:44<9:40:36, 490.65s/it, group_entropy_acc=65.52%, cer_entropy_weighted_mean_all_acc=62.07%, cer_prob_product_log_last_acc=58.62%, self_consistency_acc=62.07%, p_true_acc=62.07%, normilized_likelihood_acc=62.07%, normilized_entropy_acc=65.52%, topk_entropy_acc=62.07%, window_entropy_acc=62.07%] 
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 4.212723731994629
    Answer: 144
    Ground truth: 144
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 7.9996866657200245
    Answer: 144
    Ground truth: 144
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 7.999689698219299
    Answer: 144
    Ground truth: 144
Method 4: self_consistency
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 0.5333333333333333
    Answer: 144
    Ground truth: 144
Method 5: p_true
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 6.9453125
    Answer: 144
    Ground truth: 144
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 6.76331302523613
    Answer: 144
    Ground truth: 144
Method 7: normilized_entropy
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 6.8519242107868195
    Answer: 144
    Ground truth: 144
Method 8: topk_entropy
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 5.892244040966034
    Answer: 144
    Ground truth: 144
Method 9: window_entropy
  Batch 1:
    Text: To find the other number, we need to use the relationship between the least comm...
    Score: 9.230681419372559
    Answer: 144
    Ground truth: 144

==================================================
Method name: group_entropy, running accuracy: 66.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 63.33333333333333
Method name: cer_prob_product_log_last, running accuracy: 60.0
Method name: self_consistency, running accuracy: 63.33333333333333
Method name: p_true, running accuracy: 63.33333333333333
Method name: normilized_likelihood, running accuracy: 63.33333333333333
Method name: normilized_entropy, running accuracy: 66.66666666666666
Method name: topk_entropy, running accuracy: 63.33333333333333
Method name: window_entropy, running accuracy: 63.33333333333333
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  29%|██▉       | 29/100 [3:36:23<9:40:36, 490.65s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=63.33%, cer_prob_product_log_last_acc=60.00%, self_consistency_acc=63.33%, p_true_acc=63.33%, normilized_likelihood_acc=63.33%, normilized_entropy_acc=66.67%, topk_entropy_acc=63.33%, window_entropy_acc=63.33%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 30/100 [3:36:23<8:39:18, 445.11s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=63.33%, cer_prob_product_log_last_acc=60.00%, self_consistency_acc=63.33%, p_true_acc=63.33%, normilized_likelihood_acc=63.33%, normilized_entropy_acc=66.67%, topk_entropy_acc=63.33%, window_entropy_acc=63.33%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 4.178585588932037
    Answer: 15
    Ground truth: 15
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 14.99933783468117
    Answer: 15
    Ground truth: 15
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 14.999366164207458
    Answer: 15
    Ground truth: 15
Method 4: self_consistency
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 0.9375
    Answer: 15
    Ground truth: 15
Method 5: p_true
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 11.453125
    Answer: 15
    Ground truth: 15
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 4.635004296898842
    Answer: 15
    Ground truth: 15
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 3.272091716527939
    Answer: 15
    Ground truth: 15
Method 8: topk_entropy
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 2.9962093234062195
    Answer: 15
    Ground truth: 15
Method 9: window_entropy
  Batch 1:
    Text: Let's break it down into steps:

1. Let's assume Lenny has $x amount of money. 
...
    Score: 12.30514794588089
    Answer: 15
    Ground truth: 15

==================================================
Method name: group_entropy, running accuracy: 67.74193548387096
Method name: cer_entropy_weighted_mean_all, running accuracy: 64.51612903225806
Method name: cer_prob_product_log_last, running accuracy: 61.29032258064516
Method name: self_consistency, running accuracy: 64.51612903225806
Method name: p_true, running accuracy: 64.51612903225806
Method name: normilized_likelihood, running accuracy: 64.51612903225806
Method name: normilized_entropy, running accuracy: 67.74193548387096
Method name: topk_entropy, running accuracy: 64.51612903225806
Method name: window_entropy, running accuracy: 64.51612903225806
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  30%|███       | 30/100 [3:40:04<8:39:18, 445.11s/it, group_entropy_acc=67.74%, cer_entropy_weighted_mean_all_acc=64.52%, cer_prob_product_log_last_acc=61.29%, self_consistency_acc=64.52%, p_true_acc=64.52%, normilized_likelihood_acc=64.52%, normilized_entropy_acc=67.74%, topk_entropy_acc=64.52%, window_entropy_acc=64.52%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 31/100 [3:40:04<7:14:22, 377.72s/it, group_entropy_acc=67.74%, cer_entropy_weighted_mean_all_acc=64.52%, cer_prob_product_log_last_acc=61.29%, self_consistency_acc=64.52%, p_true_acc=64.52%, normilized_likelihood_acc=64.52%, normilized_entropy_acc=67.74%, topk_entropy_acc=64.52%, window_entropy_acc=64.52%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1: Let's first find the pay ratio of Denali to Nate in both cases.
The pay ...
    Score: 1.1943658590316772
    Answer: 8
    Ground truth: 5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Let's first find the pay ratio of Denali to Nate in both cases.
The pay ...
    Score: 2.931394321763796
    Answer: 8
    Ground truth: 5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Let's first find the pay ratio of Denali to Nate in both cases.
The pay ...
    Score: 2.575214445590973
    Answer: 8
    Ground truth: 5
Method 4: self_consistency
  Batch 1:
    Text: Step 1: Let's first find the pay ratio of Denali to Nate in both cases.
The pay ...
    Score: 0.2
    Answer: 8
    Ground truth: 5
Method 5: p_true
  Batch 1:
    Text: Step 1:  To solve this problem, we need to establish the ratio of Denali's pay t...
    Score: 1.6484375
    Answer: 5
    Ground truth: 5
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: Let's first find the pay ratio of Denali to Nate in both cases.
The pay ...
    Score: 2.27993106842041
    Answer: 8
    Ground truth: 5
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: Let's first find the pay ratio of Denali to Nate in both cases.
The pay ...
    Score: 1.996443510055542
    Answer: 8
    Ground truth: 5
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: Let's first find the pay ratio of Denali to Nate in both cases.
The pay ...
    Score: 1.7838154435157776
    Answer: 8
    Ground truth: 5
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  To solve this problem, we need to establish the ratio of Denali's pay t...
    Score: 3.0870847702026367
    Answer: 5
    Ground truth: 5

==================================================
Method name: group_entropy, running accuracy: 65.625
Method name: cer_entropy_weighted_mean_all, running accuracy: 62.5
Method name: cer_prob_product_log_last, running accuracy: 59.375
Method name: self_consistency, running accuracy: 62.5
Method name: p_true, running accuracy: 65.625
Method name: normilized_likelihood, running accuracy: 62.5
Method name: normilized_entropy, running accuracy: 65.625
Method name: topk_entropy, running accuracy: 62.5
Method name: window_entropy, running accuracy: 65.625
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  31%|███       | 31/100 [3:49:42<7:14:22, 377.72s/it, group_entropy_acc=65.62%, cer_entropy_weighted_mean_all_acc=62.50%, cer_prob_product_log_last_acc=59.38%, self_consistency_acc=62.50%, p_true_acc=65.62%, normilized_likelihood_acc=62.50%, normilized_entropy_acc=65.62%, topk_entropy_acc=62.50%, window_entropy_acc=65.62%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 32/100 [3:49:42<8:16:22, 437.97s/it, group_entropy_acc=65.62%, cer_entropy_weighted_mean_all_acc=62.50%, cer_prob_product_log_last_acc=59.38%, self_consistency_acc=62.50%, p_true_acc=65.62%, normilized_likelihood_acc=62.50%, normilized_entropy_acc=65.62%, topk_entropy_acc=62.50%, window_entropy_acc=65.62%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 1.712701439857483
    Answer: 1
    Ground truth: 1
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 4.99698822107828
    Answer: 1
    Ground truth: 1
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 4.995649516582489
    Answer: 1
    Ground truth: 1
Method 4: self_consistency
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 0.4166666666666667
    Answer: 1
    Ground truth: 1
Method 5: p_true
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 3.49609375
    Answer: 1
    Ground truth: 1
Method 6: normilized_likelihood
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 2.413462221622467
    Answer: 1
    Ground truth: 1
Method 7: normilized_entropy
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 2.532366067171097
    Answer: 1
    Ground truth: 1
Method 8: topk_entropy
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 2.318481743335724
    Answer: 1
    Ground truth: 1
Method 9: window_entropy
  Batch 1:
    Text: We can simplify the given expression using the identity 1 - tan^2(x) = sec^2(x) ...
    Score: 5.482493996620178
    Answer: 1
    Ground truth: 1

==================================================
Method name: group_entropy, running accuracy: 66.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 63.63636363636363
Method name: cer_prob_product_log_last, running accuracy: 60.60606060606061
Method name: self_consistency, running accuracy: 63.63636363636363
Method name: p_true, running accuracy: 66.66666666666666
Method name: normilized_likelihood, running accuracy: 63.63636363636363
Method name: normilized_entropy, running accuracy: 66.66666666666666
Method name: topk_entropy, running accuracy: 63.63636363636363
Method name: window_entropy, running accuracy: 66.66666666666666
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  32%|███▏      | 32/100 [3:58:30<8:16:22, 437.97s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=63.64%, cer_prob_product_log_last_acc=60.61%, self_consistency_acc=63.64%, p_true_acc=66.67%, normilized_likelihood_acc=63.64%, normilized_entropy_acc=66.67%, topk_entropy_acc=63.64%, window_entropy_acc=66.67%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 33/100 [3:58:30<8:39:15, 465.01s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=63.64%, cer_prob_product_log_last_acc=60.61%, self_consistency_acc=63.64%, p_true_acc=66.67%, normilized_likelihood_acc=63.64%, normilized_entropy_acc=66.67%, topk_entropy_acc=63.64%, window_entropy_acc=66.67%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 2.136980354785919
    Answer: 9
    Ground truth: 9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 14.999954832577806
    Answer: 9
    Ground truth: 9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 14.999970555305481
    Answer: 9
    Ground truth: 9
Method 4: self_consistency
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 0.9375
    Answer: 9
    Ground truth: 9
Method 5: p_true
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 2.6842041015625
    Answer: 9
    Ground truth: 9
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 1.3274085819721222
    Answer: 9
    Ground truth: 9
Method 7: normilized_entropy
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 1.5544028580188751
    Answer: 9
    Ground truth: 9
Method 8: topk_entropy
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 1.5520880222320557
    Answer: 9
    Ground truth: 9
Method 9: window_entropy
  Batch 1:
    Text: To find the length of the altitude to the base of the triangle, we need to first...
    Score: 6.013610243797302
    Answer: 9
    Ground truth: 9

==================================================
Method name: group_entropy, running accuracy: 67.64705882352942
Method name: cer_entropy_weighted_mean_all, running accuracy: 64.70588235294117
Method name: cer_prob_product_log_last, running accuracy: 61.76470588235294
Method name: self_consistency, running accuracy: 64.70588235294117
Method name: p_true, running accuracy: 67.64705882352942
Method name: normilized_likelihood, running accuracy: 64.70588235294117
Method name: normilized_entropy, running accuracy: 67.64705882352942
Method name: topk_entropy, running accuracy: 64.70588235294117
Method name: window_entropy, running accuracy: 67.64705882352942
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  33%|███▎      | 33/100 [4:01:47<8:39:15, 465.01s/it, group_entropy_acc=67.65%, cer_entropy_weighted_mean_all_acc=64.71%, cer_prob_product_log_last_acc=61.76%, self_consistency_acc=64.71%, p_true_acc=67.65%, normilized_likelihood_acc=64.71%, normilized_entropy_acc=67.65%, topk_entropy_acc=64.71%, window_entropy_acc=67.65%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 34/100 [4:01:47<7:03:06, 384.64s/it, group_entropy_acc=67.65%, cer_entropy_weighted_mean_all_acc=64.71%, cer_prob_product_log_last_acc=61.76%, self_consistency_acc=64.71%, p_true_acc=67.65%, normilized_likelihood_acc=64.71%, normilized_entropy_acc=67.65%, topk_entropy_acc=64.71%, window_entropy_acc=67.65%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 3.470043122768402
    Answer: 1
    Ground truth: 1
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 15.999959070500873
    Answer: 1
    Ground truth: 1
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 15.999971508979797
    Answer: 1
    Ground truth: 1
Method 4: self_consistency
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 1.0
    Answer: 1
    Ground truth: 1
Method 5: p_true
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 15.01953125
    Answer: 1
    Ground truth: 1
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 4.126954883337021
    Answer: 1
    Ground truth: 1
Method 7: normilized_entropy
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 3.3263546377420425
    Answer: 1
    Ground truth: 1
Method 8: topk_entropy
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 3.2666110917925835
    Answer: 1
    Ground truth: 1
Method 9: window_entropy
  Batch 1:
    Text: To find the value of $h(-1)$, we need to substitute $x=-1$ into the function $h(...
    Score: 8.733111530542374
    Answer: 1
    Ground truth: 1

==================================================
Method name: group_entropy, running accuracy: 68.57142857142857
Method name: cer_entropy_weighted_mean_all, running accuracy: 65.71428571428571
Method name: cer_prob_product_log_last, running accuracy: 62.857142857142854
Method name: self_consistency, running accuracy: 65.71428571428571
Method name: p_true, running accuracy: 68.57142857142857
Method name: normilized_likelihood, running accuracy: 65.71428571428571
Method name: normilized_entropy, running accuracy: 68.57142857142857
Method name: topk_entropy, running accuracy: 65.71428571428571
Method name: window_entropy, running accuracy: 68.57142857142857
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  34%|███▍      | 34/100 [4:04:10<7:03:06, 384.64s/it, group_entropy_acc=68.57%, cer_entropy_weighted_mean_all_acc=65.71%, cer_prob_product_log_last_acc=62.86%, self_consistency_acc=65.71%, p_true_acc=68.57%, normilized_likelihood_acc=65.71%, normilized_entropy_acc=68.57%, topk_entropy_acc=65.71%, window_entropy_acc=68.57%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 35/100 [4:04:10<5:38:01, 312.03s/it, group_entropy_acc=68.57%, cer_entropy_weighted_mean_all_acc=65.71%, cer_prob_product_log_last_acc=62.86%, self_consistency_acc=65.71%, p_true_acc=68.57%, normilized_likelihood_acc=65.71%, normilized_entropy_acc=68.57%, topk_entropy_acc=65.71%, window_entropy_acc=68.57%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 2.2358574867248535
    Answer: 224
    Ground truth: 224
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 5.992984606550207
    Answer: 224
    Ground truth: 224
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 5.991689085960388
    Answer: 224
    Ground truth: 224
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 0.375
    Answer: 224
    Ground truth: 224
Method 5: p_true
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 5.05859375
    Answer: 224
    Ground truth: 224
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 4.620680123567581
    Answer: 224
    Ground truth: 224
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 4.442969441413879
    Answer: 224
    Ground truth: 224
Method 8: topk_entropy
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 3.8167790174484253
    Answer: 224
    Ground truth: 224
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  Let's start by understanding the concept of a truncated cone. A truncat...
    Score: 5.5646631717681885
    Answer: 224
    Ground truth: 224

==================================================
Method name: group_entropy, running accuracy: 69.44444444444444
Method name: cer_entropy_weighted_mean_all, running accuracy: 66.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 63.888888888888886
Method name: self_consistency, running accuracy: 66.66666666666666
Method name: p_true, running accuracy: 69.44444444444444
Method name: normilized_likelihood, running accuracy: 66.66666666666666
Method name: normilized_entropy, running accuracy: 69.44444444444444
Method name: topk_entropy, running accuracy: 66.66666666666666
Method name: window_entropy, running accuracy: 69.44444444444444
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  35%|███▌      | 35/100 [4:11:28<5:38:01, 312.03s/it, group_entropy_acc=69.44%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=63.89%, self_consistency_acc=66.67%, p_true_acc=69.44%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=69.44%, topk_entropy_acc=66.67%, window_entropy_acc=69.44%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 36/100 [4:11:28<6:13:04, 349.77s/it, group_entropy_acc=69.44%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=63.89%, self_consistency_acc=66.67%, p_true_acc=69.44%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=69.44%, topk_entropy_acc=66.67%, window_entropy_acc=69.44%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 1.5310888290405273
    Answer: 8
    Ground truth: 9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 3.9566074353066805
    Answer: 8
    Ground truth: 9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 3.9191777110099792
    Answer: 8
    Ground truth: 9
Method 4: self_consistency
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 0.36363636363636365
    Answer: 8
    Ground truth: 9
Method 5: p_true
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 2.16796875
    Answer: 8
    Ground truth: 9
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 1.8056107759475708
    Answer: 8
    Ground truth: 9
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 1.7153532207012177
    Answer: 8
    Ground truth: 9
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 1.3911034762859344
    Answer: 8
    Ground truth: 9
Method 9: window_entropy
  Batch 1:
    Text: Step 1: To find the sum of all possible values of $n$, we need to identify the c...
    Score: 2.5700103044509888
    Answer: 8
    Ground truth: 9

==================================================
Method name: group_entropy, running accuracy: 67.56756756756756
Method name: cer_entropy_weighted_mean_all, running accuracy: 64.86486486486487
Method name: cer_prob_product_log_last, running accuracy: 62.16216216216216
Method name: self_consistency, running accuracy: 64.86486486486487
Method name: p_true, running accuracy: 67.56756756756756
Method name: normilized_likelihood, running accuracy: 64.86486486486487
Method name: normilized_entropy, running accuracy: 67.56756756756756
Method name: topk_entropy, running accuracy: 64.86486486486487
Method name: window_entropy, running accuracy: 67.56756756756756
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  36%|███▌      | 36/100 [4:22:41<6:13:04, 349.77s/it, group_entropy_acc=67.57%, cer_entropy_weighted_mean_all_acc=64.86%, cer_prob_product_log_last_acc=62.16%, self_consistency_acc=64.86%, p_true_acc=67.57%, normilized_likelihood_acc=64.86%, normilized_entropy_acc=67.57%, topk_entropy_acc=64.86%, window_entropy_acc=67.57%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 37/100 [4:22:41<7:49:01, 446.69s/it, group_entropy_acc=67.57%, cer_entropy_weighted_mean_all_acc=64.86%, cer_prob_product_log_last_acc=62.16%, self_consistency_acc=64.86%, p_true_acc=67.57%, normilized_likelihood_acc=64.86%, normilized_entropy_acc=67.57%, topk_entropy_acc=64.86%, window_entropy_acc=67.57%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 8.611734986305237
    Answer: 49
    Ground truth: 49
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 12.998509739699394
    Answer: 49
    Ground truth: 49
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 12.998420357704163
    Answer: 49
    Ground truth: 49
Method 4: self_consistency
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 0.8666666666666667
    Answer: 49
    Ground truth: 49
Method 5: p_true
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 10.859375
    Answer: 49
    Ground truth: 49
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 12.236168637871742
    Answer: 49
    Ground truth: 49
Method 7: normilized_entropy
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 11.889107406139374
    Answer: 49
    Ground truth: 49
Method 8: topk_entropy
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 10.008671835064888
    Answer: 49
    Ground truth: 49
Method 9: window_entropy
  Batch 1:
    Text: To find the least possible sum of three consecutive prime numbers that is a mult...
    Score: 15.201750755310059
    Answer: 49
    Ground truth: 49

==================================================
Method name: group_entropy, running accuracy: 68.42105263157895
Method name: cer_entropy_weighted_mean_all, running accuracy: 65.78947368421053
Method name: cer_prob_product_log_last, running accuracy: 63.1578947368421
Method name: self_consistency, running accuracy: 65.78947368421053
Method name: p_true, running accuracy: 68.42105263157895
Method name: normilized_likelihood, running accuracy: 65.78947368421053
Method name: normilized_entropy, running accuracy: 68.42105263157895
Method name: topk_entropy, running accuracy: 65.78947368421053
Method name: window_entropy, running accuracy: 68.42105263157895
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  37%|███▋      | 37/100 [4:28:22<7:49:01, 446.69s/it, group_entropy_acc=68.42%, cer_entropy_weighted_mean_all_acc=65.79%, cer_prob_product_log_last_acc=63.16%, self_consistency_acc=65.79%, p_true_acc=68.42%, normilized_likelihood_acc=65.79%, normilized_entropy_acc=68.42%, topk_entropy_acc=65.79%, window_entropy_acc=68.42%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 38/100 [4:28:22<7:08:51, 415.02s/it, group_entropy_acc=68.42%, cer_entropy_weighted_mean_all_acc=65.79%, cer_prob_product_log_last_acc=63.16%, self_consistency_acc=65.79%, p_true_acc=68.42%, normilized_likelihood_acc=65.79%, normilized_entropy_acc=68.42%, topk_entropy_acc=65.79%, window_entropy_acc=68.42%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 1.59043550491333
    Answer: 5
    Ground truth: 5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 15.999878805723528
    Answer: 5
    Ground truth: 5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 15.99991250038147
    Answer: 5
    Ground truth: 5
Method 4: self_consistency
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 1.0
    Answer: 5
    Ground truth: 5
Method 5: p_true
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 10.53125
    Answer: 5
    Ground truth: 5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 1.1994733586907387
    Answer: 5
    Ground truth: 5
Method 7: normilized_entropy
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 0.966863676905632
    Answer: 5
    Ground truth: 5
Method 8: topk_entropy
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 0.9637687131762505
    Answer: 5
    Ground truth: 5
Method 9: window_entropy
  Batch 1:
    Text: To find the units digit of the sum of the squares of the first nine positive int...
    Score: 2.4620537161827087
    Answer: 5
    Ground truth: 5

==================================================
Method name: group_entropy, running accuracy: 69.23076923076923
Method name: cer_entropy_weighted_mean_all, running accuracy: 66.66666666666666
Method name: cer_prob_product_log_last, running accuracy: 64.1025641025641
Method name: self_consistency, running accuracy: 66.66666666666666
Method name: p_true, running accuracy: 69.23076923076923
Method name: normilized_likelihood, running accuracy: 66.66666666666666
Method name: normilized_entropy, running accuracy: 69.23076923076923
Method name: topk_entropy, running accuracy: 66.66666666666666
Method name: window_entropy, running accuracy: 69.23076923076923
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  38%|███▊      | 38/100 [4:31:48<7:08:51, 415.02s/it, group_entropy_acc=69.23%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=64.10%, self_consistency_acc=66.67%, p_true_acc=69.23%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=69.23%, topk_entropy_acc=66.67%, window_entropy_acc=69.23%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 39/100 [4:31:48<5:58:05, 352.23s/it, group_entropy_acc=69.23%, cer_entropy_weighted_mean_all_acc=66.67%, cer_prob_product_log_last_acc=64.10%, self_consistency_acc=66.67%, p_true_acc=69.23%, normilized_likelihood_acc=66.67%, normilized_entropy_acc=69.23%, topk_entropy_acc=66.67%, window_entropy_acc=69.23%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we need to consider the possible combinations of (x,y,z) ...
    Score: 0.3845517635345459
    Answer: 1896 + 16 + 55 + 20 + 82 + 19 + 23 - 6 \times (379) - 18
    Ground truth: 225
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we need to consider all possible combinations of nonnegat...
    Score: 0.9998398962339989
    Answer: 6
    Ground truth: 225
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we need to consider all possible combinations of nonnegat...
    Score: 0.9998223185539246
    Answer: 6
    Ground truth: 225
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into cases based on the value of $z$....
    Score: 0.09090909090909091
    Answer: i^{-1} \text{ which is -i and }i^{-s} \text{ for $s = 4k + 2$ where}$
$ k \geq 0
    Ground truth: 225
Method 5: p_true
  Batch 1:
    Text: To find ordered triples $(x,y,z)$ of nonnegative integers less than $20$ for whi...
    Score: 0.8671875
    Answer: 44
    Ground truth: 225
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to consider the possible combinations of (x,y,z) ...
    Score: 0.9933218359947205
    Answer: 1896 + 16 + 55 + 20 + 82 + 19 + 23 - 6 \times (379) - 18
    Ground truth: 225
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to consider the possible combinations of (x,y,z) ...
    Score: 0.8356688022613525
    Answer: 1896 + 16 + 55 + 20 + 82 + 19 + 23 - 6 \times (379) - 18
    Ground truth: 225
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to consider the possible combinations of (x,y,z) ...
    Score: 0.6955961585044861
    Answer: 1896 + 16 + 55 + 20 + 82 + 19 + 23 - 6 \times (379) - 18
    Ground truth: 225
Method 9: window_entropy
  Batch 1:
    Text: The problem asks us to find the number of ordered triples $(x,y,z)$ of nonnegati...
    Score: 0.4809110164642334
    Answer: 83
    Ground truth: 225

==================================================
Method name: group_entropy, running accuracy: 67.5
Method name: cer_entropy_weighted_mean_all, running accuracy: 65.0
Method name: cer_prob_product_log_last, running accuracy: 62.5
Method name: self_consistency, running accuracy: 65.0
Method name: p_true, running accuracy: 67.5
Method name: normilized_likelihood, running accuracy: 65.0
Method name: normilized_entropy, running accuracy: 67.5
Method name: topk_entropy, running accuracy: 65.0
Method name: window_entropy, running accuracy: 67.5
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  39%|███▉      | 39/100 [4:49:12<5:58:05, 352.23s/it, group_entropy_acc=67.50%, cer_entropy_weighted_mean_all_acc=65.00%, cer_prob_product_log_last_acc=62.50%, self_consistency_acc=65.00%, p_true_acc=67.50%, normilized_likelihood_acc=65.00%, normilized_entropy_acc=67.50%, topk_entropy_acc=65.00%, window_entropy_acc=67.50%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 40/100 [4:49:12<9:19:59, 559.99s/it, group_entropy_acc=67.50%, cer_entropy_weighted_mean_all_acc=65.00%, cer_prob_product_log_last_acc=62.50%, self_consistency_acc=65.00%, p_true_acc=67.50%, normilized_likelihood_acc=65.00%, normilized_entropy_acc=67.50%, topk_entropy_acc=65.00%, window_entropy_acc=67.50%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 4.276280164718628
    Answer: -49
    Ground truth: -49
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 7.995556154012492
    Answer: -49
    Ground truth: -49
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 3.9931987261433193
    Answer: -49
    Ground truth: -49
Method 4: self_consistency
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 0.6666666666666666
    Answer: -49
    Ground truth: -49
Method 5: p_true
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 5.35546875
    Answer: -49
    Ground truth: -49
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 8.16867870092392
    Answer: -49
    Ground truth: -49
Method 7: normilized_entropy
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 7.802636027336121
    Answer: -49
    Ground truth: -49
Method 8: topk_entropy
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 6.347789645195007
    Answer: -49
    Ground truth: -49
Method 9: window_entropy
  Batch 1:
    Text: To find the determinant of 7A, we need to recall the properties of determinants....
    Score: 7.662242949008942
    Answer: -49
    Ground truth: -49

==================================================
Method name: group_entropy, running accuracy: 68.29268292682927
Method name: cer_entropy_weighted_mean_all, running accuracy: 65.85365853658537
Method name: cer_prob_product_log_last, running accuracy: 63.41463414634146
Method name: self_consistency, running accuracy: 65.85365853658537
Method name: p_true, running accuracy: 68.29268292682927
Method name: normilized_likelihood, running accuracy: 65.85365853658537
Method name: normilized_entropy, running accuracy: 68.29268292682927
Method name: topk_entropy, running accuracy: 65.85365853658537
Method name: window_entropy, running accuracy: 68.29268292682927
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  40%|████      | 40/100 [4:54:35<9:19:59, 559.99s/it, group_entropy_acc=68.29%, cer_entropy_weighted_mean_all_acc=65.85%, cer_prob_product_log_last_acc=63.41%, self_consistency_acc=65.85%, p_true_acc=68.29%, normilized_likelihood_acc=65.85%, normilized_entropy_acc=68.29%, topk_entropy_acc=65.85%, window_entropy_acc=68.29%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 41/100 [4:54:35<8:00:36, 488.75s/it, group_entropy_acc=68.29%, cer_entropy_weighted_mean_all_acc=65.85%, cer_prob_product_log_last_acc=63.41%, self_consistency_acc=65.85%, p_true_acc=68.29%, normilized_likelihood_acc=65.85%, normilized_entropy_acc=68.29%, topk_entropy_acc=65.85%, window_entropy_acc=68.29%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Final answer index is out of range
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we can first write the given equations in terms of cosine...
    Score: 0.4106936454772949
    Answer: 178
    Ground truth: 12
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1: Start with the given equations
\begin{align*}
\cos \alpha + \cos \beta +...
    Score: 0.9997354482508423
    Answer: 6
    Ground truth: 12
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1: Start with the given equations
\begin{align*}
\cos \alpha + \cos \beta +...
    Score: 0.9997137188911438
    Answer: 6
    Ground truth: 12
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we can first write the given equations in terms of cosine...
    Score: 0.14285714285714285
    Answer: 178
    Ground truth: 12
Method 5: p_true
  Batch 1:
    Text: Step 1: Start with the given equations
\begin{align*}
\cos \alpha + \cos \beta +...
    Score: 0.90234375
    Answer: 6
    Ground truth: 12
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1: To find the minimum value of $\cos \alpha$, we first want to identify th...
    Score: 0.6061044633388519
    Answer: 16
    Ground truth: 12
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1: To find the minimum value of $\cos \alpha$, we first want to identify th...
    Score: 0.587043285369873
    Answer: 16
    Ground truth: 12
Method 8: topk_entropy
  Batch 1:
    Text: Step 1: To find the minimum value of $\cos \alpha$, we first want to identify th...
    Score: 0.515200287103653
    Answer: 16
    Ground truth: 12
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can first write the given equations in terms of cosine...
    Score: 0.6306822299957275
    Answer: 178
    Ground truth: 12

==================================================
Method name: group_entropy, running accuracy: 66.66666666666666
Method name: cer_entropy_weighted_mean_all, running accuracy: 64.28571428571429
Method name: cer_prob_product_log_last, running accuracy: 61.904761904761905
Method name: self_consistency, running accuracy: 64.28571428571429
Method name: p_true, running accuracy: 66.66666666666666
Method name: normilized_likelihood, running accuracy: 64.28571428571429
Method name: normilized_entropy, running accuracy: 66.66666666666666
Method name: topk_entropy, running accuracy: 64.28571428571429
Method name: window_entropy, running accuracy: 66.66666666666666
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  41%|████      | 41/100 [5:15:53<8:00:36, 488.75s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=64.29%, cer_prob_product_log_last_acc=61.90%, self_consistency_acc=64.29%, p_true_acc=66.67%, normilized_likelihood_acc=64.29%, normilized_entropy_acc=66.67%, topk_entropy_acc=64.29%, window_entropy_acc=66.67%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 42/100 [5:15:53<11:41:28, 725.66s/it, group_entropy_acc=66.67%, cer_entropy_weighted_mean_all_acc=64.29%, cer_prob_product_log_last_acc=61.90%, self_consistency_acc=64.29%, p_true_acc=66.67%, normilized_likelihood_acc=64.29%, normilized_entropy_acc=66.67%, topk_entropy_acc=64.29%, window_entropy_acc=66.67%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 4.408488035202026
    Answer: 3
    Ground truth: 3
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 7.999866863953082
    Answer: 3
    Ground truth: 3
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 7.9998873472213745
    Answer: 3
    Ground truth: 3
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 0.5333333333333333
    Answer: 3
    Ground truth: 3
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 7.3046875
    Answer: 3
    Ground truth: 3
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 8.726516976952553
    Answer: 3
    Ground truth: 3
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 8.669496864080429
    Answer: 3
    Ground truth: 3
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 7.7065969705581665
    Answer: 3
    Ground truth: 3
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we will use the property of congruence modulo 9, where a ...
    Score: 10.114741742610931
    Answer: 3
    Ground truth: 3

==================================================
Method name: group_entropy, running accuracy: 67.44186046511628
Method name: cer_entropy_weighted_mean_all, running accuracy: 65.11627906976744
Method name: cer_prob_product_log_last, running accuracy: 62.7906976744186
Method name: self_consistency, running accuracy: 65.11627906976744
Method name: p_true, running accuracy: 67.44186046511628
Method name: normilized_likelihood, running accuracy: 65.11627906976744
Method name: normilized_entropy, running accuracy: 67.44186046511628
Method name: topk_entropy, running accuracy: 65.11627906976744
Method name: window_entropy, running accuracy: 67.44186046511628
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  42%|████▏     | 42/100 [5:20:58<11:41:28, 725.66s/it, group_entropy_acc=67.44%, cer_entropy_weighted_mean_all_acc=65.12%, cer_prob_product_log_last_acc=62.79%, self_consistency_acc=65.12%, p_true_acc=67.44%, normilized_likelihood_acc=65.12%, normilized_entropy_acc=67.44%, topk_entropy_acc=65.12%, window_entropy_acc=67.44%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 43/100 [5:20:58<9:29:18, 599.27s/it, group_entropy_acc=67.44%, cer_entropy_weighted_mean_all_acc=65.12%, cer_prob_product_log_last_acc=62.79%, self_consistency_acc=65.12%, p_true_acc=67.44%, normilized_likelihood_acc=65.12%, normilized_entropy_acc=67.44%, topk_entropy_acc=65.12%, window_entropy_acc=67.44%] 
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Let's assume $X$...
    Score: 0.28228330612182617
    Answer: 688
    Ground truth: 687
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve for the three digit number, we first need to form an equation. The valu...
    Score: 0.9996010046280478
    Answer: 130
    Ground truth: 687
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve for the three digit number, we first need to form an equation. The valu...
    Score: 0.999552309513092
    Answer: 130
    Ground truth: 687
Method 4: self_consistency
  Batch 1:
    Text: To find the value of the three-digit integer $XYD$, let's analyze the problem.

...
    Score: 0.14285714285714285
    Answer: 747
    Ground truth: 687
Method 5: p_true
  Batch 1:
    Text: To solve for $D$, we can start by expressing the integers in terms of $X$ and $Y...
    Score: 0.83203125
    Answer: 623
    Ground truth: 687
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Let's assume $X$...
    Score: 1.0721376538276672
    Answer: 688
    Ground truth: 687
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Let's assume $X$...
    Score: 0.9993935823440552
    Answer: 688
    Ground truth: 687
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down into steps.

Step 1: Let's assume $X$...
    Score: 0.8791767656803131
    Answer: 688
    Ground truth: 687
Method 9: window_entropy
  Batch 1:
    Text: To find the value of the three-digit integer $XYD$, let's analyze the problem.

...
    Score: 0.5578811168670654
    Answer: 747
    Ground truth: 687

==================================================
Method name: group_entropy, running accuracy: 65.9090909090909
Method name: cer_entropy_weighted_mean_all, running accuracy: 63.63636363636363
Method name: cer_prob_product_log_last, running accuracy: 61.36363636363637
Method name: self_consistency, running accuracy: 63.63636363636363
Method name: p_true, running accuracy: 65.9090909090909
Method name: normilized_likelihood, running accuracy: 63.63636363636363
Method name: normilized_entropy, running accuracy: 65.9090909090909
Method name: topk_entropy, running accuracy: 63.63636363636363
Method name: window_entropy, running accuracy: 65.9090909090909
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  43%|████▎     | 43/100 [5:32:31<9:29:18, 599.27s/it, group_entropy_acc=65.91%, cer_entropy_weighted_mean_all_acc=63.64%, cer_prob_product_log_last_acc=61.36%, self_consistency_acc=63.64%, p_true_acc=65.91%, normilized_likelihood_acc=63.64%, normilized_entropy_acc=65.91%, topk_entropy_acc=63.64%, window_entropy_acc=65.91%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 44/100 [5:32:31<9:45:34, 627.40s/it, group_entropy_acc=65.91%, cer_entropy_weighted_mean_all_acc=63.64%, cer_prob_product_log_last_acc=61.36%, self_consistency_acc=63.64%, p_true_acc=65.91%, normilized_likelihood_acc=63.64%, normilized_entropy_acc=65.91%, topk_entropy_acc=63.64%, window_entropy_acc=65.91%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 2.2301164865493774
    Answer: 36
    Ground truth: 24
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 5.995971924837991
    Answer: 36
    Ground truth: 24
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 5.993039965629578
    Answer: 36
    Ground truth: 24
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 0.5
    Answer: 36
    Ground truth: 24
Method 5: p_true
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 3.876953125
    Answer: 36
    Ground truth: 24
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 5.094708204269409
    Answer: 36
    Ground truth: 24
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 4.685850530862808
    Answer: 36
    Ground truth: 24
Method 8: topk_entropy
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 4.009871035814285
    Answer: 36
    Ground truth: 24
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  The ratio of the measures of the angles of a triangle is given as 3:2:1...
    Score: 5.849740028381348
    Answer: 36
    Ground truth: 24

==================================================
Method name: group_entropy, running accuracy: 64.44444444444444
Method name: cer_entropy_weighted_mean_all, running accuracy: 62.22222222222222
Method name: cer_prob_product_log_last, running accuracy: 60.0
Method name: self_consistency, running accuracy: 62.22222222222222
Method name: p_true, running accuracy: 64.44444444444444
Method name: normilized_likelihood, running accuracy: 62.22222222222222
Method name: normilized_entropy, running accuracy: 64.44444444444444
Method name: topk_entropy, running accuracy: 62.22222222222222
Method name: window_entropy, running accuracy: 64.44444444444444
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  44%|████▍     | 44/100 [5:38:32<9:45:34, 627.40s/it, group_entropy_acc=64.44%, cer_entropy_weighted_mean_all_acc=62.22%, cer_prob_product_log_last_acc=60.00%, self_consistency_acc=62.22%, p_true_acc=64.44%, normilized_likelihood_acc=62.22%, normilized_entropy_acc=64.44%, topk_entropy_acc=62.22%, window_entropy_acc=64.44%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 45/100 [5:38:32<8:22:00, 547.65s/it, group_entropy_acc=64.44%, cer_entropy_weighted_mean_all_acc=62.22%, cer_prob_product_log_last_acc=60.00%, self_consistency_acc=62.22%, p_true_acc=64.44%, normilized_likelihood_acc=62.22%, normilized_entropy_acc=64.44%, topk_entropy_acc=62.22%, window_entropy_acc=64.44%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To determine the number of positive integers $n>1$ for which $2^{24}$ is a perfe...
    Score: 0.7767400741577148
    Answer: 7
    Ground truth: 7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: We know that $2^{24}$ can be factorized as $2^8 \cdot 2^8 \cdot 2^8$ where  $2^8...
    Score: 2.785528238379145
    Answer: 5
    Ground truth: 7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: We know that $2^{24}$ can be factorized as $2^8 \cdot 2^8 \cdot 2^8$ where  $2^8...
    Score: 2.1905640065670013
    Answer: 5
    Ground truth: 7
Method 4: self_consistency
  Batch 1:
    Text: We know that $2^{24}$ can be factorized as $2^8 \cdot 2^8 \cdot 2^8$ where  $2^8...
    Score: 0.3
    Answer: 5
    Ground truth: 7
Method 5: p_true
  Batch 1:
    Text: To determine the number of positive integers $n>1$ for which $2^{24}$ is a perfe...
    Score: 1.21484375
    Answer: 7
    Ground truth: 7
Method 6: normilized_likelihood
  Batch 1:
    Text: To determine the number of positive integers $n>1$ for which $2^{24}$ is a perfe...
    Score: 2.2268161475658417
    Answer: 7
    Ground truth: 7
Method 7: normilized_entropy
  Batch 1:
    Text: To determine the number of positive integers $n>1$ for which $2^{24}$ is a perfe...
    Score: 1.9335574507713318
    Answer: 7
    Ground truth: 7
Method 8: topk_entropy
  Batch 1:
    Text: To determine the number of positive integers $n>1$ for which $2^{24}$ is a perfe...
    Score: 1.7106715142726898
    Answer: 7
    Ground truth: 7
Method 9: window_entropy
  Batch 1:
    Text: To determine the number of positive integers $n>1$ for which $2^{24}$ is a perfe...
    Score: 2.132839798927307
    Answer: 7
    Ground truth: 7

==================================================
Method name: group_entropy, running accuracy: 65.21739130434783
Method name: cer_entropy_weighted_mean_all, running accuracy: 60.86956521739131
Method name: cer_prob_product_log_last, running accuracy: 58.69565217391305
Method name: self_consistency, running accuracy: 60.86956521739131
Method name: p_true, running accuracy: 65.21739130434783
Method name: normilized_likelihood, running accuracy: 63.04347826086957
Method name: normilized_entropy, running accuracy: 65.21739130434783
Method name: topk_entropy, running accuracy: 63.04347826086957
Method name: window_entropy, running accuracy: 65.21739130434783
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  45%|████▌     | 45/100 [5:44:50<8:22:00, 547.65s/it, group_entropy_acc=65.22%, cer_entropy_weighted_mean_all_acc=60.87%, cer_prob_product_log_last_acc=58.70%, self_consistency_acc=60.87%, p_true_acc=65.22%, normilized_likelihood_acc=63.04%, normilized_entropy_acc=65.22%, topk_entropy_acc=63.04%, window_entropy_acc=65.22%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 46/100 [5:44:50<7:26:59, 496.65s/it, group_entropy_acc=65.22%, cer_entropy_weighted_mean_all_acc=60.87%, cer_prob_product_log_last_acc=58.70%, self_consistency_acc=60.87%, p_true_acc=65.22%, normilized_likelihood_acc=63.04%, normilized_entropy_acc=65.22%, topk_entropy_acc=63.04%, window_entropy_acc=65.22%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 7.538615703582764
    Answer: 32
    Ground truth: 32
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 11.999336822411795
    Answer: 32
    Ground truth: 32
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 11.999339163303375
    Answer: 32
    Ground truth: 32
Method 4: self_consistency
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 0.9230769230769231
    Answer: 32
    Ground truth: 32
Method 5: p_true
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 7.7041015625
    Answer: 32
    Ground truth: 32
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 10.062719404697418
    Answer: 32
    Ground truth: 32
Method 7: normilized_entropy
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 9.799896985292435
    Answer: 32
    Ground truth: 32
Method 8: topk_entropy
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 8.437545612454414
    Answer: 32
    Ground truth: 32
Method 9: window_entropy
  Batch 1:
    Text: To find the area of the square, we need to first find the length of its side. Si...
    Score: 14.141635954380035
    Answer: 32
    Ground truth: 32

==================================================
Method name: group_entropy, running accuracy: 65.95744680851064
Method name: cer_entropy_weighted_mean_all, running accuracy: 61.702127659574465
Method name: cer_prob_product_log_last, running accuracy: 59.57446808510638
Method name: self_consistency, running accuracy: 61.702127659574465
Method name: p_true, running accuracy: 65.95744680851064
Method name: normilized_likelihood, running accuracy: 63.829787234042556
Method name: normilized_entropy, running accuracy: 65.95744680851064
Method name: topk_entropy, running accuracy: 63.829787234042556
Method name: window_entropy, running accuracy: 65.95744680851064
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  46%|████▌     | 46/100 [5:49:17<7:26:59, 496.65s/it, group_entropy_acc=65.96%, cer_entropy_weighted_mean_all_acc=61.70%, cer_prob_product_log_last_acc=59.57%, self_consistency_acc=61.70%, p_true_acc=65.96%, normilized_likelihood_acc=63.83%, normilized_entropy_acc=65.96%, topk_entropy_acc=63.83%, window_entropy_acc=65.96%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 47/100 [5:49:17<6:17:58, 427.89s/it, group_entropy_acc=65.96%, cer_entropy_weighted_mean_all_acc=61.70%, cer_prob_product_log_last_acc=59.57%, self_consistency_acc=61.70%, p_true_acc=65.96%, normilized_likelihood_acc=63.83%, normilized_entropy_acc=65.96%, topk_entropy_acc=63.83%, window_entropy_acc=65.96%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 0.6255671977996826
    Answer: 10
    Ground truth: 4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 1.9980179480125964
    Answer: 10
    Ground truth: 4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 1.997397243976593
    Answer: 10
    Ground truth: 4
Method 4: self_consistency
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 0.25
    Answer: 10
    Ground truth: 4
Method 5: p_true
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 1.78515625
    Answer: 10
    Ground truth: 4
Method 6: normilized_likelihood
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 1.2285846173763275
    Answer: 10
    Ground truth: 4
Method 7: normilized_entropy
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 1.1135387122631073
    Answer: 10
    Ground truth: 4
Method 8: topk_entropy
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 0.9499587118625641
    Answer: 10
    Ground truth: 4
Method 9: window_entropy
  Batch 1:
    Text: Using Cauchy-Schwarz:

We have $(x^2+y^2+z^2)(x^2+y^2+z^2) \ge (xy + xz + yz)^2 ...
    Score: 1.851775884628296
    Answer: 10
    Ground truth: 4

==================================================
Method name: group_entropy, running accuracy: 64.58333333333334
Method name: cer_entropy_weighted_mean_all, running accuracy: 60.416666666666664
Method name: cer_prob_product_log_last, running accuracy: 58.333333333333336
Method name: self_consistency, running accuracy: 60.416666666666664
Method name: p_true, running accuracy: 64.58333333333334
Method name: normilized_likelihood, running accuracy: 62.5
Method name: normilized_entropy, running accuracy: 64.58333333333334
Method name: topk_entropy, running accuracy: 62.5
Method name: window_entropy, running accuracy: 64.58333333333334
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  47%|████▋     | 47/100 [6:03:31<6:17:58, 427.89s/it, group_entropy_acc=64.58%, cer_entropy_weighted_mean_all_acc=60.42%, cer_prob_product_log_last_acc=58.33%, self_consistency_acc=60.42%, p_true_acc=64.58%, normilized_likelihood_acc=62.50%, normilized_entropy_acc=64.58%, topk_entropy_acc=62.50%, window_entropy_acc=64.58%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 48/100 [6:03:31<8:01:28, 555.54s/it, group_entropy_acc=64.58%, cer_entropy_weighted_mean_all_acc=60.42%, cer_prob_product_log_last_acc=58.33%, self_consistency_acc=60.42%, p_true_acc=64.58%, normilized_likelihood_acc=62.50%, normilized_entropy_acc=64.58%, topk_entropy_acc=62.50%, window_entropy_acc=64.58%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 1.0713378190994263
    Answer: 1
    Ground truth: 1
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 2.9978649009406295
    Answer: 1
    Ground truth: 1
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 2.9967780709266663
    Answer: 1
    Ground truth: 1
Method 4: self_consistency
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 0.5
    Answer: 1
    Ground truth: 1
Method 5: p_true
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 2.6484375
    Answer: 1
    Ground truth: 1
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 1.0425621420145035
    Answer: 1
    Ground truth: 1
Method 7: normilized_entropy
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 1.0440500676631927
    Answer: 1
    Ground truth: 1
Method 8: topk_entropy
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 0.9009624719619751
    Answer: 1
    Ground truth: 1
Method 9: window_entropy
  Batch 1:
    Text: Let's denote the number of chocolate candies as C, vanilla candies as V, pepperm...
    Score: 2.527305483818054
    Answer: 1
    Ground truth: 1

==================================================
Method name: group_entropy, running accuracy: 65.3061224489796
Method name: cer_entropy_weighted_mean_all, running accuracy: 61.224489795918366
Method name: cer_prob_product_log_last, running accuracy: 59.183673469387756
Method name: self_consistency, running accuracy: 61.224489795918366
Method name: p_true, running accuracy: 65.3061224489796
Method name: normilized_likelihood, running accuracy: 63.26530612244898
Method name: normilized_entropy, running accuracy: 65.3061224489796
Method name: topk_entropy, running accuracy: 63.26530612244898
Method name: window_entropy, running accuracy: 65.3061224489796
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  48%|████▊     | 48/100 [6:15:35<8:01:28, 555.54s/it, group_entropy_acc=65.31%, cer_entropy_weighted_mean_all_acc=61.22%, cer_prob_product_log_last_acc=59.18%, self_consistency_acc=61.22%, p_true_acc=65.31%, normilized_likelihood_acc=63.27%, normilized_entropy_acc=65.31%, topk_entropy_acc=63.27%, window_entropy_acc=65.31%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 49/100 [6:15:35<8:35:19, 606.27s/it, group_entropy_acc=65.31%, cer_entropy_weighted_mean_all_acc=61.22%, cer_prob_product_log_last_acc=59.18%, self_consistency_acc=61.22%, p_true_acc=65.31%, normilized_likelihood_acc=63.27%, normilized_entropy_acc=65.31%, topk_entropy_acc=63.27%, window_entropy_acc=65.31%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 1.5168476104736328
    Answer: 1
    Ground truth: 2039191
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 8.986119211915915
    Answer: 1
    Ground truth: 2039191
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 8.976181328296661
    Answer: 1
    Ground truth: 2039191
Method 4: self_consistency
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 0.75
    Answer: 1
    Ground truth: 2039191
Method 5: p_true
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 5.13525390625
    Answer: 1
    Ground truth: 2039191
Method 6: normilized_likelihood
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 5.014576256275177
    Answer: 1
    Ground truth: 2039191
Method 7: normilized_entropy
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 4.517305970191956
    Answer: 1
    Ground truth: 2039191
Method 8: topk_entropy
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 3.888812005519867
    Answer: 1
    Ground truth: 2039191
Method 9: window_entropy
  Batch 1:
    Text: Step 1:  We start by trying to simplify the given functional equation $f(x^2) + ...
    Score: 4.938262581825256
    Answer: 1
    Ground truth: 2039191

==================================================
Method name: group_entropy, running accuracy: 64.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 60.0
Method name: cer_prob_product_log_last, running accuracy: 57.99999999999999
Method name: self_consistency, running accuracy: 60.0
Method name: p_true, running accuracy: 64.0
Method name: normilized_likelihood, running accuracy: 62.0
Method name: normilized_entropy, running accuracy: 64.0
Method name: topk_entropy, running accuracy: 62.0
Method name: window_entropy, running accuracy: 64.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  49%|████▉     | 49/100 [6:25:12<8:35:19, 606.27s/it, group_entropy_acc=64.00%, cer_entropy_weighted_mean_all_acc=60.00%, cer_prob_product_log_last_acc=58.00%, self_consistency_acc=60.00%, p_true_acc=64.00%, normilized_likelihood_acc=62.00%, normilized_entropy_acc=64.00%, topk_entropy_acc=62.00%, window_entropy_acc=64.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 50/100 [6:25:12<8:17:55, 597.51s/it, group_entropy_acc=64.00%, cer_entropy_weighted_mean_all_acc=60.00%, cer_prob_product_log_last_acc=58.00%, self_consistency_acc=60.00%, p_true_acc=64.00%, normilized_likelihood_acc=62.00%, normilized_entropy_acc=64.00%, topk_entropy_acc=62.00%, window_entropy_acc=64.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 0.8743550777435303
    Answer: 6
    Ground truth: 2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 2.950103555908682
    Answer: 6
    Ground truth: 2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 2.879753589630127
    Answer: 6
    Ground truth: 2
Method 4: self_consistency
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 0.3
    Answer: 6
    Ground truth: 2
Method 5: p_true
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 1.537109375
    Answer: 6
    Ground truth: 2
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 1.3706415593624115
    Answer: 6
    Ground truth: 2
Method 7: normilized_entropy
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 1.2358677685260773
    Answer: 6
    Ground truth: 2
Method 8: topk_entropy
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 1.0175223350524902
    Answer: 6
    Ground truth: 2
Method 9: window_entropy
  Batch 1:
    Text: To solve the given inequality, we will follow these steps:

First, we need to mu...
    Score: 1.5807734727859497
    Answer: 6
    Ground truth: 2

==================================================
Method name: group_entropy, running accuracy: 62.745098039215684
Method name: cer_entropy_weighted_mean_all, running accuracy: 58.82352941176471
Method name: cer_prob_product_log_last, running accuracy: 56.86274509803921
Method name: self_consistency, running accuracy: 58.82352941176471
Method name: p_true, running accuracy: 62.745098039215684
Method name: normilized_likelihood, running accuracy: 60.78431372549019
Method name: normilized_entropy, running accuracy: 62.745098039215684
Method name: topk_entropy, running accuracy: 60.78431372549019
Method name: window_entropy, running accuracy: 62.745098039215684
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  50%|█████     | 50/100 [6:36:10<8:17:55, 597.51s/it, group_entropy_acc=62.75%, cer_entropy_weighted_mean_all_acc=58.82%, cer_prob_product_log_last_acc=56.86%, self_consistency_acc=58.82%, p_true_acc=62.75%, normilized_likelihood_acc=60.78%, normilized_entropy_acc=62.75%, topk_entropy_acc=60.78%, window_entropy_acc=62.75%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 51/100 [6:36:10<8:22:37, 615.45s/it, group_entropy_acc=62.75%, cer_entropy_weighted_mean_all_acc=58.82%, cer_prob_product_log_last_acc=56.86%, self_consistency_acc=58.82%, p_true_acc=62.75%, normilized_likelihood_acc=60.78%, normilized_entropy_acc=62.75%, topk_entropy_acc=60.78%, window_entropy_acc=62.75%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 1.5443046689033508
    Answer: 5
    Ground truth: 5
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 15.999586108897448
    Answer: 5
    Ground truth: 5
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 15.999632835388184
    Answer: 5
    Ground truth: 5
Method 4: self_consistency
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 1.0
    Answer: 5
    Ground truth: 5
Method 5: p_true
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 12.4609375
    Answer: 5
    Ground truth: 5
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 1.8571829870343208
    Answer: 5
    Ground truth: 5
Method 7: normilized_entropy
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 1.3232326656579971
    Answer: 5
    Ground truth: 5
Method 8: topk_entropy
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 1.3118788599967957
    Answer: 5
    Ground truth: 5
Method 9: window_entropy
  Batch 1:
    Text: To find the midpoint of a line segment, we use the midpoint formula: 
Midpoint =...
    Score: 7.148898512125015
    Answer: 5
    Ground truth: 5

==================================================
Method name: group_entropy, running accuracy: 63.46153846153846
Method name: cer_entropy_weighted_mean_all, running accuracy: 59.61538461538461
Method name: cer_prob_product_log_last, running accuracy: 57.692307692307686
Method name: self_consistency, running accuracy: 59.61538461538461
Method name: p_true, running accuracy: 63.46153846153846
Method name: normilized_likelihood, running accuracy: 61.53846153846154
Method name: normilized_entropy, running accuracy: 63.46153846153846
Method name: topk_entropy, running accuracy: 61.53846153846154
Method name: window_entropy, running accuracy: 63.46153846153846
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  51%|█████     | 51/100 [6:38:54<8:22:37, 615.45s/it, group_entropy_acc=63.46%, cer_entropy_weighted_mean_all_acc=59.62%, cer_prob_product_log_last_acc=57.69%, self_consistency_acc=59.62%, p_true_acc=63.46%, normilized_likelihood_acc=61.54%, normilized_entropy_acc=63.46%, topk_entropy_acc=61.54%, window_entropy_acc=63.46%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 52/100 [6:38:54<6:24:07, 480.15s/it, group_entropy_acc=63.46%, cer_entropy_weighted_mean_all_acc=59.62%, cer_prob_product_log_last_acc=57.69%, self_consistency_acc=59.62%, p_true_acc=63.46%, normilized_likelihood_acc=61.54%, normilized_entropy_acc=63.46%, topk_entropy_acc=61.54%, window_entropy_acc=63.46%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we want to find conditions under which the three planes a...
    Score: 0.331836462020874
    Answer: 0
    Ground truth: 1
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we want to find conditions under which the three planes a...
    Score: 2.9533125762852044
    Answer: 0
    Ground truth: 1
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we want to find conditions under which the three planes a...
    Score: 2.9934844970703125
    Answer: 0
    Ground truth: 1
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we want to find conditions under which the three planes a...
    Score: 0.2727272727272727
    Answer: 0
    Ground truth: 1
Method 5: p_true
  Batch 1:
    Text: To ensure the given planes all intersect at the same line, their triple scalar p...
    Score: 0.8671875
    Answer: 0, 2, 3
    Ground truth: 1
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we need to find the possible values of $a$, $b$, and $c$ ...
    Score: 1.2760687470436096
    Answer: 3,-1
    Ground truth: 1
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we need to find the possible values of $a$, $b$, and $c$ ...
    Score: 1.1121065616607666
    Answer: 3,-1
    Ground truth: 1
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we need to find the possible values of $a$, $b$, and $c$ ...
    Score: 0.9665001034736633
    Answer: 3,-1
    Ground truth: 1
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we need to find the possible values of $a$, $b$, and $c$ ...
    Score: 0.6759753227233887
    Answer: 3,-1
    Ground truth: 1

==================================================
Method name: group_entropy, running accuracy: 62.264150943396224
Method name: cer_entropy_weighted_mean_all, running accuracy: 58.490566037735846
Method name: cer_prob_product_log_last, running accuracy: 56.60377358490566
Method name: self_consistency, running accuracy: 58.490566037735846
Method name: p_true, running accuracy: 62.264150943396224
Method name: normilized_likelihood, running accuracy: 60.37735849056604
Method name: normilized_entropy, running accuracy: 62.264150943396224
Method name: topk_entropy, running accuracy: 60.37735849056604
Method name: window_entropy, running accuracy: 62.264150943396224
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  52%|█████▏    | 52/100 [6:53:48<6:24:07, 480.15s/it, group_entropy_acc=62.26%, cer_entropy_weighted_mean_all_acc=58.49%, cer_prob_product_log_last_acc=56.60%, self_consistency_acc=58.49%, p_true_acc=62.26%, normilized_likelihood_acc=60.38%, normilized_entropy_acc=62.26%, topk_entropy_acc=60.38%, window_entropy_acc=62.26%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 53/100 [6:53:48<7:53:22, 604.30s/it, group_entropy_acc=62.26%, cer_entropy_weighted_mean_all_acc=58.49%, cer_prob_product_log_last_acc=56.60%, self_consistency_acc=58.49%, p_true_acc=62.26%, normilized_likelihood_acc=60.38%, normilized_entropy_acc=62.26%, topk_entropy_acc=60.38%, window_entropy_acc=62.26%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into several steps.

1.  **Find the ...
    Score: 0.39294958114624023
    Answer: 960
    Ground truth: 62
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: Step 1:  We first need to identify the constraint imposed by Manoj's preference....
    Score: 0.9999408644217489
    Answer: 18
    Ground truth: 62
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: Step 1:  We first need to identify the constraint imposed by Manoj's preference....
    Score: 0.9999475479125977
    Answer: 18
    Ground truth: 62
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we will first count the total number of ways to assign th...
    Score: 0.07692307692307693
    Answer: 32
    Ground truth: 62
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we will first count the total number of ways to assign th...
    Score: 0.796875
    Answer: 32
    Ground truth: 62
Method 6: normilized_likelihood
  Batch 1:
    Text: Let's break this down step by step:

1. First, we consider the condition given: ...
    Score: 1.2428933382034302
    Answer: 62
    Ground truth: 62
Method 7: normilized_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. First, we consider the condition given: ...
    Score: 1.2397844195365906
    Answer: 62
    Ground truth: 62
Method 8: topk_entropy
  Batch 1:
    Text: Let's break this down step by step:

1. First, we consider the condition given: ...
    Score: 1.0577991604804993
    Answer: 62
    Ground truth: 62
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can break it down into several steps.

1.  **Find the ...
    Score: 1.0844427347183228
    Answer: 960
    Ground truth: 62

==================================================
Method name: group_entropy, running accuracy: 61.111111111111114
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.407407407407405
Method name: cer_prob_product_log_last, running accuracy: 55.55555555555556
Method name: self_consistency, running accuracy: 57.407407407407405
Method name: p_true, running accuracy: 61.111111111111114
Method name: normilized_likelihood, running accuracy: 61.111111111111114
Method name: normilized_entropy, running accuracy: 62.96296296296296
Method name: topk_entropy, running accuracy: 61.111111111111114
Method name: window_entropy, running accuracy: 61.111111111111114
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  53%|█████▎    | 53/100 [7:00:53<7:53:22, 604.30s/it, group_entropy_acc=61.11%, cer_entropy_weighted_mean_all_acc=57.41%, cer_prob_product_log_last_acc=55.56%, self_consistency_acc=57.41%, p_true_acc=61.11%, normilized_likelihood_acc=61.11%, normilized_entropy_acc=62.96%, topk_entropy_acc=61.11%, window_entropy_acc=61.11%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 54/100 [7:00:53<7:02:07, 550.59s/it, group_entropy_acc=61.11%, cer_entropy_weighted_mean_all_acc=57.41%, cer_prob_product_log_last_acc=55.56%, self_consistency_acc=57.41%, p_true_acc=61.11%, normilized_likelihood_acc=61.11%, normilized_entropy_acc=62.96%, topk_entropy_acc=61.11%, window_entropy_acc=61.11%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 1.1156375408172607
    Answer: 144
    Ground truth: 144
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 3.9798934919513167
    Answer: 144
    Ground truth: 144
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 3.9685370326042175
    Answer: 144
    Ground truth: 144
Method 4: self_consistency
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 0.36363636363636365
    Answer: 144
    Ground truth: 144
Method 5: p_true
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 1.5302734375
    Answer: 144
    Ground truth: 144
Method 6: normilized_likelihood
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 1.7782243490219116
    Answer: 144
    Ground truth: 144
Method 7: normilized_entropy
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 1.9026725590229034
    Answer: 144
    Ground truth: 144
Method 8: topk_entropy
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 1.6708113253116608
    Answer: 144
    Ground truth: 144
Method 9: window_entropy
  Batch 1:
    Text: To create the pie chart of this result, we need to determine the total number of...
    Score: 3.670340061187744
    Answer: 144
    Ground truth: 144

==================================================
Method name: group_entropy, running accuracy: 61.81818181818181
Method name: cer_entropy_weighted_mean_all, running accuracy: 58.18181818181818
Method name: cer_prob_product_log_last, running accuracy: 56.36363636363636
Method name: self_consistency, running accuracy: 58.18181818181818
Method name: p_true, running accuracy: 61.81818181818181
Method name: normilized_likelihood, running accuracy: 61.81818181818181
Method name: normilized_entropy, running accuracy: 63.63636363636363
Method name: topk_entropy, running accuracy: 61.81818181818181
Method name: window_entropy, running accuracy: 61.81818181818181
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  54%|█████▍    | 54/100 [7:06:17<7:02:07, 550.59s/it, group_entropy_acc=61.82%, cer_entropy_weighted_mean_all_acc=58.18%, cer_prob_product_log_last_acc=56.36%, self_consistency_acc=58.18%, p_true_acc=61.82%, normilized_likelihood_acc=61.82%, normilized_entropy_acc=63.64%, topk_entropy_acc=61.82%, window_entropy_acc=61.82%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 55/100 [7:06:17<6:01:56, 482.60s/it, group_entropy_acc=61.82%, cer_entropy_weighted_mean_all_acc=58.18%, cer_prob_product_log_last_acc=56.36%, self_consistency_acc=58.18%, p_true_acc=61.82%, normilized_likelihood_acc=61.82%, normilized_entropy_acc=63.64%, topk_entropy_acc=61.82%, window_entropy_acc=61.82%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 2.0034613609313965
    Answer: -11
    Ground truth: -8
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 4.994164671363565
    Answer: -11
    Ground truth: -8
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 2.489843301552291
    Answer: -11
    Ground truth: -8
Method 4: self_consistency
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 0.5
    Answer: -11
    Ground truth: -8
Method 5: p_true
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 2.89453125
    Answer: -11
    Ground truth: -8
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 2.4097636342048645
    Answer: -11
    Ground truth: -8
Method 7: normilized_entropy
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 2.185083895921707
    Answer: -11
    Ground truth: -8
Method 8: topk_entropy
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 1.883383721113205
    Answer: -11
    Ground truth: -8
Method 9: window_entropy
  Batch 1:
    Text: To find the value of $k$, we first need to determine the vertex of the parabola....
    Score: 4.364154815673828
    Answer: -11
    Ground truth: -8

==================================================
Method name: group_entropy, running accuracy: 60.71428571428571
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.14285714285714
Method name: cer_prob_product_log_last, running accuracy: 55.35714285714286
Method name: self_consistency, running accuracy: 57.14285714285714
Method name: p_true, running accuracy: 60.71428571428571
Method name: normilized_likelihood, running accuracy: 60.71428571428571
Method name: normilized_entropy, running accuracy: 62.5
Method name: topk_entropy, running accuracy: 60.71428571428571
Method name: window_entropy, running accuracy: 60.71428571428571
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  55%|█████▌    | 55/100 [7:16:40<6:01:56, 482.60s/it, group_entropy_acc=60.71%, cer_entropy_weighted_mean_all_acc=57.14%, cer_prob_product_log_last_acc=55.36%, self_consistency_acc=57.14%, p_true_acc=60.71%, normilized_likelihood_acc=60.71%, normilized_entropy_acc=62.50%, topk_entropy_acc=60.71%, window_entropy_acc=60.71%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 56/100 [7:16:40<6:24:37, 524.49s/it, group_entropy_acc=60.71%, cer_entropy_weighted_mean_all_acc=57.14%, cer_prob_product_log_last_acc=55.36%, self_consistency_acc=57.14%, p_true_acc=60.71%, normilized_likelihood_acc=60.71%, normilized_entropy_acc=62.50%, topk_entropy_acc=60.71%, window_entropy_acc=60.71%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 2.3353763818740845
    Answer: 10
    Ground truth: 10
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 8.999939590487434
    Answer: 10
    Ground truth: 10
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 8.99995744228363
    Answer: 10
    Ground truth: 10
Method 4: self_consistency
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 0.6
    Answer: 10
    Ground truth: 10
Method 5: p_true
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 7.53515625
    Answer: 10
    Ground truth: 10
Method 6: normilized_likelihood
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 3.504227891564369
    Answer: 10
    Ground truth: 10
Method 7: normilized_entropy
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 2.3788918554782867
    Answer: 10
    Ground truth: 10
Method 8: topk_entropy
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 2.212465226650238
    Answer: 10
    Ground truth: 10
Method 9: window_entropy
  Batch 1:
    Text: To simplify the given expression $(x\sqrt{x^3})^4$, we will use the properties o...
    Score: 7.298844337463379
    Answer: 10
    Ground truth: 10

==================================================
Method name: group_entropy, running accuracy: 61.40350877192983
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.89473684210527
Method name: cer_prob_product_log_last, running accuracy: 56.14035087719298
Method name: self_consistency, running accuracy: 57.89473684210527
Method name: p_true, running accuracy: 61.40350877192983
Method name: normilized_likelihood, running accuracy: 61.40350877192983
Method name: normilized_entropy, running accuracy: 63.1578947368421
Method name: topk_entropy, running accuracy: 61.40350877192983
Method name: window_entropy, running accuracy: 61.40350877192983
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  56%|█████▌    | 56/100 [7:20:45<6:24:37, 524.49s/it, group_entropy_acc=61.40%, cer_entropy_weighted_mean_all_acc=57.89%, cer_prob_product_log_last_acc=56.14%, self_consistency_acc=57.89%, p_true_acc=61.40%, normilized_likelihood_acc=61.40%, normilized_entropy_acc=63.16%, topk_entropy_acc=61.40%, window_entropy_acc=61.40%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 57/100 [7:20:45<5:15:50, 440.72s/it, group_entropy_acc=61.40%, cer_entropy_weighted_mean_all_acc=57.89%, cer_prob_product_log_last_acc=56.14%, self_consistency_acc=57.89%, p_true_acc=61.40%, normilized_likelihood_acc=61.40%, normilized_entropy_acc=63.16%, topk_entropy_acc=61.40%, window_entropy_acc=61.40%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 2.4480016231536865
    Answer: 0
    Ground truth: 4
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 5.785529075789282
    Answer: 0
    Ground truth: 4
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 5.8675273060798645
    Answer: 0
    Ground truth: 4
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 0.5
    Answer: 0
    Ground truth: 4
Method 5: p_true
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 3.2451171875
    Answer: 0
    Ground truth: 4
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 2.6179887652397156
    Answer: 0
    Ground truth: 4
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 2.496309816837311
    Answer: 0
    Ground truth: 4
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 1.9060436189174652
    Answer: 0
    Ground truth: 4
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, we can use Vieta's formulas and properties of quadratic e...
    Score: 4.026064872741699
    Answer: 0
    Ground truth: 4

==================================================
Method name: group_entropy, running accuracy: 60.3448275862069
Method name: cer_entropy_weighted_mean_all, running accuracy: 56.896551724137936
Method name: cer_prob_product_log_last, running accuracy: 55.172413793103445
Method name: self_consistency, running accuracy: 56.896551724137936
Method name: p_true, running accuracy: 60.3448275862069
Method name: normilized_likelihood, running accuracy: 60.3448275862069
Method name: normilized_entropy, running accuracy: 62.06896551724138
Method name: topk_entropy, running accuracy: 60.3448275862069
Method name: window_entropy, running accuracy: 60.3448275862069
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  57%|█████▋    | 57/100 [7:36:03<5:15:50, 440.72s/it, group_entropy_acc=60.34%, cer_entropy_weighted_mean_all_acc=56.90%, cer_prob_product_log_last_acc=55.17%, self_consistency_acc=56.90%, p_true_acc=60.34%, normilized_likelihood_acc=60.34%, normilized_entropy_acc=62.07%, topk_entropy_acc=60.34%, window_entropy_acc=60.34%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 58/100 [7:36:03<6:48:42, 583.87s/it, group_entropy_acc=60.34%, cer_entropy_weighted_mean_all_acc=56.90%, cer_prob_product_log_last_acc=55.17%, self_consistency_acc=56.90%, p_true_acc=60.34%, normilized_likelihood_acc=60.34%, normilized_entropy_acc=62.07%, topk_entropy_acc=60.34%, window_entropy_acc=60.34%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 4.1489405035972595
    Answer: 2560
    Ground truth: 2560
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 15.99996735525542
    Answer: 2560
    Ground truth: 2560
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 7.999979913234924
    Answer: 2560
    Ground truth: 2560
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 1.0
    Answer: 2560
    Ground truth: 2560
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 11.51171875
    Answer: 2560
    Ground truth: 2560
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 1.6808603554964066
    Answer: 2560
    Ground truth: 2560
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 2.0001480281352997
    Answer: 2560
    Ground truth: 2560
Method 8: topk_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 1.9940839856863022
    Answer: 2560
    Ground truth: 2560
Method 9: window_entropy
  Batch 1:
    Text: To solve this problem, let's break it down step by step:

1. We know that at 1:0...
    Score: 4.1035261154174805
    Answer: 2560
    Ground truth: 2560

==================================================
Method name: group_entropy, running accuracy: 61.016949152542374
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.6271186440678
Method name: cer_prob_product_log_last, running accuracy: 55.932203389830505
Method name: self_consistency, running accuracy: 57.6271186440678
Method name: p_true, running accuracy: 61.016949152542374
Method name: normilized_likelihood, running accuracy: 61.016949152542374
Method name: normilized_entropy, running accuracy: 62.71186440677966
Method name: topk_entropy, running accuracy: 61.016949152542374
Method name: window_entropy, running accuracy: 61.016949152542374
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  58%|█████▊    | 58/100 [7:39:46<6:48:42, 583.87s/it, group_entropy_acc=61.02%, cer_entropy_weighted_mean_all_acc=57.63%, cer_prob_product_log_last_acc=55.93%, self_consistency_acc=57.63%, p_true_acc=61.02%, normilized_likelihood_acc=61.02%, normilized_entropy_acc=62.71%, topk_entropy_acc=61.02%, window_entropy_acc=61.02%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 59/100 [7:39:46<5:24:58, 475.58s/it, group_entropy_acc=61.02%, cer_entropy_weighted_mean_all_acc=57.63%, cer_prob_product_log_last_acc=55.93%, self_consistency_acc=57.63%, p_true_acc=61.02%, normilized_likelihood_acc=61.02%, normilized_entropy_acc=62.71%, topk_entropy_acc=61.02%, window_entropy_acc=61.02%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 5.532463431358337
    Answer: 9
    Ground truth: 9
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 10.999795552984441
    Answer: 9
    Ground truth: 9
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 10.999826908111572
    Answer: 9
    Ground truth: 9
Method 4: self_consistency
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 0.7857142857142857
    Answer: 9
    Ground truth: 9
Method 5: p_true
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 9.546875
    Answer: 9
    Ground truth: 9
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 7.3497177213430405
    Answer: 9
    Ground truth: 9
Method 7: normilized_entropy
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 7.166569471359253
    Answer: 9
    Ground truth: 9
Method 8: topk_entropy
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 6.138582482933998
    Answer: 9
    Ground truth: 9
Method 9: window_entropy
  Batch 1:
    Text: To find the number of distinct diagonals of a convex hexagon, we can use the for...
    Score: 13.757790505886078
    Answer: 9
    Ground truth: 9

==================================================
Method name: group_entropy, running accuracy: 61.66666666666667
Method name: cer_entropy_weighted_mean_all, running accuracy: 58.333333333333336
Method name: cer_prob_product_log_last, running accuracy: 56.666666666666664
Method name: self_consistency, running accuracy: 58.333333333333336
Method name: p_true, running accuracy: 61.66666666666667
Method name: normilized_likelihood, running accuracy: 61.66666666666667
Method name: normilized_entropy, running accuracy: 63.33333333333333
Method name: topk_entropy, running accuracy: 61.66666666666667
Method name: window_entropy, running accuracy: 61.66666666666667
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  59%|█████▉    | 59/100 [7:44:10<5:24:58, 475.58s/it, group_entropy_acc=61.67%, cer_entropy_weighted_mean_all_acc=58.33%, cer_prob_product_log_last_acc=56.67%, self_consistency_acc=58.33%, p_true_acc=61.67%, normilized_likelihood_acc=61.67%, normilized_entropy_acc=63.33%, topk_entropy_acc=61.67%, window_entropy_acc=61.67%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 60/100 [7:44:10<4:34:41, 412.04s/it, group_entropy_acc=61.67%, cer_entropy_weighted_mean_all_acc=58.33%, cer_prob_product_log_last_acc=56.67%, self_consistency_acc=58.33%, p_true_acc=61.67%, normilized_likelihood_acc=61.67%, normilized_entropy_acc=63.33%, topk_entropy_acc=61.67%, window_entropy_acc=61.67%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 0.8205509185791016
    Answer: -4
    Ground truth: 23
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 2.943886467328582
    Answer: -4
    Ground truth: 23
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 1.1229088399222835
    Answer: -4
    Ground truth: 23
Method 4: self_consistency
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 0.25
    Answer: -4
    Ground truth: 23
Method 5: p_true
  Batch 1:
    Text: To find the remainder of the division, we will use the polynomial long division ...
    Score: 0.70703125
    Answer: 7
    Ground truth: 23
Method 6: normilized_likelihood
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 1.9713485836982727
    Answer: -4
    Ground truth: 23
Method 7: normilized_entropy
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 1.8460137248039246
    Answer: -4
    Ground truth: 23
Method 8: topk_entropy
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 1.5052018761634827
    Answer: -4
    Ground truth: 23
Method 9: window_entropy
  Batch 1:
    Text: We want to find the remainder when $6y^3+5y^2-16y+8$ is divided by $2y+3$. Since...
    Score: 1.2549355030059814
    Answer: -4
    Ground truth: 23

==================================================
Method name: group_entropy, running accuracy: 60.65573770491803
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.377049180327866
Method name: cer_prob_product_log_last, running accuracy: 55.73770491803278
Method name: self_consistency, running accuracy: 57.377049180327866
Method name: p_true, running accuracy: 60.65573770491803
Method name: normilized_likelihood, running accuracy: 60.65573770491803
Method name: normilized_entropy, running accuracy: 62.295081967213115
Method name: topk_entropy, running accuracy: 60.65573770491803
Method name: window_entropy, running accuracy: 60.65573770491803
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  60%|██████    | 60/100 [7:56:19<4:34:41, 412.04s/it, group_entropy_acc=60.66%, cer_entropy_weighted_mean_all_acc=57.38%, cer_prob_product_log_last_acc=55.74%, self_consistency_acc=57.38%, p_true_acc=60.66%, normilized_likelihood_acc=60.66%, normilized_entropy_acc=62.30%, topk_entropy_acc=60.66%, window_entropy_acc=60.66%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 61/100 [7:56:19<5:29:42, 507.24s/it, group_entropy_acc=60.66%, cer_entropy_weighted_mean_all_acc=57.38%, cer_prob_product_log_last_acc=55.74%, self_consistency_acc=57.38%, p_true_acc=60.66%, normilized_likelihood_acc=60.66%, normilized_entropy_acc=62.30%, topk_entropy_acc=60.66%, window_entropy_acc=60.66%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 5.403028845787048
    Answer: 105
    Ground truth: 105
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 13.996577708010399
    Answer: 105
    Ground truth: 105
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 13.995593130588531
    Answer: 105
    Ground truth: 105
Method 4: self_consistency
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 1.0
    Answer: 105
    Ground truth: 105
Method 5: p_true
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 7.689453125
    Answer: 105
    Ground truth: 105
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 7.282285828143358
    Answer: 105
    Ground truth: 105
Method 7: normilized_entropy
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 6.78766168653965
    Answer: 105
    Ground truth: 105
Method 8: topk_entropy
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 6.167705990374088
    Answer: 105
    Ground truth: 105
Method 9: window_entropy
  Batch 1:
    Text: To find the least positive integer divisible by the four smallest odd, positive ...
    Score: 10.163426101207733
    Answer: 105
    Ground truth: 105

==================================================
Method name: group_entropy, running accuracy: 61.29032258064516
Method name: cer_entropy_weighted_mean_all, running accuracy: 58.06451612903226
Method name: cer_prob_product_log_last, running accuracy: 56.451612903225815
Method name: self_consistency, running accuracy: 58.06451612903226
Method name: p_true, running accuracy: 61.29032258064516
Method name: normilized_likelihood, running accuracy: 61.29032258064516
Method name: normilized_entropy, running accuracy: 62.903225806451616
Method name: topk_entropy, running accuracy: 61.29032258064516
Method name: window_entropy, running accuracy: 61.29032258064516
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  61%|██████    | 61/100 [8:00:10<5:29:42, 507.24s/it, group_entropy_acc=61.29%, cer_entropy_weighted_mean_all_acc=58.06%, cer_prob_product_log_last_acc=56.45%, self_consistency_acc=58.06%, p_true_acc=61.29%, normilized_likelihood_acc=61.29%, normilized_entropy_acc=62.90%, topk_entropy_acc=61.29%, window_entropy_acc=61.29%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 62/100 [8:00:10<4:28:46, 424.39s/it, group_entropy_acc=61.29%, cer_entropy_weighted_mean_all_acc=58.06%, cer_prob_product_log_last_acc=56.45%, self_consistency_acc=58.06%, p_true_acc=61.29%, normilized_likelihood_acc=61.29%, normilized_entropy_acc=62.90%, topk_entropy_acc=61.29%, window_entropy_acc=61.29%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find $(x+1)^{48}$, first we need to simplify $x$. 

We can do this by noticin...
    Score: 0.3087596893310547
    Answer: 244140625
    Ground truth: 125
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find $(x+1)^{48}$, we first need to simplify the expression for $x$. We can r...
    Score: 0.9914787954698531
    Answer: \frac{1}{3^{47}}
    Ground truth: 125
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find $(x+1)^{48}$, first we need to simplify $x$. 

We can do this by noticin...
    Score: 0.22976425834250927
    Answer: 244140625
    Ground truth: 125
Method 4: self_consistency
  Batch 1:
    Text: To find $(x+1)^{48}$, we start by simplifying $x$.

We have,
$$x={4\over{(\sqrt5...
    Score: 0.2
    Answer: 1296x^2 - 252000\sqrt5x-180000
    Ground truth: 125
Method 5: p_true
  Batch 1:
    Text: To find the value of $(x+1)^{48}$, we first simplify the given expression for $x...
    Score: 0.62109375
    Answer: 134217728
    Ground truth: 125
Method 6: normilized_likelihood
  Batch 1:
    Text: To find $(x+1)^{48}$, first we need to simplify $x$. 

We can do this by noticin...
    Score: 1.171531081199646
    Answer: 244140625
    Ground truth: 125
Method 7: normilized_entropy
  Batch 1:
    Text: To find $(x+1)^{48}$, first we need to simplify $x$. 

We can do this by noticin...
    Score: 1.0997373461723328
    Answer: 244140625
    Ground truth: 125
Method 8: topk_entropy
  Batch 1:
    Text: To find $(x+1)^{48}$, first we need to simplify $x$. 

We can do this by noticin...
    Score: 0.9677046239376068
    Answer: 244140625
    Ground truth: 125
Method 9: window_entropy
  Batch 1:
    Text: To find $(x+1)^{48}$, first we need to simplify $x$. 

We can do this by noticin...
    Score: 0.8444167375564575
    Answer: 244140625
    Ground truth: 125

==================================================
Method name: group_entropy, running accuracy: 60.317460317460316
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.14285714285714
Method name: cer_prob_product_log_last, running accuracy: 55.55555555555556
Method name: self_consistency, running accuracy: 57.14285714285714
Method name: p_true, running accuracy: 60.317460317460316
Method name: normilized_likelihood, running accuracy: 60.317460317460316
Method name: normilized_entropy, running accuracy: 61.904761904761905
Method name: topk_entropy, running accuracy: 60.317460317460316
Method name: window_entropy, running accuracy: 60.317460317460316
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  62%|██████▏   | 62/100 [8:16:48<4:28:46, 424.39s/it, group_entropy_acc=60.32%, cer_entropy_weighted_mean_all_acc=57.14%, cer_prob_product_log_last_acc=55.56%, self_consistency_acc=57.14%, p_true_acc=60.32%, normilized_likelihood_acc=60.32%, normilized_entropy_acc=61.90%, topk_entropy_acc=60.32%, window_entropy_acc=60.32%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 63/100 [8:16:48<6:07:46, 596.40s/it, group_entropy_acc=60.32%, cer_entropy_weighted_mean_all_acc=57.14%, cer_prob_product_log_last_acc=55.56%, self_consistency_acc=57.14%, p_true_acc=60.32%, normilized_likelihood_acc=60.32%, normilized_entropy_acc=61.90%, topk_entropy_acc=60.32%, window_entropy_acc=60.32%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the number of days in a year with a 3 in the tens digit, we need to cons...
    Score: 2.962402105331421
    Answer: 9
    Ground truth: 18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To solve this, we must first note how the months are structured in terms of the ...
    Score: 0.9984285047809494
    Answer: 45
    Ground truth: 18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To solve this, we must first note how the months are structured in terms of the ...
    Score: 0.998185932636261
    Answer: 45
    Ground truth: 18
Method 4: self_consistency
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

1. First, we need to con...
    Score: 0.16666666666666666
    Answer: 21
    Ground truth: 18
Method 5: p_true
  Batch 1:
    Text: To solve this problem, let's break it down into steps:

1. First, we need to con...
    Score: 0.65234375
    Answer: 21
    Ground truth: 18
Method 6: normilized_likelihood
  Batch 1:
    Text: To solve this, we must first note how the months are structured in terms of the ...
    Score: 1.266097366809845
    Answer: 45
    Ground truth: 18
Method 7: normilized_entropy
  Batch 1:
    Text: To solve this, we must first note how the months are structured in terms of the ...
    Score: 1.3083320260047913
    Answer: 45
    Ground truth: 18
Method 8: topk_entropy
  Batch 1:
    Text: To solve this, we must first note how the months are structured in terms of the ...
    Score: 1.1407139301300049
    Answer: 45
    Ground truth: 18
Method 9: window_entropy
  Batch 1:
    Text: To find the number of days in a year with a 3 in the tens digit, we need to cons...
    Score: 1.4185677766799927
    Answer: 9
    Ground truth: 18

==================================================
Method name: group_entropy, running accuracy: 59.375
Method name: cer_entropy_weighted_mean_all, running accuracy: 56.25
Method name: cer_prob_product_log_last, running accuracy: 54.6875
Method name: self_consistency, running accuracy: 56.25
Method name: p_true, running accuracy: 59.375
Method name: normilized_likelihood, running accuracy: 59.375
Method name: normilized_entropy, running accuracy: 60.9375
Method name: topk_entropy, running accuracy: 59.375
Method name: window_entropy, running accuracy: 59.375
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  63%|██████▎   | 63/100 [8:21:56<6:07:46, 596.40s/it, group_entropy_acc=59.38%, cer_entropy_weighted_mean_all_acc=56.25%, cer_prob_product_log_last_acc=54.69%, self_consistency_acc=56.25%, p_true_acc=59.38%, normilized_likelihood_acc=59.38%, normilized_entropy_acc=60.94%, topk_entropy_acc=59.38%, window_entropy_acc=59.38%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 64/100 [8:21:56<5:05:53, 509.81s/it, group_entropy_acc=59.38%, cer_entropy_weighted_mean_all_acc=56.25%, cer_prob_product_log_last_acc=54.69%, self_consistency_acc=56.25%, p_true_acc=59.38%, normilized_likelihood_acc=59.38%, normilized_entropy_acc=60.94%, topk_entropy_acc=59.38%, window_entropy_acc=59.38%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To solve the problem $126_7 - 54_7,$ we will first convert each number to base 1...
    Score: 2.763169050216675
    Answer: 36_7
    Ground truth: 42_7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 6.7474723418709015
    Answer: 42_7
    Ground truth: 42_7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 1.50406921116004
    Answer: 42_7
    Ground truth: 42_7
Method 4: self_consistency
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 0.4666666666666667
    Answer: 42_7
    Ground truth: 42_7
Method 5: p_true
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 3.1083984375
    Answer: 42_7
    Ground truth: 42_7
Method 6: normilized_likelihood
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 4.215068161487579
    Answer: 42_7
    Ground truth: 42_7
Method 7: normilized_entropy
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 4.1314613819122314
    Answer: 42_7
    Ground truth: 42_7
Method 8: topk_entropy
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 3.4921621680259705
    Answer: 42_7
    Ground truth: 42_7
Method 9: window_entropy
  Batch 1:
    Text: To compute $126_7 - 54_7$ in base 7, we first perform the subtraction. 

First, ...
    Score: 2.6891883611679077
    Answer: 42_7
    Ground truth: 42_7

==================================================
Method name: group_entropy, running accuracy: 58.46153846153847
Method name: cer_entropy_weighted_mean_all, running accuracy: 56.92307692307692
Method name: cer_prob_product_log_last, running accuracy: 55.38461538461539
Method name: self_consistency, running accuracy: 56.92307692307692
Method name: p_true, running accuracy: 60.0
Method name: normilized_likelihood, running accuracy: 60.0
Method name: normilized_entropy, running accuracy: 61.53846153846154
Method name: topk_entropy, running accuracy: 60.0
Method name: window_entropy, running accuracy: 60.0
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  64%|██████▍   | 64/100 [8:26:48<5:05:53, 509.81s/it, group_entropy_acc=58.46%, cer_entropy_weighted_mean_all_acc=56.92%, cer_prob_product_log_last_acc=55.38%, self_consistency_acc=56.92%, p_true_acc=60.00%, normilized_likelihood_acc=60.00%, normilized_entropy_acc=61.54%, topk_entropy_acc=60.00%, window_entropy_acc=60.00%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 65/100 [8:26:48<4:19:19, 444.56s/it, group_entropy_acc=58.46%, cer_entropy_weighted_mean_all_acc=56.92%, cer_prob_product_log_last_acc=55.38%, self_consistency_acc=56.92%, p_true_acc=60.00%, normilized_likelihood_acc=60.00%, normilized_entropy_acc=61.54%, topk_entropy_acc=60.00%, window_entropy_acc=60.00%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: First, we need to calculate the number of handshakes that each player does.

Eac...
    Score: 0.6361620426177979
    Answer: 66
    Ground truth: 162
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: First, we need to calculate the number of handshakes that each player does.

Eac...
    Score: 1.9997888458298338
    Answer: 66
    Ground truth: 162
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: First, we need to calculate the number of handshakes that each player does.

Eac...
    Score: 1.9997909665107727
    Answer: 66
    Ground truth: 162
Method 4: self_consistency
  Batch 1:
    Text: To find the total number of handshakes, we need to calculate the number of hands...
    Score: 0.15384615384615385
    Answer: 90
    Ground truth: 162
Method 5: p_true
  Batch 1:
    Text: First, we need to calculate the number of handshakes that each player does.

Eac...
    Score: 1.5
    Answer: 66
    Ground truth: 162
Method 6: normilized_likelihood
  Batch 1:
    Text: First, we need to calculate the number of handshakes that each player does.

Eac...
    Score: 0.9230590462684631
    Answer: 66
    Ground truth: 162
Method 7: normilized_entropy
  Batch 1:
    Text: First, we need to calculate the number of handshakes that each player does.

Eac...
    Score: 1.0648136138916016
    Answer: 66
    Ground truth: 162
Method 8: topk_entropy
  Batch 1:
    Text: First, we need to calculate the number of handshakes that each player does.

Eac...
    Score: 0.9024578928947449
    Answer: 66
    Ground truth: 162
Method 9: window_entropy
  Batch 1:
    Text: To find the total number of handshakes, let's break down the process step by ste...
    Score: 1.387511134147644
    Answer: 396
    Ground truth: 162

==================================================
Method name: group_entropy, running accuracy: 57.57575757575758
Method name: cer_entropy_weighted_mean_all, running accuracy: 56.060606060606055
Method name: cer_prob_product_log_last, running accuracy: 54.54545454545454
Method name: self_consistency, running accuracy: 56.060606060606055
Method name: p_true, running accuracy: 59.09090909090909
Method name: normilized_likelihood, running accuracy: 59.09090909090909
Method name: normilized_entropy, running accuracy: 60.60606060606061
Method name: topk_entropy, running accuracy: 59.09090909090909
Method name: window_entropy, running accuracy: 59.09090909090909
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  65%|██████▌   | 65/100 [8:32:36<4:19:19, 444.56s/it, group_entropy_acc=57.58%, cer_entropy_weighted_mean_all_acc=56.06%, cer_prob_product_log_last_acc=54.55%, self_consistency_acc=56.06%, p_true_acc=59.09%, normilized_likelihood_acc=59.09%, normilized_entropy_acc=60.61%, topk_entropy_acc=59.09%, window_entropy_acc=59.09%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 66/100 [8:32:36<3:55:32, 415.65s/it, group_entropy_acc=57.58%, cer_entropy_weighted_mean_all_acc=56.06%, cer_prob_product_log_last_acc=54.55%, self_consistency_acc=56.06%, p_true_acc=59.09%, normilized_likelihood_acc=59.09%, normilized_entropy_acc=60.61%, topk_entropy_acc=59.09%, window_entropy_acc=59.09%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 3.193575382232666
    Answer: 106
    Ground truth: 106
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 4.9998279106937975
    Answer: 106
    Ground truth: 106
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 4.999843001365662
    Answer: 106
    Ground truth: 106
Method 4: self_consistency
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 0.3125
    Answer: 106
    Ground truth: 106
Method 5: p_true
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 2.634765625
    Answer: 106
    Ground truth: 106
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 2.8849968016147614
    Answer: 106
    Ground truth: 106
Method 7: normilized_entropy
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 2.92479607462883
    Answer: 106
    Ground truth: 106
Method 8: topk_entropy
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 2.5673976093530655
    Answer: 106
    Ground truth: 106
Method 9: window_entropy
  Batch 1:
    Text: To find the sum of the coefficients of $g(x)$, we first need to find $g(x)$.

Si...
    Score: 5.536238133907318
    Answer: 106
    Ground truth: 106

==================================================
Method name: group_entropy, running accuracy: 58.2089552238806
Method name: cer_entropy_weighted_mean_all, running accuracy: 56.71641791044776
Method name: cer_prob_product_log_last, running accuracy: 55.223880597014926
Method name: self_consistency, running accuracy: 56.71641791044776
Method name: p_true, running accuracy: 59.70149253731343
Method name: normilized_likelihood, running accuracy: 59.70149253731343
Method name: normilized_entropy, running accuracy: 61.19402985074627
Method name: topk_entropy, running accuracy: 59.70149253731343
Method name: window_entropy, running accuracy: 59.70149253731343
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  66%|██████▌   | 66/100 [8:40:32<3:55:32, 415.65s/it, group_entropy_acc=58.21%, cer_entropy_weighted_mean_all_acc=56.72%, cer_prob_product_log_last_acc=55.22%, self_consistency_acc=56.72%, p_true_acc=59.70%, normilized_likelihood_acc=59.70%, normilized_entropy_acc=61.19%, topk_entropy_acc=59.70%, window_entropy_acc=59.70%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 67/100 [8:40:32<3:58:33, 433.75s/it, group_entropy_acc=58.21%, cer_entropy_weighted_mean_all_acc=56.72%, cer_prob_product_log_last_acc=55.22%, self_consistency_acc=56.72%, p_true_acc=59.70%, normilized_likelihood_acc=59.70%, normilized_entropy_acc=61.19%, topk_entropy_acc=59.70%, window_entropy_acc=59.70%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 2.0315164923667908
    Answer: 18
    Ground truth: 18
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 15.999911887214388
    Answer: 18
    Ground truth: 18
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 15.99993121623993
    Answer: 18
    Ground truth: 18
Method 4: self_consistency
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 1.0
    Answer: 18
    Ground truth: 18
Method 5: p_true
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 13.1640625
    Answer: 18
    Ground truth: 18
Method 6: normilized_likelihood
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 2.752337172627449
    Answer: 18
    Ground truth: 18
Method 7: normilized_entropy
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 1.070814698934555
    Answer: 18
    Ground truth: 18
Method 8: topk_entropy
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 1.0779814720153809
    Answer: 18
    Ground truth: 18
Method 9: window_entropy
  Batch 1:
    Text: To find 30% of the same number when we know 20% of the number is 12, we need to ...
    Score: 5.618772268295288
    Answer: 18
    Ground truth: 18

==================================================
Method name: group_entropy, running accuracy: 58.82352941176471
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.35294117647059
Method name: cer_prob_product_log_last, running accuracy: 55.88235294117647
Method name: self_consistency, running accuracy: 57.35294117647059
Method name: p_true, running accuracy: 60.29411764705882
Method name: normilized_likelihood, running accuracy: 60.29411764705882
Method name: normilized_entropy, running accuracy: 61.76470588235294
Method name: topk_entropy, running accuracy: 60.29411764705882
Method name: window_entropy, running accuracy: 60.29411764705882
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  67%|██████▋   | 67/100 [8:43:19<3:58:33, 433.75s/it, group_entropy_acc=58.82%, cer_entropy_weighted_mean_all_acc=57.35%, cer_prob_product_log_last_acc=55.88%, self_consistency_acc=57.35%, p_true_acc=60.29%, normilized_likelihood_acc=60.29%, normilized_entropy_acc=61.76%, topk_entropy_acc=60.29%, window_entropy_acc=60.29%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 68/100 [8:43:19<3:08:35, 353.62s/it, group_entropy_acc=58.82%, cer_entropy_weighted_mean_all_acc=57.35%, cer_prob_product_log_last_acc=55.88%, self_consistency_acc=57.35%, p_true_acc=60.29%, normilized_likelihood_acc=60.29%, normilized_entropy_acc=61.76%, topk_entropy_acc=60.29%, window_entropy_acc=60.29%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 1.8630099296569824
    Answer: 37
    Ground truth: 37
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 4.989862203237672
    Answer: 37
    Ground truth: 37
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 4.981513261795044
    Answer: 37
    Ground truth: 37
Method 4: self_consistency
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 0.5555555555555556
    Answer: 37
    Ground truth: 37
Method 5: p_true
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 1.9375
    Answer: 37
    Ground truth: 37
Method 6: normilized_likelihood
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 3.3023462295532227
    Answer: 37
    Ground truth: 37
Method 7: normilized_entropy
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 3.2439118027687073
    Answer: 37
    Ground truth: 37
Method 8: topk_entropy
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 2.5583364367485046
    Answer: 37
    Ground truth: 37
Method 9: window_entropy
  Batch 1:
    Text: They all pooled together $\$25.67 + \$17.21 + \$39.17 + \$26.32 = \$108.37.$ 
Th...
    Score: 3.8471699953079224
    Answer: 37
    Ground truth: 37

==================================================
Method name: group_entropy, running accuracy: 59.42028985507246
Method name: cer_entropy_weighted_mean_all, running accuracy: 57.971014492753625
Method name: cer_prob_product_log_last, running accuracy: 56.52173913043478
Method name: self_consistency, running accuracy: 57.971014492753625
Method name: p_true, running accuracy: 60.86956521739131
Method name: normilized_likelihood, running accuracy: 60.86956521739131
Method name: normilized_entropy, running accuracy: 62.31884057971014
Method name: topk_entropy, running accuracy: 60.86956521739131
Method name: window_entropy, running accuracy: 60.86956521739131
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  68%|██████▊   | 68/100 [8:48:45<3:08:35, 353.62s/it, group_entropy_acc=59.42%, cer_entropy_weighted_mean_all_acc=57.97%, cer_prob_product_log_last_acc=56.52%, self_consistency_acc=57.97%, p_true_acc=60.87%, normilized_likelihood_acc=60.87%, normilized_entropy_acc=62.32%, topk_entropy_acc=60.87%, window_entropy_acc=60.87%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 69/100 [8:48:45<2:58:29, 345.46s/it, group_entropy_acc=59.42%, cer_entropy_weighted_mean_all_acc=57.97%, cer_prob_product_log_last_acc=56.52%, self_consistency_acc=57.97%, p_true_acc=60.87%, normilized_likelihood_acc=60.87%, normilized_entropy_acc=62.32%, topk_entropy_acc=60.87%, window_entropy_acc=60.87%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 2.9165087938308716
    Answer: 2
    Ground truth: 2
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 9.999112633980868
    Answer: 2
    Ground truth: 2
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 9.999110698699951
    Answer: 2
    Ground truth: 2
Method 4: self_consistency
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 0.7692307692307693
    Answer: 2
    Ground truth: 2
Method 5: p_true
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 6.1796875
    Answer: 2
    Ground truth: 2
Method 6: normilized_likelihood
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 4.004103377461433
    Answer: 2
    Ground truth: 2
Method 7: normilized_entropy
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 3.339832082390785
    Answer: 2
    Ground truth: 2
Method 8: topk_entropy
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 3.038251742720604
    Answer: 2
    Ground truth: 2
Method 9: window_entropy
  Batch 1:
    Text: To compute the given expression, we can start by applying the definition of the ...
    Score: 5.496099770069122
    Answer: 2
    Ground truth: 2

==================================================
Method name: group_entropy, running accuracy: 60.0
Method name: cer_entropy_weighted_mean_all, running accuracy: 58.57142857142858
Method name: cer_prob_product_log_last, running accuracy: 57.14285714285714
Method name: self_consistency, running accuracy: 58.57142857142858
Method name: p_true, running accuracy: 61.42857142857143
Method name: normilized_likelihood, running accuracy: 61.42857142857143
Method name: normilized_entropy, running accuracy: 62.857142857142854
Method name: topk_entropy, running accuracy: 61.42857142857143
Method name: window_entropy, running accuracy: 61.42857142857143
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  69%|██████▉   | 69/100 [8:59:15<2:58:29, 345.46s/it, group_entropy_acc=60.00%, cer_entropy_weighted_mean_all_acc=58.57%, cer_prob_product_log_last_acc=57.14%, self_consistency_acc=58.57%, p_true_acc=61.43%, normilized_likelihood_acc=61.43%, normilized_entropy_acc=62.86%, topk_entropy_acc=61.43%, window_entropy_acc=61.43%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 70/100 [8:59:15<3:35:21, 430.71s/it, group_entropy_acc=60.00%, cer_entropy_weighted_mean_all_acc=58.57%, cer_prob_product_log_last_acc=57.14%, self_consistency_acc=58.57%, p_true_acc=61.43%, normilized_likelihood_acc=61.43%, normilized_entropy_acc=62.86%, topk_entropy_acc=61.43%, window_entropy_acc=61.43%]
Group name: sampling
Current method: group_entropy, method config: {'decoding_mode': '', 'method': 'group_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_entropy_weighted_mean_all, method config: {'decoding_mode': 'all', 'method': 'cer', 'scoring_mode': 'weighted_mean', 'sampling_mode': 'temperature', 'confidence': 'entropy'}
Current method: cer_prob_product_log_last, method config: {'decoding_mode': 'last', 'method': 'cer', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: self_consistency, method config: {'decoding_mode': '', 'method': 'self_consistency', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: p_true, method config: {'decoding_mode': '', 'method': 'p_true', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_likelihood, method config: {'decoding_mode': '', 'method': 'nl', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: normilized_entropy, method config: {'decoding_mode': '', 'method': 'ne', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: topk_entropy, method config: {'decoding_mode': '', 'method': 'topk_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}
Current method: window_entropy, method config: {'decoding_mode': '', 'method': 'window_entropy', 'scoring_mode': 'log', 'sampling_mode': 'temperature', 'confidence': 'default'}

=== Group 1 ===
Method 1: group_entropy
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 3.967934787273407
    Answer: 7
    Ground truth: 7
Method 2: cer_entropy_weighted_mean_all
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 13.999890537385545
    Answer: 7
    Ground truth: 7
Method 3: cer_prob_product_log_last
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 13.999912738800049
    Answer: 7
    Ground truth: 7
Method 4: self_consistency
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 0.875
    Answer: 7
    Ground truth: 7
Method 5: p_true
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 12.296875
    Answer: 7
    Ground truth: 7
Method 6: normilized_likelihood
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 2.9502302706241608
    Answer: 7
    Ground truth: 7
Method 7: normilized_entropy
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 2.6198195070028305
    Answer: 7
    Ground truth: 7
Method 8: topk_entropy
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 2.345236584544182
    Answer: 7
    Ground truth: 7
Method 9: window_entropy
  Batch 1:
    Text: To find the constant coefficient, we need to distribute the coefficients inside ...
    Score: 8.228371977806091
    Answer: 7
    Ground truth: 7

==================================================
Method name: group_entropy, running accuracy: 60.56338028169014
Method name: cer_entropy_weighted_mean_all, running accuracy: 59.154929577464785
Method name: cer_prob_product_log_last, running accuracy: 57.74647887323944
Method name: self_consistency, running accuracy: 59.154929577464785
Method name: p_true, running accuracy: 61.97183098591549
Method name: normilized_likelihood, running accuracy: 61.97183098591549
Method name: normilized_entropy, running accuracy: 63.38028169014085
Method name: topk_entropy, running accuracy: 61.97183098591549
Method name: window_entropy, running accuracy: 61.97183098591549
==================================================
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  70%|███████   | 70/100 [9:02:43<3:35:21, 430.71s/it, group_entropy_acc=60.56%, cer_entropy_weighted_mean_all_acc=59.15%, cer_prob_product_log_last_acc=57.75%, self_consistency_acc=59.15%, p_true_acc=61.97%, normilized_likelihood_acc=61.97%, normilized_entropy_acc=63.38%, topk_entropy_acc=61.97%, window_entropy_acc=61.97%]Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 71/100 [9:02:43<2:55:57, 364.04s/it, group_entropy_acc=60.56%, cer_entropy_weighted_mean_all_acc=59.15%, cer_prob_product_log_last_acc=57.75%, self_consistency_acc=59.15%, p_true_acc=61.97%, normilized_likelihood_acc=61.97%, normilized_entropy_acc=63.38%, topk_entropy_acc=61.97%, window_entropy_acc=61.97%]
Group name: sampling
Processing math__data_sunqiao_projects_models_Llama-3.1-8B-Instruct:  71%|███████   | 71/100 [9:03:40<3:42:04, 459.45s/it, group_entropy_acc=60.56%, cer_entropy_weighted_mean_all_acc=59.15%, cer_prob_product_log_last_acc=57.75%, self_consistency_acc=59.15%, p_true_acc=61.97%, normilized_likelihood_acc=61.97%, normilized_entropy_acc=63.38%, topk_entropy_acc=61.97%, window_entropy_acc=61.97%]
Traceback (most recent call last):
  File "/data/sunqiao/projects/compconf/main.py", line 14, in <module>
    run(config=config)
  File "/data/sunqiao/projects/compconf/src/evaluation.py", line 452, in run
    evaluate_dataset(
  File "/data/sunqiao/projects/compconf/src/evaluation.py", line 365, in evaluate_dataset
    batch_results = evaluate_batch_examples(
  File "/data/sunqiao/projects/compconf/src/evaluation.py", line 288, in evaluate_batch_examples
    group_results = handle_sampling_group(model, tokenizer, lingua_model, batch_questions, tokenized_batch, config, group_cfgs, device)
  File "/data/sunqiao/projects/compconf/src/evaluation.py", line 49, in handle_sampling_group
    batch_output = model.generate(
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/generation/utils.py", line 2539, in generate
    result = self._sample(
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/generation/utils.py", line 2870, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/utils/generic.py", line 940, in wrapper
    output = func(self, *args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/accelerate/hooks.py", line 364, in pre_forward
    return send_to_device(args, self.execution_device), send_to_device(
  File "/data/sunqiao/anaconda3/envs/cer/lib/python3.10/site-packages/accelerate/utils/operations.py", line 173, in send_to_device
    elif isinstance(tensor, (tuple, list)):
KeyboardInterrupt
